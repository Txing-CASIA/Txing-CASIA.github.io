<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"txing-casia.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="A survey of deep RL and IL for autonomous driving policy learning 论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.01993v1 背景  首先介绍5类结合了IL和RL的自动驾驶模型。First, a taxonomy of the literature studies is constructed fro">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning | A survey of deep RL and IL for autonomous driving policy learning">
<meta property="og:url" content="https://txing-casia.github.io/2022/05/31/2022-05-30-Reinforcement%20Learning%20-%20a%20survey%20of%20deep%20RL%20and%20IL%20for%20autonomous%20driving%20policy%20learning/index.html">
<meta property="og:site_name" content="Txing">
<meta property="og:description" content="A survey of deep RL and IL for autonomous driving policy learning 论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.01993v1 背景  首先介绍5类结合了IL和RL的自动驾驶模型。First, a taxonomy of the literature studies is constructed fro">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-05-30T16:00:00.000Z">
<meta property="article:modified_time" content="2023-03-25T01:54:15.011Z">
<meta property="article:author" content="Txing">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="HRL">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://txing-casia.github.io/2022/05/31/2022-05-30-Reinforcement%20Learning%20-%20a%20survey%20of%20deep%20RL%20and%20IL%20for%20autonomous%20driving%20policy%20learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Reinforcement Learning | A survey of deep RL and IL for autonomous driving policy learning | Txing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Txing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到 | 伽蓝之堂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/05/31/2022-05-30-Reinforcement%20Learning%20-%20a%20survey%20of%20deep%20RL%20and%20IL%20for%20autonomous%20driving%20policy%20learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reinforcement Learning | A survey of deep RL and IL for autonomous driving policy learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-31 00:00:00" itemprop="dateCreated datePublished" datetime="2022-05-31T00:00:00+08:00">2022-05-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-03-25 09:54:15" itemprop="dateModified" datetime="2023-03-25T09:54:15+08:00">2023-03-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1
id="a-survey-of-deep-rl-and-il-for-autonomous-driving-policy-learning">A
survey of deep RL and IL for autonomous driving policy learning</h1>
<p>论文链接：https://arxiv.org/abs/2101.01993v1</p>
<h2 id="背景">背景</h2>
<ul>
<li><p>首先介绍5类结合了IL和RL的自动驾驶模型。First, a taxonomy of the
literature studies is constructed from the system perspective, among
which five modes of integration of DRL/DIL models into an AD
architecture are identified.</p></li>
<li><p>其次介绍自动驾驶中具体的RL和IL任务和公式。Second, the
formulations of DRL/DIL models for conducting specified AD tasks are
comprehensively reviewed, where various designs on the model state and
action spaces and the reinforcement learning rewards are
covered.</p></li>
<li><p>最后介绍RL和IL如何解决自动驾驶模型与参与者和环境交互的安全问题。Finally,
an in-depth review is conducted on how the critical issues of AD
applications regarding driving safety, interaction with other traffic
participants and uncertainty of the environment are addressed by the
DRL/DIL models.</p></li>
<li><p><strong>task-driven</strong> and <strong>problem-driven</strong>
perspectives</p></li>
<li><p>代表性文章：</p>
<ul>
<li>[1] C. Urmson and W. Whittaker, “Self-driving cars and the urban
challenge,” IEEE Intelligent Systems, vol. 23, no. 2, pp. 66–68,
2008.</li>
<li>[2] S. Thrun, “Toward robotic cars,” Communications of the ACM, vol.
53, no. 4, pp. 99–106, 2010.</li>
<li>[3] A. Eskandarian, Handbook of intelligent vehicles. Springer,
2012, vol. 2.</li>
<li>[4] S. M. Grigorescu, B. Trasnea, T. T. Cocias, and G. Macesanu, “A
survey of deep learning techniques for autonomous driving,” J. Field
Robotics, vol. 37, no. 3, pp. 362–386, 2020.</li>
</ul></li>
<li><p>驾驶策略基于多个等级的抽象（multiple levels of
abstraction），例如行为规划、运动规划和控制（behavior planning, motion
planning and control）</p></li>
<li><p>典型的自动驾驶模型结构图：</p>
<p><img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220602-1.png" /></p></li>
<li><p>相关综述的调研：</p>
<ul>
<li>[13, 15] survey the motion planning and control methods of automated
vehicles before the era of DL.</li>
<li>[29–33] review general DRL/DIL methods without considering any
particular applications.</li>
<li>[4] addresses the deep learning techniques for AD with a focus on
perception and control, while [34] addresses control only.</li>
<li>[35] provides a taxonomy of AD tasks to which DRL models have been
applied and highlights the key challenges.</li>
</ul></li>
<li><p>pomdp相关参考文献：</p>
<ul>
<li>G. Shani, J. Pineau, and R. Kaplow, “A survey of point-based pomdp
solvers,” Autonomous Agents and Multi-Agent Systems, vol. 27, no. 1, pp.
1–51, 2013.</li>
<li>W. S. Lovejoy, “A survey of algorithmic methods for partially
observed markov decision processes,” Annals of Operations Research, vol.
28, no. 1, pp. 47–65, 1991.</li>
</ul></li>
<li><p>强化学习（reinforcement learning）和模仿学习（imitation
learning）算法分类示意图：</p>
<p><img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220620-1.png" /></p></li>
<li><p>learning from demonstrations (LfD)</p></li>
<li><p><strong>模仿学习</strong>的形式：</p>
<ul>
<li>A demonstration dataset <span class="math display">\[D=\{\xi_i
\}_{i=0,...,N}\]</span>,表示一系列的轨迹</li>
<li><span
class="math display">\[\xi_i=\{(s^i_t,a^i_t)\}_{t=1,...,T}\]</span>是state-action
pairs（状态行为对）的序列</li>
<li>专家策略<span class="math display">\[\pi_E\]</span></li>
<li>待优化的模仿策略<span class="math display">\[\pi^*\]</span></li>
<li><span class="math display">\[\pi^*=\arg\min_{\pi}
\mathbb{D}(\pi_E,\pi)\]</span></li>
<li><span
class="math display">\[\mathbb{D}\]</span>是策略间的相似性度量函数</li>
</ul></li>
<li><p>模仿学习的三个分类：</p>
<ul>
<li><p><strong>Behavior Clone (BC) :</strong></p>
<ul>
<li><p><span class="math display">\[\min_{\theta}
\mathbb{E}\mid\mid\pi_{\theta}-\pi_E\mid\mid_2\]</span></p></li>
<li><p><span class="math display">\[J(\theta)=\mathbb{E}_{(s,a)\sim
D}[(\pi_{\theta}(s)-a)^2]\]</span></p></li>
<li><p>BC在训练集中表现良好，但在泛化性上表现差，covariate shift [66,
67]</p></li>
<li><p>代表方法：</p>
<ul>
<li><p><strong>DAgger</strong>——S. Ross, G. J. Gordon, and D. Bagnell,
“A reduction of imitation learning and structured prediction to
no-regret online learning,” in International Conference on Artificial
Intelligence and Statistics, ser. JMLR Proceedings, vol. 15, 2011, pp.
627–635.</p></li>
<li><p><strong>SafeDAgger</strong>——J. Zhang and K. Cho,
“Query-efficient imitation learning for end-to-end autonomous driving,”
arXiv preprint arXiv:1605.06450, 2016.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Inverse Reinforcement Learning (IRL)：</strong></p>
<ul>
<li><p><span class="math display">\[\max_{\theta}
\mathbb{E}_{\pi_E}[G_t|r_{\theta}]-\mathbb{E}_{\pi}[G_t|r_{\theta}]\]</span></p></li>
<li><p><span class="math display">\[J(\theta)=\mathbb{E}_{\xi_i \sim
D}[\log P(\xi_i|r_{\theta})]\]</span></p></li>
<li><p>代表方法：</p>
<ul>
<li><p><strong>guided cost learning (GCL)</strong>——C. Finn, S. Levine,
and P. Abbeel, “Guided cost learning: Deep inverse optimal control via
policy optimization,” in International Conference on Machine Learning,
2016, pp. 49–58.</p>
<p>（it handles unknown dynamics in high-dimensional complex systems and
learns complex neural network cost functions through an efficient
sample-based approximation.）</p></li>
</ul></li>
</ul></li>
<li><p><strong>Generative Adversarial Imitation Learning
(GAIL):</strong></p></li>
<li><p>Generative adversarial imitation learning (GAIL) [81] directly
learns a policy from expert demonstrations while requiring neither the
<strong>reward design</strong> in RL nor the <strong>expensive RL
process</strong> in the inner loop of IRL.</p></li>
<li><p><span class="math display">\[\min_{\pi_{\theta}}\max_{D_{\omega}}
\mathbb{E}_{\pi_{\theta}}[\log
D_{\omega}(s,a)]+\mathbb{E}_{\pi_E}[\log(1-D_{\omega}(s,a))]-\lambda
H(\pi_{\theta})\]</span>，其中<span
class="math display">\[H(\pi)\]</span>是一个正则熵项，生成器和判别器通过下式更新：</p></li>
<li><p><span
class="math display">\[\nabla_{\theta}J(\theta)=\mathbb{E}_{\pi}[\nabla_{\theta}
\log \pi_{\theta}(a\mid
s)Q(s,a)]-\lambda\nabla_{\theta}H(\pi_{\theta})\]</span></p></li>
<li><p><span
class="math display">\[\nabla_{\omega}J(\omega)=\mathbb{E}_{\pi}[\nabla_{\omega}
\log D_{\omega}(s,a)]+\mathbb{E}_{\pi_E}[\nabla_{\omega}
\log(1-D_{\omega}(s,a))]\]</span></p></li>
<li><p>Fu et al.[84] proposed adversarial inverse reinforcement learning
(AIRL) based on an adversarial reward learning formulation, which can
recover reward functions that are robust to dynamics changes.</p>
<ul>
<li>J. Fu, K. Luo, and S. Levine, “Learning robust rewards with
adversarial inverse reinforcement learning,” CoRR, vol. abs/1710.11248,
2017.</li>
</ul></li>
</ul></li>
<li><h2 id="adautonomous-driving系统的几个模块">AD（Autonomous
driving）系统的几个模块：</h2></li>
</ul>
<h2 id="主要内容">主要内容</h2>
<ul>
<li></li>
</ul>
<h2 id="总结">总结</h2>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/HRL/" rel="tag"># HRL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/14/2022-05-14-%E5%88%A9%E7%94%A8patch%E5%88%B6%E4%BD%9C%E7%9A%84%E5%AF%B9%E8%B1%A1%E5%9C%A8animation%E4%BB%A5%E5%8F%8A%E7%94%9F%E6%88%90Gif%E6%96%87%E4%BB%B6%E6%97%B6%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF%E7%9A%84%E9%97%AE%E9%A2%98/" rel="prev" title="利用patch制作的对象在animation以及生成Gif文件时显示错误的问题">
      <i class="fa fa-chevron-left"></i> 利用patch制作的对象在animation以及生成Gif文件时显示错误的问题
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/07/14/2022-07-14-Autonomous%20Driving%20-%20Exploring%20Imitation%20Learning%20for%20Autonomous%20Driving%20with%20Feedback%20Synthesizer%20and%20Differentiable%20Rasterization%20(2021%20Apollo%206.0)/" rel="next" title="Autonomous Driving | Exploring Imitation Learning for Autonomous Driving with Feedback Synthesizer and Differentiable Rasterization (2021, Apollo 6.0)">
      Autonomous Driving | Exploring Imitation Learning for Autonomous Driving with Feedback Synthesizer and Differentiable Rasterization (2021, Apollo 6.0) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#a-survey-of-deep-rl-and-il-for-autonomous-driving-policy-learning"><span class="nav-number">1.</span> <span class="nav-text">A
survey of deep RL and IL for autonomous driving policy learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">1.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adautonomous-driving%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%87%A0%E4%B8%AA%E6%A8%A1%E5%9D%97"><span class="nav-number">1.2.</span> <span class="nav-text">AD（Autonomous
driving）系统的几个模块：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9"><span class="nav-number">1.3.</span> <span class="nav-text">主要内容</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.4.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Txing</p>
  <div class="site-description" itemprop="description">泛用类人决战型机器人博士</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">229</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/yourname" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Txing</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
