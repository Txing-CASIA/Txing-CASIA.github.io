<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"txing-casia.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="泛用人形决战型机器人博士">
<meta property="og:type" content="website">
<meta property="og:title" content="Txing">
<meta property="og:url" content="https://txing-casia.github.io/page/4/index.html">
<meta property="og:site_name" content="Txing">
<meta property="og:description" content="泛用人形决战型机器人博士">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Txing">
<meta property="article:tag" content="Txing">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://txing-casia.github.io/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Txing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Txing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到 | 伽蓝之堂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/17/2022-08-17-Autonomous%20Driving%20-%20SMARTS%20Scalable%20Multi-Agent%20Reinforcement%20Learning%20Training%20School%20for%20Autonomous%20Driving/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/17/2022-08-17-Autonomous%20Driving%20-%20SMARTS%20Scalable%20Multi-Agent%20Reinforcement%20Learning%20Training%20School%20for%20Autonomous%20Driving/" class="post-title-link" itemprop="url">Autonomous Driving | SMARTS Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving (Huawei)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-17 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-17T00:00:00+08:00">2022-08-17</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.6k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="smarts-scalable-multi-agent-reinforcement-learning-training-school-for-autonomous-driving">SMARTS:
Scalable Multi-Agent Reinforcement Learning Training School for
Autonomous Driving</h2>
<p>顾名思义，SMARTS是针对多智能体算法的自动驾驶强化学习仿真平台。</p>
<p>开源了基准任务和代码：https://github.com/huawei-noah/SMARTS</p>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>自动驾驶仿真的挑战之一是天气问题；绝大部分数据是在好天气（fair
weather）下采集的；当前的L4自动驾驶面对复杂的交互情况时。倾向于减速等待，而不是提前主动找办法通过；</p></li>
<li><p>Waymo的California2018的自动驾驶事故数据中, 57%是发生了追尾（rear
endings），29%是发生了侧面碰撞（sideswipes），并且事故都是由于他车造成的，因此说明过于保守的驾驶策略</p></li>
<li><p>waymo的汽车相比人类驾驶员经常过分刹车，导致乘客晕车</p></li>
<li><p>多智能体交互的分级标准：“multi-agent learning levels”, or
“M-levels“</p></li>
<li><p>double
merge道路场景（即&gt;--&lt;形道路）是多智能体交互的难点，车辆需考虑是继续走还是等待；在间隙不够大的时候是否需要变道；其他车开到了自车车道上，是否和它交换位置？等</p></li>
<li><p>平台设计目标：</p>
<ul>
<li><p>Bootstrapping Realistic Interaction</p>
<ul>
<li><ol type="1">
<li>physics,</li>
</ol></li>
<li><ol start="2" type="1">
<li>behavior of road users,</li>
</ol></li>
<li><ol start="3" type="1">
<li>road structure &amp; regula-tions,</li>
</ol></li>
<li><ol start="4" type="1">
<li>background traffic flow.</li>
</ol></li>
</ul></li>
<li><p>Heterogeneous Agent Computing（异构智能体计算）</p></li>
<li><p>Simulation Providers</p></li>
<li><p>Interaction Scenarios</p></li>
<li><p>Distributed Computing</p></li>
</ul></li>
<li><p>Key Features：</p></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220817-1.png"
alt="Levels of multi-agent learning in autonomous driving" />
<figcaption aria-hidden="true">Levels of multi-agent learning in
autonomous driving</figcaption>
</figure>
<ul>
<li>场景：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220817-2.png"
alt="Levels of multi-agent learning in autonomous driving" />
<figcaption aria-hidden="true">Levels of multi-agent learning in
autonomous driving</figcaption>
</figure>
<ul>
<li><p><strong>Observation.</strong> The observation is a stack of three
consecutive frames, which covers the dynamic objects and key events. For
each frame, it contains: 1) relative position of goal; 2) distance to
the center of lane; 3) speed; 4) steering; 5) a list of heading errors;
6) at most eight neighboring vehicles’ driving states (relative
distance, speed and position); 7) a bird’s-eye view gray-scale image
with the agent at the center.</p></li>
<li><p><strong>Action.</strong> The action used here is a
four-dimensional vector of discrete values, for longitudinal
control—keep lane and slow down—and lateral control—turn right and turn
left.</p></li>
<li><p><strong>Reward.</strong> The reward is a weighted sum of the
reward components shaped according to ego vehicle states, interactions
involving surrounding vehicles, and key traffic events. More details can
be found in our implementation code.</p></li>
</ul>
<h3 id="总结">总结</h3>
<p>华为推出的针对多智能体的自动驾驶仿真环境，设计相对灵活，支持场景和他车的编辑，以及Waymo的真实数据，后续可以继续看看</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/12/2022-08-12-Autonomous%20Driving%20-%20An%20Optimistic%20Perspective%20on%20Offline%20Reinforcement%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/2022-08-12-Autonomous%20Driving%20-%20An%20Optimistic%20Perspective%20on%20Offline%20Reinforcement%20Learning/" class="post-title-link" itemprop="url">Autonomous Driving | An Optimistic Perspective on Offline Reinforcement Learning (Google)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.3k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="an-optimistic-perspective-on-offline-reinforcement-learning-google">An
Optimistic Perspective on Offline Reinforcement Learning (Google)</h2>
<p>作者用online DQN在60款 Atari
2600游戏上获取数据样本，然后用这些样本(fixed
dataset)训练offline强化学习算法，一些offline的算法性能可以超过online的算法。本文提出的Random
Ensemble Mixture
(REM)算法在离线回放数据上的表现超过了强的基准算法。因此作者认为在离线样本足够多，多样化充分的情况下，使用鲁棒的RL算法可以获得高质量的策略。</p>
<p>代码： github.com/google-research/batch_rl</p>
<h3 id="introduction">1 Introduction</h3>
<p>离线强化学习的设定是不与真实环境的主动交互，而是通过对收集的离线回放数据中学习策略，在评估模型中生成新的交互数据。相应的情景在以下场景均会面临：</p>
<ul>
<li><strong>robotics</strong>
<ul>
<li>Cabi, S., Colmenarejo, S. G., Novikov, A., Konyushkova, K., Reed,
S., Jeong, R., Zołna, K., Aytar, Y., Budden, D., Vecerik, M., et al. A
framework for data-driven robotics. arXiv preprint arXiv:1909.12200,
2019.</li>
<li>Dasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper,
K., Singh, S., Levine, S., and Finn, C. Robonet: Large-scale multi-robot
learning. CoRL, 2019.</li>
</ul></li>
<li><strong>autonomous driving</strong>
<ul>
<li>Yu, F., Xian, W., Chen, Y., Liu, F., Liao, M., Madhavan, V., and
Darrell, T. Bdd100k: A diverse driving video database with scalable
annotation tooling. CVPR, 2018.</li>
</ul></li>
<li><strong>recommendation systems</strong>
<ul>
<li>Strehl, A. L., Langford, J., Li, L., and Kakade, S. Learning from
logged implicit exploration data. NeurIPS, 2010.</li>
<li>Bottou, L., Peters, J., Quiñonero-Candela, J., Charles, D. X.,
Chickering, D. M., Portugaly, E., Ray, D., Simard, P., and Snelson, E.
Counterfactual reasoning and learning systems: The example of
computational advertising. JMLR, 2013.</li>
</ul></li>
<li><strong>healthcare</strong>
<ul>
<li>Shortreed, S. M., Laber, E., Lizotte, D. J., Stroup, T. S., Pineau,
J., and Murphy, S. A. Informing sequential clinical decisionmaking
through reinforcement learning: an empirical study. Machine learning,
2011.</li>
</ul></li>
</ul>
<p>面对离线强化学习问题时，off-policy算法普遍表现不好，设计大的replay
buffer反而会损害off-policy算法的性能（由于算法的off-policyness）</p>
<ul>
<li>对比的方法包括：
<ul>
<li><strong>offline QR-DQN</strong>：Dabney, W., Rowland, M., Bellemare,
M. G., and Munos, R. Distributional reinforcement learning with quantile
regression. AAAI, 2018.</li>
<li><strong>DQN</strong>：（Nature）</li>
<li><strong>Random Ensemble
Mixture</strong>（REM，随机集成混合）：为本文提出方法</li>
<li><strong>online C51</strong>： 分布式DQN算法。Bellemare, M. G.,
Dabney, W., and Munos, R. A distributional perspective on reinforcement
learning. ICML, 2017.</li>
<li><strong>distributional QR-DQN (SOTA)</strong>：Dabney, W., Rowland,
M., Bellemare, M. G., and Munos, R. Distributional reinforcement
learning with quantile regression. AAAI, 2018</li>
</ul></li>
</ul>
<h3 id="off-policy-reinforcement-learning">2 Off-policy Reinforcement
Learning</h3>
<ul>
<li><p>DQN算法介绍</p>
<ul>
<li>Huber loss：介于MSE和MAE之间的，对数据异常值更不敏感的loss</li>
</ul>
<p><span class="math display">\[
l_{\lambda}(u)=
\begin{align}
\begin{cases}
\frac{1}{2}u^2,&amp;\mid u \mid \leq \lambda\\
\lambda(\mid u\mid-\frac{1}{2}\lambda),&amp; \text{otherwise}
\end{cases}
\end{align}
\]</span></p></li>
<li><p>baseline方法：分布式RL（Distributional RL）</p>
<ul>
<li>C51</li>
<li>QR-DQN</li>
</ul></li>
</ul>
<h3 id="offline-reinforcement-learning">3 Offline Reinforcement
Learning</h3>
<ul>
<li><p>offline的模式分离了模型对经验的利用、生成能力（exploit） vs
探索效率（explore）</p></li>
<li><p>offline RL面临的挑战是<strong>distribution
mismatch</strong>：错误匹配当前使用的策略和固定的离线数据集。例如，当采取了数据集中不存在的行为时，并不知道响应的奖励是多少。<br />
</p></li>
<li><p>本文尝试在不解决distribution
mismatch的基础上，训练高性能的agent</p></li>
</ul>
<h3 id="developing-robust-offline-rl-algorithms">4 Developing Robust
Offline RL Algorithms</h3>
<ul>
<li>采用集成（Ensemble）可以提高模型的泛化能力，本文使用了Ensemble
DQN和REM两个采取该思想的方法。</li>
</ul>
<h4 id="ensemble-dqn">4.1 Ensemble-DQN</h4>
<ul>
<li><p>该方法是对DQN算法的简单扩展，使用集成多个参数化的Q函数来近视Q值。</p>
<ul>
<li>Faußer, S. and Schwenker, F. Neural network ensembles in
reinforcement learning. Neural Processing Letters, 2015<br />
</li>
<li>Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep
exploration via bootstrapped DQN. NeurIPS, 2016.<br />
</li>
<li>Anschel, O., Baram, N., and Shimkin, N. Averaged-dqn: Variance
reduction and stabilization for deep reinforcement learning. ICML,
2017.</li>
</ul></li>
<li><p>每个参数化Q函数的优化目标是近似真实的Q值，参考下面这篇文章：</p>
<ul>
<li><strong>Bootstrapped-DQN</strong>：Osband, I., Blundell, C.,
Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped DQN.
NeurIPS, 2016.</li>
</ul></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220812-2.png"
alt="损失函数" />
<figcaption aria-hidden="true">损失函数</figcaption>
</figure>
<p>其中，<span
class="math display">\[l_{\lambda}\]</span>是Huber损失。算法使用所有Q函数的均值作为输出。</p></li>
</ul>
<h4 id="random-ensemble-mixture-rem">4.2 Random Ensemble Mixture
(REM)</h4>
<ul>
<li>引入了dropout：
<ul>
<li>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and
Salakhutdinov, R. Dropout: a simple way to prevent neural networks from
overfitting. JMLR, 2014</li>
</ul></li>
<li>不同于Ensemble-DQN，REM构造一个包含多个Q函数近似器的凸组合（convex
combination），将多个Q 函数近似器作为1个近似器使用。使用(K −
1)-simplex计算混合的概率。</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220812-3.png"
alt="模型结构" />
<figcaption aria-hidden="true">模型结构</figcaption>
</figure>
<ul>
<li><p>对于每个mini-batch，随机产生一个分类分布（categorical
distribution）<span
class="math display">\[\alpha\]</span>，它定义了一个逼近最优Q-函数的K个估计器的凸组合</p></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220817-3.png"
alt="REM的loss形式" />
<figcaption aria-hidden="true">REM的loss形式</figcaption>
</figure>
<p>其中，<span
class="math display">\[P_{\Delta}\]</span>表示标准的(K-1)-simplex的概率分布，<span
class="math display">\[\Delta^{K-1}=\{\alpha\in R^K:
\alpha_1+\alpha_2+...+\alpha_K=1,\alpha_k\geq0,k=1,...,K
\}\]</span></p></li>
<li><p>对于<span class="math display">\[P_{\Delta}\]</span>：先从Uniform
(0,1)分布中采样K个独立同分布的值，然后归一化它们获得有效的分类分布（<span
class="math display">\[a&#39;_k \sim U(0,1),a_k=a&#39;_k/\sum_k
a&#39;_i\]</span>）</p></li>
<li><p>对于Q值的求解，使用<span
class="math display">\[Q(s,a)=\frac{1}{K}\sum_k Q_{\theta}^k
(s,a)\]</span></p></li>
</ul>
<h3 id="offline-rl-on-atari-2600-games">5 Offline RL on Atari 2600
Games</h3>
<ul>
<li>将Nature DQN在60个Atari游戏上的行为数据用来构建DQN
replay数据集，每个游戏2亿帧（200 million frames）</li>
<li>每个游戏训练5个智能体，因此60个游戏一共有60个数据集</li>
<li>ofline RL算法性能对比：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220812-1.png"
alt="Offline RL on Atari 2600." />
<figcaption aria-hidden="true">Offline RL on Atari 2600.</figcaption>
</figure>
<ul>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220818-1.png"
alt="Offline QR-DQN vs. DQN (Nature)" />
<figcaption aria-hidden="true">Offline QR-DQN vs. DQN
(Nature)</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220818-2.png"
alt="Offline Agents on DQN Replay Dataset" />
<figcaption aria-hidden="true">Offline Agents on DQN Replay
Dataset</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220818-3.png"
alt="Asymptotic performance of offline agents" />
<figcaption aria-hidden="true">Asymptotic performance of offline
agents</figcaption>
</figure></li>
<li><p>文中提到offline连续强化学习方法，实验了offline DDPG，offline
TD3，offline BCQ等算法</p>
<ul>
<li><p>Fujimoto, S., Meger, D., and Precup, D. Off-policy deep
reinforcement learning without exploration. ICML, 2019b.</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220818-4.png"
alt="Asymptotic performance of offline agents" />
<figcaption aria-hidden="true">Asymptotic performance of offline
agents</figcaption>
</figure></li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<p>总的来说，文章本身目的更倾向于提供一种乐观的横向对比，证明offline
RL在一些情况下可以获取SOTA的性能，甚至超过online RL。提供的几个offline
Q-learning变体和offline连续情况的RL算法可以再看看。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/04/2022-08-04-Autonomous%20Driving%20-%20UMBRELLA%20Uncertainty-Aware%20Model-Based%20Offline%20Reinforcement%20Learning%20Leveraging%20Planning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/04/2022-08-04-Autonomous%20Driving%20-%20UMBRELLA%20Uncertainty-Aware%20Model-Based%20Offline%20Reinforcement%20Learning%20Leveraging%20Planning/" class="post-title-link" itemprop="url">Autonomous Driving | UMBRELLA: Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging Planning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-04 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-04T00:00:00+08:00">2022-08-04</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>5.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="umbrella-uncertainty-aware-model-based-offline-reinforcement-learning-leveraging-planning">UMBRELLA:
Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging
Planning</h2>
<ul>
<li>在学习过程中考虑了随机不确定性的影响，提高了模型的可迁移性和可解释性</li>
</ul>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>目前的模型考虑到多智能体之间的交互和复杂行为，多采用工程设计的驾驶策略，但这样难以适用更复杂的任务。强化学习通过试错学习的方式避免了这些手工设计，但需要大量的试错机会。相比于模仿学习习得次优行为，强化学习可以提升不同类型数据的质量</p></li>
<li><p>主要贡献：</p>
<ul>
<li>提出了一个基于模型的离线规划算法UMBRELLA，在观测上考虑了认知和偶然的不确定性（epistemic
和 aleatoric）<br />
</li>
<li>引入了一个消融实验，优化最坏情况模型</li>
<li>实验中，在稠密车流（with dense
traffic）的城市和高速场景中，胜过了BC行为克隆方法和MBOP算法</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<ul>
<li><p><strong>Model-based Offline Reinforcement Learning</strong>：</p>
<p>其主要的问题是the distributional
shift。MOReL和MBOP方法将动态模型的认知不确定性估计结合到奖励函数中，以惩罚未被行为分布覆盖的状态</p>
<ul>
<li>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline
reinforcement learning: Tutorial, review, and perspectives on open
problems. CoRR, 2020. arXiv:2005.01643.</li>
</ul>
<p>这些方法在多智能体环境中测试，没考虑到由行人和其它车辆行为造成的偶然不确定性（aleatoric
uncertainty）的影响。[Henaff et al.,
2019]通过由条件变分自动编码器（conditional variational
autoencoder，CVAE）表示的随机动力学模型解决了这个问题[Kingma and
Welling,
2014]。然而，他们的方法依赖于策略学习，这与基于模型的离线规划相比，可解释性和控制灵活性有所降低。</p>
<ul>
<li>Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive
policy learning with uncertainty regularization for driving in dense
traffic. In International Conference on Learning Representations,
\2019.<br />
</li>
<li>Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes.
In International Conference on Learning Representations, 2014.</li>
</ul></li>
<li><p><strong>Interaction-aware Motion Prediction and
Planning</strong>：</p>
<p>单纯通过规划安全的pass进行自动驾驶忽略了车辆之间包含规划和预测（planning
and prediction）的行为交互</p>
<p>引入博弈论的方法考虑了多智能体的动力学过程，但带来了计算上的开销。</p>
<p>一些基于学习的模型可以生成更多的场景，但是没有考虑认知不确定性</p>
<ul>
<li>Jerry Liu, Wenyuan Zeng, Raquel Urtasun, and Ersin Yumer. Deep
structured reactive planning. In IEEE International Conference on
Robotics and Automation, 2021.<br />
</li>
<li>Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A. Wright,
Rowan McAllister, Joseph E. Gonzalez, and Sergey Levine. Contingencies
from observations: Tractable contingency planning with learned behavior
models. In IEEE International Conference on Robotics and Automation,
2021.</li>
</ul></li>
</ul>
<h3 id="the-umbrella-framework">3 The UMBRELLA Framework</h3>
<ul>
<li><p>UMBRELLA 是 MBOP 算法的扩展</p>
<ul>
<li>Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline
planning. In International Conference on Learning Representations,
2021.</li>
</ul></li>
<li><p>问题形式化，用MDP的<span class="math display">\[(S; A; p; r;
\gamma)\]</span>
表示，在offline设定中，智能体不与环境直接交互，而是从数据集中学习策略<span
class="math display">\[\pi_d\]</span>。如果观测值<span
class="math display">\[O\]</span>并不是完全可得，那么问题就变化为（partially
observable MDP，POMDP），使用 <span
class="math display">\[M_{PO}=(S;A;O;p;r;\gamma)\]</span>，处理该问题的常用方法是使用nth-order
history
方法，可以近似得到状态估计，然后将其转变为MDP，用标准的RL方法处理。</p></li>
<li><p>UMBRELLA使用连续的潜在变量（continuous latent variable）<span
class="math display">\[z_t\in Z\]</span>
，并枚举自车所有的可能行为（行为采样通过学到的BC
policy）。预测N条长度界限为H的轨迹。还使用了一个return-weighted
trajectory optimizer，处理POMDP问题，用持续观测状态从<span
class="math display">\[o_{t-_c:t}\]</span>直到时间步t估计缺失的观测状态。（相关解释参考VAE算法）</p>
<ul>
<li><span
class="math display">\[z_t=\mu_{\phi}+\sigma_{\phi}*\epsilon\]</span></li>
<li><span
class="math display">\[(\mu_{\phi},\sigma_{\phi})=q_{\phi}(s_t,s_{t+1})\]</span></li>
<li><span class="math display">\[\epsilon \sim
\mathcal{N}(0,1)\]</span></li>
</ul></li>
<li><p>随机动力学模型：</p>
<ul>
<li><p>为了建模不同的未来可能情况，学习随机前向动力学模型<span
class="math display">\[f_{m,\theta}:S\times A\times Z \rightarrow S
\times \mathbb{R}\]</span></p></li>
<li><p>该模型采用CVAE架构，预测下一时刻的状态</p>
<ul>
<li>Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes.
In International Conference on Learning Representations, 2014.</li>
</ul></li>
<li><p><span
class="math display">\[\hat{s}_{t+1}=f_m(s_t,a_t,z_t)_s\]</span></p></li>
<li><p><span
class="math display">\[\hat{r}_{t}=f_m(s_t,a_t,z_t)_r\]</span></p></li>
<li><p>其中潜在变量<span
class="math display">\[z_t\]</span>建模了不同的未来预测，并确保了输出对于输入是非确定性的。在训练过程中，该潜在变量从后验分布<span
class="math display">\[q_{\phi}(z\mid s_t,s_{t+1})\]</span>中采样，<span
class="math display">\[\phi\]</span>是参数。由于实际采样只能从先验分布中采，因此使用Kullback-Leibler
(KL) divergence度量后验分布和先验分布<span
class="math display">\[p(z)\]</span>并最小化距离。</p>
<ul>
<li>潜变量用于区分细分情况，精细化建模</li>
</ul></li>
<li><p>定义Evidence Lower BOund (ELBO)目标训练VAEs。</p>
<ul>
<li>Evidence Lower BOund： https://zhuanlan.zhihu.com/p/400322786</li>
</ul></li>
<li><p>损失函数为： <span class="math display">\[
L(\theta,\phi;s_t,s_{t+1},a_t,r_t)=\mid\mid s_{t+1} -
f_{m,\theta}(s_t,a_t,z_t)_s\mid\mid_2^2 + \mid\mid
r_t-f_{m,\theta}(s_t,a_t,z_t)_r\mid\mid^2_2 \\
\zeta D_{KL}(q_{\phi}(z_t\mid s_t,s_{t+1})\mid\mid p(z_t))
\]</span></p></li>
<li><p>BC policies <span class="math display">\[f_{b,\psi}:S\times
A^{n_c} \rightarrow A \]</span> ，<span
class="math display">\[f_b(s_t,a_{(t-n_c):(t-1)})\]</span>，该函数使用当前的状态和<span
class="math display">\[n_c\]</span>个之前的行为作为输入，输出行为<span
class="math display">\[a_t\]</span>。通过将之前的行为串联考虑，可以使得输出的行为更加平滑。</p></li>
<li><p>训练BC Policy使用最小化损失函数：</p>
<p><span
class="math display">\[L(\psi;s_t,a_{(t-n_c):(t-1)},a_t)=\mid\mid a_t -
f_{b,\psi}(s_t,a_{(t-n_c):(t-1)})\mid\mid^2_2\]</span></p></li>
<li><p>截断价值函数<span class="math display">\[f_{R,\xi}:S\times
A^{n_c}\rightarrow \mathbb{R}\]</span> 估计H个时间步后的期望回报<span
class="math display">\[\hat{R}_H\]</span></p></li>
</ul></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-3.png"
alt="Simplified network architecture of the stochastic forward dynamics model during training" />
<figcaption aria-hidden="true">Simplified network architecture of the
stochastic forward dynamics model during training</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-4.png"
alt="Behavior-cloned policy network architecture" />
<figcaption aria-hidden="true">Behavior-cloned policy network
architecture</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-5.png"
alt="Truncated value function network architecture" />
<figcaption aria-hidden="true">Truncated value function network
architecture</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-6.png"
alt="Signal flow of the stochastic forward dynamics model" />
<figcaption aria-hidden="true">Signal flow of the stochastic forward
dynamics model</figcaption>
</figure></li>
<li><p><strong>UMBRELLA-Planning</strong>：采用了MPC，在每一步规划未来H步的最优控制轨迹，然后只执行第一步行为，之后再次进行规划，并不断迭代。通过这样的迭代优化可以降低模型误差带来的影响。</p></li>
<li><p><strong>UMBRELLA Trajectory Optimizer</strong>：</p>
<p>最终的轨迹输出使用MPPI framework</p>
<ul>
<li>Grady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model
predictive path integral control: From theory to parallel computation.
Journal of Guidance, Control, and Dynamics, 40(2): 344–357, 2017.</li>
</ul></li>
<li><p>最后，模型通过re-weighting获得最优轨迹：</p></li>
</ul>
<p><span class="math display">\[
T^*_t = \frac{\sum_{n=1}^N e^{kR_n} A_{n,t+1}}{\sum_{n=1}^N e^{kR_n}}
\]</span></p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-1.png"
alt="UMBRELLA Planning" />
<figcaption aria-hidden="true">UMBRELLA Planning</figcaption>
</figure>
<ul>
<li><strong>Pessimistic Trajectory Optimizer</strong>：
面对认知的不确定性，UMBRELLA-P对最坏情况的结果进行优化，并采取悲观的行动</li>
</ul>
<h3 id="experimental-evaluation">4 Experimental Evaluation</h3>
<ul>
<li>环境：
<ul>
<li><strong>NGSIM</strong>：多智能体自动驾驶环境<br />
</li>
<li><strong>CARLA</strong>：城市多智能体自动驾驶场景</li>
</ul></li>
<li>基线方法：
<ul>
<li><strong>1-step IL</strong>：模仿专家行为的BC policy</li>
<li><strong>MBOP</strong>：当前最好的基于模型的离线RL方法
<ul>
<li>Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline
planning. In International Conference on Learning Representations,
2021.<br />
</li>
</ul></li>
<li><strong>MPUR</strong>：最好的基于模型策略学习方法
<ul>
<li>Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive
policy learning with uncertainty regularization for driving in dense
traffic. In International Conference on Learning Representations,
2019.<br />
</li>
</ul></li>
</ul></li>
<li>指标：
<ul>
<li>Success rate (SR)：The rate of collision-free
episodes，并要求在时间内到达目标位置</li>
<li>Mean distance (MD)：NGSIM中的纵向行驶距离</li>
<li>mean successful time (MST)<br />
</li>
<li>mean proximity reward <span
class="math display">\[\overline{r}_{prox}\]</span></li>
<li>mean lane reward <span
class="math display">\[\overline{r}_{lane}\]</span><br />
</li>
<li>mean final reward <span
class="math display">\[\overline{r}\]</span><br />
</li>
</ul></li>
<li>实验结果：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-2.png"
alt="性能对比" />
<figcaption aria-hidden="true">性能对比</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>奖励函数误设计仍然是一个较大的问题，也是自动驾驶策略不像人的原因之一。</p>
<ul>
<li>W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt,
and Peter Stone. Reward (mis)design for autonomous driving. 2021.
arxiv:2104.13906.</li>
</ul>
<p>模型考虑偶然不确定性：使用潜变量<span
class="math display">\[z\]</span>，枚举可能的轨迹</p>
<p>模型考虑认知不确定性：在reward中进行re-weighting和使用参数<span
class="math display">\[\beta\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/03/2022-08-03-Autonomous%20Driving%20-%20Urban%20Driver%20Learning%20to%20Drive%20from%20Real-world%20Demonstrations%20Using%20Policy%20Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/03/2022-08-03-Autonomous%20Driving%20-%20Urban%20Driver%20Learning%20to%20Drive%20from%20Real-world%20Demonstrations%20Using%20Policy%20Gradients/" class="post-title-link" itemprop="url">Autonomous Driving | Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-03 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-03T00:00:00+08:00">2022-08-03</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="urban-driver-learning-to-drive-from-real-world-demonstrations-using-policy-gradients">Urban
Driver: Learning to Drive from Real-world Demonstrations Using Policy
Gradients</h2>
<ul>
<li>取得了城市驾驶场景中最好的效果（Urban driving scenarios）</li>
<li>数据：使用100小时的城市道路专家示教数据</li>
<li>不必添加复杂的状态扰动；</li>
<li>不必在训练中收集额外的同策略数据；</li>
</ul>
<h3 id="introduction">Introduction</h3>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220803-1.png"
alt="本文的闭环训练算法概览" />
<figcaption aria-hidden="true">本文的闭环训练算法概览</figcaption>
</figure>
<ul>
<li>工业界最好轨迹规划器文献：
<ul>
<li>H. Fan, F. Zhu, C. Liu, L. Zhang, L. Zhuang, D. Li, W. Zhu, J. Hu,
H. Li, and Q. Kong. Baidu apollo em motion planner. ArXiv, 2018.</li>
</ul></li>
<li>本文主要贡献：
<ul>
<li>复杂城市驾驶场景中，第一个证明了用策略梯度学习，可以从大量真实世界演示数据中学习模仿驾驶策略；</li>
<li>一个新的可微分仿真器，可基于过去的数据进行闭环仿真，并通过时间的反向传播计算策略梯度，实现快速学习；</li>
<li>单纯在仿真器中训练可在真实世界中控制自动驾驶车辆，优于其他方法；</li>
<li>源码可得：https://planning.l5kit.org.</li>
</ul></li>
</ul>
<h3 id="related-work">Related work</h3>
<ul>
<li><p><strong>Trajectory-based optimization</strong>：</p>
<ul>
<li><p>这是当前工业界的主流方法（a dominant approach）</p></li>
<li><p>依赖手工定义的损失和奖励</p></li>
<li><p>损失的优化可结合一系列经典的算法：</p>
<ul>
<li>A* [11]</li>
<li>RRTs [12]</li>
<li>POMDP with solver [13]</li>
<li>dynamic programming [14]<br />
</li>
</ul></li>
<li><p>整体上是依赖human engineering，而不是数据驱动</p></li>
</ul></li>
<li><p><strong>Reinforcement learning（RL）</strong>：</p>
<ul>
<li>依赖仿真器的构造、精确编码和优化的奖励信号
<ul>
<li>S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent,
reinforcement learning for autonomous driving. ArXiv, 2016.<br />
</li>
</ul></li>
<li>手工编程的仿真器，不能还原真实的长尾场景</li>
<li>本文直接通过 mid-level representations 从真实世界的 log
中构建仿真环境</li>
</ul></li>
<li><p><strong>Imitation learning (IL) and Inverse Reinforcement
Learning (IRL)</strong>：</p>
<ul>
<li>原始的行为克隆（Naive behavioral
cloning）面临协变量偏移问题（covariate shift）</li>
<li>Adversarial Imitation Learning [31, 32,
33]，还没有在自动驾驶场景使用</li>
</ul></li>
<li><p><strong>Neural Motion Planners</strong>：</p>
<ul>
<li>在[34]中，原始感觉输入和高清地图被用于估计未来可能的SDV位置的成本量。基于这些成本量，可以对轨迹进行采样，并且选择最低成本的轨迹来执行。这些方法目前没有在实车测试。
<ul>
<li>W. Zeng, W. Luo, S. Suo, A. Sadat, B. Yang, S. Casas, and R.
Urtasun. End-to-end interpretable neural motion planner. Int. Conference
on Computer Vision and Pattern Recognition (CVPR), \2019.<br />
</li>
<li>S. Casas, A. Sadat, and R. Urtasun. Mp3: A unified model to map,
perceive, predict and plan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 14403–14412, 2021.</li>
</ul></li>
</ul></li>
<li><p><strong>Mid-representations and the availability of large-scale
real-world AD datasets</strong>：</p>
<ul>
<li>J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V.
Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving
motion prediction dataset. Conference on Robot Learning (CoRL),
2020.</li>
<li>M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
P. C. De Wang, S. Lucey, D. Ramanan, and J. Hays. Argoverse: 3d tracking
and forecasting with rich maps supplementary material. Int. Conf. on
Computer Vision and Pattern Recognition (CVPR).<br />
</li>
<li>state-of-the-art solutions for motion forecasting [8, 9]
<ul>
<li>[8] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C.
Schmid. Vectornet: Encoding hd maps and agent dynamics from vectorized
representation. In Int. Conf. on Computer Vision and Pattern Recognition
(CVPR), 2020.</li>
<li>[9] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R.
Urtasun. Learning lane graph representations for motion forecasting.
2020.<br />
</li>
</ul></li>
</ul></li>
<li><p><strong>Data-driven simulation</strong>：</p>
<ul>
<li>[23] created a photo-realistic simulator for training an end-to-end
RL policy.</li>
<li>[5] simulated a bird’s-eye view of dense traffic on a highway.</li>
<li>Finally, two recent works [39, 40] developed data-driven simulators
and showed their usefulness for training and validating ML
planners.</li>
</ul></li>
</ul>
<h3
id="differentiable-traffic-simulator-from-real-world-driving-data">Differentiable
Traffic Simulator from Real-world Driving Data</h3>
<ul>
<li>真实世界的经验轨迹：<span
class="math display">\[\overline{\tau}=\{\overline{s}_1,\overline{s}_2,...,\overline{s}_T\}\]</span></li>
<li>仿真的目标是迭代地生成观测状态序列<span
class="math display">\[\tau=\{s_1,s_2,...,s_T\}\]</span>，然后计算车辆轨迹<span
class="math display">\[p_t\]</span>，包括<span
class="math display">\[(x;y;\theta)\]</span></li>
<li><span class="math display">\[s_{t+1}=S(s_t,a_t)\]</span>，<span
class="math display">\[p_{t+1}=f(p_t,a_t)\]</span></li>
</ul>
<h3 id="imitation-learning-using-a-differentiable-simulator">Imitation
Learning Using a Differentiable Simulator</h3>
<ul>
<li><span class="math display">\[L(s_t,a_t)=\mid\mid \overline{p}_t -
p_t\mid\mid_1\]</span></li>
<li><span
class="math display">\[J(\pi)=\mathbb{E}_{\overline{\tau}\sim\pi_E}\mathbb{E}_{\tau\sim\pi}
\sum_{t}\gamma^t L(s_t,a_t)\]</span>，<span
class="math display">\[\pi_E\]</span>是专家策略，<span
class="math display">\[\pi\]</span>是模型的策略，希望两个策略接近</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220803-2.png"
alt="Imitation learning from expert demonstrations" />
<figcaption aria-hidden="true">Imitation learning from expert
demonstrations</figcaption>
</figure>
<blockquote>
<p>P.S.：由于轨迹的开始阶段均来自专家策略，会引入bias，在策略更新的时候，在运动开始的第K步之后才计算梯度，以此避免bias</p>
</blockquote>
<ul>
<li><p>策略梯度的计算（用下标表示偏微分，<span
class="math display">\[\theta\]</span>是策略参数）： <span
class="math display">\[
J_s^t = L_s+L_a\pi_s+\gamma J^{t+1}_{\theta}(S_s+S_a\pi_{s})\\
J_{\theta}^t =
L_a\pi_{\theta}+\gamma(J_s^{t+1}S_a\pi_{\theta}+J_{\theta}^{t+1})
\]</span> &gt; Ref: N. Heess, G. Wayne, D. Silver, T. Lillicrap, T.
Erez, and Y. Tassa. Learning continuous control policies by stochastic
value gradients. In Advances in Neural Information Processing Systems,
\2015.<br />
### Experiments</p></li>
<li><p>Lyft Motion Prediction Dataset
[6]：数据采集自加利福尼亚州帕洛阿尔托的复杂城市路线。数据集捕捉各种真实世界的情况，例如在多车道交通中驾驶、转弯、在十字路口与车辆互动等。</p>
<ul>
<li>J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V.
Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving
motion prediction dataset. Conference on Robot Learning (CoRL),
2020.<br />
</li>
</ul></li>
<li><p>模型在100小时子集上训练，并在25小时子集上测试。</p></li>
<li><p>three state-of-the-art baselines：</p>
<ul>
<li>Naive Behavioral Cloning (BC)<br />
</li>
<li>Behavioral Cloning + Perturbations (BC-perturb)
<ul>
<li>M. Bansal, A. Krizhevsky, and A. Ogale. Chauffeurnet: Learning to
drive by imitating the best and synthesizing the worst. 12 2018.<br />
</li>
</ul></li>
<li>Multi-step Prediction (MS Prediction)
<ul>
<li>A. Venkatraman, M. Hebert, and J. Bagnell. Improving multi-step
prediction of learned time series models. In AAAI, 2015.</li>
</ul></li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220803-3.png"
alt="性能对比" />
<figcaption aria-hidden="true">性能对比</figcaption>
</figure>
<blockquote>
<p>指标值越小越好，本文模型取得最好的表现以及最低的l1K指标（综合其它指标，每1000英里干预次数）</p>
</blockquote>
<ul>
<li>评价指标：
<ul>
<li><strong>L2</strong>: L2 distance to the underlying expert position
in the driving log in meters.</li>
<li><strong>Off-road events</strong>: we report a failure if the planner
deviates more than 2m laterally from the reference trajectory – this
captures events such as running off-road and into opposing traffic.</li>
<li><strong>Collisions</strong>: collisions of the SDV with any other
agent, broken down into front, side and rear collisions w.r.t. the
SDV.</li>
<li><strong>Comfort</strong>: we monitor the absolute value of
acceleration, and raise a failure should this exceed 3 m/s2.</li>
<li><strong>I1K</strong>: we accumulate safety-critical failures
(collisions and off-road events) into one key metric for ease of
comparison, namely Interventions per 1000 Miles (I1K)</li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220803-4.png"
alt="仿真结果" />
<figcaption aria-hidden="true">仿真结果</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>策略梯度的推导部分可以继续看看，本文有仿真和实车实验，但方法对比上，对其它算法进行了修改，因此并不完整。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/01/2022-08-01-Autonomous%20Driving%20-%20DriverGym%20Democratising%20Reinforcement%20Learning%20for%20Autonomous%20Driving/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/01/2022-08-01-Autonomous%20Driving%20-%20DriverGym%20Democratising%20Reinforcement%20Learning%20for%20Autonomous%20Driving/" class="post-title-link" itemprop="url">Autonomous Driving | DriverGym Democratising Reinforcement Learning for Autonomous Driving</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-01 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-01T00:00:00+08:00">2022-08-01</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="drivergym-democratising-reinforcement-learning-for-autonomous-driving">DriverGym:
Democratising Reinforcement Learning for Autonomous Driving</h2>
<ul>
<li>现状：目前缺少open-source platform来用real-world
data训练和高效地验证RL算法。</li>
<li>DriverGym 平台的特点：
<ul>
<li>开源（opensource）</li>
<li>与Gym兼容（OpenAI Gym-compatible environment specifically
tailored）</li>
<li>超过1000小时专家记录数据（more than 1000 hours of expert logged
data）<br />
</li>
<li>灵活的闭环评估协议（flexible closed-loop evaluation protocol）</li>
<li>提供行为克隆baselines（provide behavior cloning baselines using
supervised learning and RL）<br />
</li>
<li>代码：https://lyft.github.io/l5kit/ （已失效）</li>
</ul></li>
<li>环境结构：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220801-2.png"
alt="DriverGym" />
<figcaption aria-hidden="true">DriverGym</figcaption>
</figure>
<blockquote>
<p>DriverGym: an open-source gym environment that enables training RL
driving policies on real-world data. The RL policy can access rich
semantic maps to control the ego (<strong>red</strong>). Other agents
(<strong>blue</strong>) can either be simulated from the data logs or
controlled using a dedicated policy pre-trained on real-world data. We
provide an extensible evaluation system (<strong>purple</strong>) with
easily configurable metrics to evaluate the idiosyncrasies of the
trained policies.</p>
</blockquote>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li>自动驾驶开源RL仿真环境的对比</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220801-3.png"
alt="自动驾驶开源RL仿真环境的对比" />
<figcaption aria-hidden="true">自动驾驶开源RL仿真环境的对比</figcaption>
</figure>
<ul>
<li><p>对仿真平台的几个需求：</p>
<ul>
<li><ol type="1">
<li>易于训练RL策略；be used to easily train RL policies using real-world
logs,</li>
</ol></li>
<li><ol start="2" type="1">
<li>可仿真实际的和反应式的周围智能体行为反应；simulate surrounding agent
behavior that is both realistic and reactive to the ego policy,</li>
</ol></li>
<li><ol start="3" type="1">
<li>可高效评估模型；effectively evaluate the trained models,</li>
</ol></li>
<li><ol start="4" type="1">
<li>设计灵活可调；be flexible in its design,</li>
</ol></li>
<li><ol start="5" type="1">
<li>包容整个相关研究社区；inclusive to the entire research
community,</li>
</ol></li>
</ul></li>
<li><p>使用了最大的公开数据集：Level 5 Prediction Dataset</p>
<ul>
<li>J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V.
Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving
motion prediction dataset. https://level-5.global/level5/data/,
2020.<br />
</li>
</ul></li>
<li><p>支持反应式的行为仿真</p>
<ul>
<li>Luca Bergamini, Y. Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del
Pero, Blazej Osinski, Hugo Grimmett, and Peter Ondruska. Simnet:
Learning reactive self-driving simulations from real-world observations.
ArXiv, abs/2105.12332, 2021.</li>
</ul></li>
<li><p>闭环评估系统中提供了自动驾驶相关的评估指标，指标支持扩展和合并，应用于策略的训练</p></li>
<li><p>主要贡献：</p>
<ul>
<li>An open-source and OpenAI gym-compatible environment for autonomous
driving task;</li>
<li>Support for more than 1000 hours of real-world expert data;</li>
<li>Support for logged agents replay or data-driven realistic agent
trajectory simulations;</li>
<li>Configurable and extensible evaluation protocol;</li>
<li>Provide pre-trained models and the corresponding reproducible
training code.</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<ul>
<li><strong>赛车仿真环境（Racing simulators）：</strong>
<ul>
<li><strong>TORCS</strong>：提供了受限的驾驶场景
<ul>
<li>E. Espié, Christophe Guionneau, Bernhard Wymann, Christos
Dimitrakakis, Rémi Coulom, and Andrew Sumner. Torcs, the open racing car
simulator. 2005.<br />
</li>
</ul></li>
<li><strong>Highway-Env</strong>：环境与Gym兼容，但缺少交通灯、评估协议和专家数据
<ul>
<li>Edouard Leurent. An environment for autonomous driving
decision-making. https://github.com/eleurent/highway-env, 2018.<br />
</li>
</ul></li>
</ul></li>
<li><strong>交通仿真环境（Traffic simulators）</strong>：
<ul>
<li><strong>CARLA</strong>：支持多变的交通情况训练和测试，但周围车辆使用手写规则（hand-coded
rules），真实性有限。
<ul>
<li>A. Dosovitskiy, G. Ros, Felipe Codevilla, Antonio M. López, and V.
Koltun. Carla: An open urban driving simulator. ArXiv, abs/1711.03938,
2017.<br />
</li>
</ul></li>
<li><strong>SUMO</strong>：支持多变的交通情况训练和测试，但周围车辆使用手写规则（hand-coded
rules），真实性有限。
<ul>
<li>Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob
Erdmann, Yun-Pang Flötteröd, Robert Hilbrich, Leonhard Lücken, Johannes
Rummel, Peter Wagner, and Evamarie Wießner. Microscopic traffic
simulation using sumo. In The 21st IEEE International Conference on
Intelligent Transportation Systems. IEEE, 2018. URL
https://elib.dlr.de/124092/.<br />
</li>
</ul></li>
<li><strong>SMARTS</strong>
：有<code>Social Agent Zoo</code>支持数据驱动周围车辆行为。
<ul>
<li>Ming Zhou, Jun Luo, Julian Villela, Yaodong Yang, David Rusu, Jiayu
Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, Aurora
Chongxi Huang, Ying Wen, Kimia Hassanzadeh, Daniel Graves, Dong Chen,
Zhengbang Zhu, Nhat M. Nguyen, Mohamed Elsayed, Kun Shao, Sanjeevan
Ahilan, Baokuan Zhang, Jiannan Wu, Zhengang Fu, Kasra Rezaee, Peyman
Yadmellat, Mohsen Rohani, Nicolas Perez Nieves, Yihan Ni, Seyedershad
Banijamali, Alexander Cowen Rivers, Zheng Tian, Daniel Palenicek,
Haitham Ammar, Hongbo Zhang, Wulong Liu, Jianye Hao, and Jintao Wang.
Smarts: Scalable multi-agent reinforcement learning training school for
autonomous driving. ArXiv, abs/2010.09776, 2020.<br />
</li>
</ul></li>
<li><strong>CRTS</strong>：提供了logs数据接口，使用64小时的真实驾驶数据（real-world
logs）训练周围车辆的行为。集成在Carla中。
<ul>
<li>Blazej Osinski, Piotr Milos, Adam Jakubowski, Pawel Ziecina, Michal
Martyniak, Christopher Galias, Antonia Breuer, Silviu Homoceanu, and
Henryk Michalewski. Carla real traffic scenarios - novel training ground
and benchmark for autonomous driving. ArXiv, abs/2012.11329, 2020.<br />
</li>
</ul></li>
<li><strong>DriverGym</strong>：支持反应式的agent使用数据驱动模型学习真实世界的数据，并提供了1000小时真实的驾驶记录可用于仿真agent</li>
</ul></li>
</ul>
<h3 id="drivergym">3 DriverGym</h3>
<ul>
<li><p>模型兼容两个流行的框架训练策略：</p>
<ul>
<li><strong>SB3</strong>：<a
target="_blank" rel="noopener" href="https://github.com/DLR-RM/stable-baselines3">Stable Baselines3
(SB3)</a>是 PyTorch 中强化学习算法的一组可靠实现。它是<a
target="_blank" rel="noopener" href="https://github.com/hill-a/stable-baselines">Stable
Baselines</a>的下一个主要版本。
<ul>
<li>Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi
Kanervisto, and Noah Dormann. Stable baselines3.
https://github.com/DLR-RM/stable-baselines3, 2019.<br />
</li>
</ul></li>
<li><strong>RLlib</strong>：RLlib是一个开源强化学习库,提供了高度可扩展能力和不同应用的统一的<a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=API&amp;spm=1001.2101.3001.7020">API</a>。RLlib原生支持Tensorflow，Tensorflow
Eager，以及PyTorch，但其内部与这些框架无关。
<ul>
<li>Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox,
Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica.
Rllib: Abstractions for distributed reinforcement learning. In ICML,
2018.</li>
</ul></li>
</ul></li>
<li><p>例子场景：绿线是策略预测的轨迹</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220801-4.png"
alt="Visualization of an episode rollout (ego in red, agents in blue) in DriverGym. The policy prediction (green line) is scaled by factor of 10 and shown at 2 second intervals for better viewing" />
<figcaption aria-hidden="true">Visualization of an episode rollout (ego
in red, agents in blue) in DriverGym. The policy prediction (green line)
is scaled by factor of 10 and shown at 2 second intervals for better
viewing</figcaption>
</figure></li>
<li><p>action space：行为为 <span class="math display">\[(x; y;
yaw)\]</span> (yaw是朝向)，用于更新ego-agent行为；环境输出 <span
class="math display">\[(acceleration; steer)\]</span>
用于计算下一时刻的observation</p></li>
<li><p>reactive agent：允许周围车辆使用两种方式控制：</p>
<ul>
<li>log replay：回放真实世界中收集的数据；</li>
<li>reactive simulation：可使用真实世界数据训练neural-network-based
agent models，用于控制车辆行为</li>
</ul></li>
<li><p>reward：支持不同的自动驾驶评价指标，在每一帧进行评价计算，指标可以整合为奖励函数。</p></li>
</ul>
<h3 id="总结">总结</h3>
<p>整体来看，支持Gym环境大大方便了仿真和调试，一些细节问题由于没有实际使用该环境还不清楚，比如车辆密度、速度、周围车辆的观测质量、轨迹质量等。</p>
<p>本文作者正对更多细粒度场景设计评估方案，例如或绿灯路口重新启动等。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/01/2022-08-01-Autonomous%20Driving%20-%20Model-based%20offline%20planning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/01/2022-08-01-Autonomous%20Driving%20-%20Model-based%20offline%20planning/" class="post-title-link" itemprop="url">Autonomous Driving | Model-based offline planning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-01 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-01T00:00:00+08:00">2022-08-01</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="model-based-offline-planning">Model-based offline planning</h2>
<ul>
<li>由于成本、安全性等因素，很多情况下不能够直接与系统交互来学习控制策略，因此，只能从记录的log数据中学习控制策略（offline
reinforcement
learning）。本文介绍了一种从log数据中学到超越成圣log数据的原策略的新策略的方法，命名为
model-based ofline planning (MBOP)。</li>
</ul>
<h3 id="introduction">Introduction</h3>
<ul>
<li>Offline reinforcement learning包括：
<ul>
<li>model-free方法：
<ul>
<li>Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized
offline reinforcement learning. CoRR, abs/1911.11361, 2019. URL
http://arxiv.org/abs/1911.11361.<br />
</li>
<li>Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
Advantage-weighted regression: Simple and scalable off-policy
reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.<br />
</li>
<li>Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep
reinforcement learning without exploration. In International Conference
on Machine Learning, pp. 2052–2062, 2019.<br />
</li>
<li>Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost
Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar
Gulcehre, Nicolas Heess, et al. Critic regularized regression. Advances
in Neural Information Processing Systems, 33, 2020.<br />
</li>
</ul></li>
<li>model-based方法：MOPO,
MoREL学习一个模型，然后用于训练一个无模型策略，这种模式和Dyna模式类似。
<ul>
<li><strong>MOPO</strong>: Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano
Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo:
Model-based offline policy optimization. arXiv preprint
arXiv:2005.13239, 2020.<br />
</li>
<li><strong>MoREL</strong>: Rahul Kidambi, Aravind Rajeswaran, Praneeth
Netrapalli, and Thorsten Joachims. Morel: Modelbased offline
reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.<br />
</li>
<li><strong>Dyna</strong>: Richard S Sutton and Andrew G Barto.
Reinforcement learning: An introduction. MIT press, 2018.</li>
</ul></li>
</ul></li>
<li>本文的算法属于model-based，利用model-predictive control (MPC)
，扩展MPPI轨迹规划器，并使用实时规划，产生目标或满足奖励条件的策略。
<ul>
<li>Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M
Rehg, Byron Boots, and Evangelos A Theodorou. Information theoretic mpc
for model-based reinforcement learning. In 2017 IEEE International
Conference on Robotics and Automation (ICRA), pp. 1714–1721. IEEE,
2017b.</li>
</ul></li>
<li>本文模型MBOP包含三个要素：
<ul>
<li>a learnt <strong>world model</strong>,</li>
<li>a learnt <strong>behavior-cloning policy</strong>,</li>
<li>a learnt <strong>fixed-horizon value-function</strong>.<br />
</li>
</ul></li>
<li>MBOP的核心优势是<strong>数据高效</strong>和<strong>自适应</strong>。只需仅100秒就可以训练出一个和奖励函数、目标状态、基于状态的约束相适应的策略。</li>
<li>MBOP能够对非平稳目标和约束执行zero-shot自适应，但是没有处理非平稳动力学特性的机制。</li>
</ul>
<h3 id="model-based-offline-planning-1">Model-based offline
planning</h3>
<ul>
<li>描述问题为Markov Decision Process (MDP)，<span
class="math display">\[(S,A,p,r,\gamma)\]</span>
<ul>
<li><span class="math display">\[s\]</span>是系统状态</li>
<li><span class="math display">\[a\]</span>是行为</li>
<li><span class="math display">\[p(s_{t+1}\mid
s_t,a_t)\]</span>是状态转移概率</li>
<li><span class="math display">\[r(s_t,a_t,s_{t+1})\]</span>是奖励</li>
<li><span class="math display">\[\gamma=1\]</span>是时间折扣系数</li>
</ul></li>
<li>MBOP包括三个函数近似器：
<ul>
<li><span
class="math display">\[f_m\]</span>：环境动力学的单步模型，<span
class="math display">\[(\hat{r}_t,\hat{s}_{t+1})=f_m(s_t,a_t)\]</span>，本文使用<span
class="math display">\[f_m(s_t,a_t)_s\]</span>表示状态预测，使用<span
class="math display">\[f_m(s_t,a_t)_r\]</span>表示奖励预测。</li>
<li><span
class="math display">\[f_b\]</span>：表示一个行为克隆网络，<span
class="math display">\[a_t=f_b(s_t,a_{t-1})\]</span>，被规划算法用来引导轨迹采样的先验。</li>
<li><span
class="math display">\[f_R\]</span>：是一个阉割的值函数，提供在状态s中采取行为a后，在固定界限<span
class="math display">\[R_H\]</span>上的收益。<span
class="math display">\[\hat{R}_H = f_R(s_t,a_{t-1})\]</span></li>
</ul></li>
<li>MBOP-POLICY
<ul>
<li>使用MPC输出每个新状态下的行为（<span
class="math display">\[a_t=\pi(s_t)\]</span>）。MPC在每一时间步执行一个固定长度的规划，返回长度为H的轨迹T。选择该轨迹的第一个行为<span
class="math display">\[a_t\]</span>并返回。</li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220802-1.png"
alt="High-Level MBOP-Policy" />
<figcaption aria-hidden="true">High-Level MBOP-Policy</figcaption>
</figure>
<ul>
<li>MBOP-TRAJOPT
<ul>
<li>在PDDM的基础上增加一个策略先验<span
class="math display">\[f_b\]</span>和价值预测<span
class="math display">\[f_R\]</span></li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220802-2.png"
alt="MBOP-Trajopt" />
<figcaption aria-hidden="true">MBOP-Trajopt</figcaption>
</figure>
<blockquote>
<p>P.S.：第11行在<span
class="math display">\[f_b\]</span>给出的行为上加权了采样轨迹的行为，其含义可能是希望在网络没有收敛时，记录下来的行为也不要偏差太大，都保持在采样轨迹附近，参数<span
class="math display">\[\beta\]</span>可被视为学习率。第17行给出多条轨迹中奖励最大的作为输出（re-weighting）</p>
</blockquote>
<h3 id="experimental-results">Experimental results</h3>
<ul>
<li><p>首先，在非常少的数据中心训练，其次，再迁移到基于相同系统动力学的两种novel
tasks中：</p>
<ul>
<li><strong>goal-conditioned tasks</strong> (that ignore the original
reward function)<br />
</li>
<li><strong>constrained tasks</strong> (that require optimising for the
original reward under some state constraint)</li>
</ul></li>
<li><p>使用的数据集RL Unplugged (RLU) 和 D4RL</p>
<ul>
<li><strong>RL Unplugged (RLU)</strong>：Caglar Gulcehre, Ziyu Wang,
Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Kon- ´rad
Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru,
et al. Rl unplugged: Benchmarks for offline reinforcement learning.
arXiv preprint arXiv:2006.13888, 2020.
<ul>
<li>cartpole-swingup</li>
<li>walker</li>
<li>quadruped</li>
</ul></li>
<li><strong>D4RL</strong>：Justin Fu, Aviral Kumar, Ofir Nachum, George
Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven
reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
<ul>
<li>halfcheetah</li>
<li>hopper</li>
<li>walker2d</li>
<li>Adroit</li>
</ul></li>
</ul></li>
<li><p>对于 RLU 中的 Quadruped 和 Walker
任务，由于数据集中性能高方差，在训练 <span
class="math display">\[f_b\]</span> 和 <span
class="math display">\[f_R\]</span>
的过程中，通过设定阈值，舍弃了性能不好的数据。 使用未过滤的数据来训练
<span class="math display">\[f_s\]</span></p></li>
<li><p>对于所有的数据集，90%用于训练，10%用于测试验证</p></li>
<li><p>性能：For the RLU datasets (Fig. 1), we observe that MBOP is able
to find a near-optimal policy on most dataset sizes in Cartpole and
Quadruped with as little as <strong>5000 steps</strong>, which
corresponds to <strong>5 episodes</strong>, or approximately 50 seconds
on Cartpole and 100 seconds on Quadruped. On the Walker datasets MBOP
requires 23 episodes (approx. 10 minutes) before it finds a reasonable
policy, and with sufficient data converges to a score of 900 which is
near optimal. On most tasks, MBOP is able to generate a policy
significantly better than the behavior data as well as the the BC
prior.</p></li>
<li><p>MBOP模型容易适应新的目标函数，例如添加新的子目标函数<span
class="math display">\[R&#39;_n\]</span>时， <span
class="math display">\[
R&#39;_n = \sum_t f_{obj}(s_t)
\]</span> 其中，<span
class="math display">\[f_{obj}\]</span>是用户自定义的目标函数。只需要将轨迹更新规则改为：
<span class="math display">\[
T_t=\frac{\sum_{n=1}^N e^{kR_n+k_{obj}R&#39;_n}A_{n,t}}{\sum_{n=1}^N
e^{kR_n+k_{obj}R&#39;_n}}
\]</span></p></li>
<li><p>为了验证上述模型的适应能力，进行了两个实验：</p>
<ul>
<li><strong>goal-conditioned control</strong>（忽略原始奖励，<span
class="math display">\[k=0\]</span>，学习新奖励）</li>
<li><strong>constrained control</strong>（增加了state-based
constraint，然后探索合适的 <span class="math display">\[k\]</span> 和
<span class="math display">\[k_{obj}\]</span> ）</li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220802-3.png"
alt="ZERO-SHOT TASK ADAPTATION" />
<figcaption aria-hidden="true">ZERO-SHOT TASK ADAPTATION</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>MBOP为策略生成提供了一种易于实施、数据高效、稳定且灵活的算法。</p>
<p>由于使用了在线规划，使其能够应对变化的目标、成本和环境限制。</p>
<p>但是算法没有在更复杂的场景和约束条件下测试，因此适用范围和效果还缺少验证。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/07/23/2022-07-23-Autonomous%20Driving%20-%20A%20Review%20of%20Motion%20Planning%20for%20Highway%20Autonomous%20Driving/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/23/2022-07-23-Autonomous%20Driving%20-%20A%20Review%20of%20Motion%20Planning%20for%20Highway%20Autonomous%20Driving/" class="post-title-link" itemprop="url">Autonomous Driving | A Review of Motion Planning for Highway Autonomous Driving</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-23 00:00:00" itemprop="dateCreated datePublished" datetime="2022-07-23T00:00:00+08:00">2022-07-23</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>10 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="a-review-of-motion-planning-for-highway-autonomous-driving">A
Review of Motion Planning for Highway Autonomous Driving</h2>
<ul>
<li>高速路具有路径高速（high speed）、路径曲率小（small curvature
roads）、规则具体（specific driver rules）的几项特点。</li>
<li>主要面临的问题：变道（Lane change），避障（obstacle
avoidance），跟车（car following）合并道路（merging）</li>
</ul>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>automakers now try to personalize Advanced Driving Assistance
Systems (ADAS) to the driver’s style [7].</p>
<ul>
<li>[7]. M. Hasenjager and H. Wersing, “Personalization in advanced
driver "assistance systems and autonomous vehicles: A review," in IEEE
Int. Conf. on Intelligent Transportation Systems (ITSC), 2017.</li>
</ul></li>
<li><p>现有的一些辅助技术：</p>
<ul>
<li>巡航控制中的纵向合/横向舒适度和安全性（longitudinal and lateral
comfort and security with the Cruise Control (CC)）</li>
<li>智能速度自适应（Intelligent Speed Adaptation (ISA)）</li>
<li>道路保持辅助（Lane Keeping Assist (LKA)）</li>
<li>离道警报（Lane Departure Warning (LDW)）</li>
</ul></li>
<li><p>自动驾驶分类标准：（With such an evolution in the automotive
field, the Society of Automotive Engineers (SAE) published a standard
classification for autonomous vehicles with a 6-level system, from 0 (no
control but active safety systems) to 5 (no human intervention for
driving) [10]）</p>
<ul>
<li>SAE International J3016, accessed 2018-11-03. [Online]. Available:
https://www.sae.org/standards/content/j3016 201401/<br />
</li>
</ul></li>
<li><p>2007 DARPA城市挑战赛：since the Defense Advanced Research
Projects Agency (DARPA) organized autonomous vehicle competitions in
2004, 2005, and 2007, and thanks to new technologies, autonomous
functions are evolving quickly and treat more complex scenarios in real
environments. The 11 finalist teams of the DARPA Urban Challenge 2007
[11] succeeded in navigating through a city environment.</p></li>
<li><p>Furthermore, highways seem to be the first environment where
drivers would be confident driving in a fully autonomous mode
[26]</p></li>
</ul>
<h3 id="considerations-for-highway-motion-planning">2 Considerations for
highway motion planning</h3>
<h4 id="a.-terminology">A. Terminology</h4>
<ul>
<li><p><strong>ego
vehicle</strong>表示被掌控（mastered）并装备传感器（sensors-equipped）的车辆</p></li>
<li><p><strong>obstacles
vehicle</strong>表示其它车辆，都被视为障碍物。</p></li>
<li><p>车辆的states包括：</p>
<ul>
<li>position</li>
<li>orientation</li>
<li>and their</li>
<li>time derivatives (position, speed, and acceleration, linear and
angular)</li>
</ul></li>
<li><p>The <strong>geometric state space</strong> is called the
<strong>configuration space</strong></p></li>
<li><p>The <strong>evolution space</strong> identifies the
<strong>configuration space-time</strong> in which the ego vehicle can
navigate.</p></li>
<li><p>configuration 和 evolution 空间被分为三个子空间：</p>
<ul>
<li>the <strong>collision space</strong>, in which the ego vehicle
collides with obstacles;</li>
<li>the <strong>uncertain space</strong>, in which there exists a
probability for the ego vehicle to be in collision;</li>
<li>the <strong>free space</strong>, in which there is no
collision.<br />
</li>
<li><strong>free-space</strong> (spatial geometric zones)</li>
<li><strong>path</strong> (sequence of spacerelated states in the free
space, i.e. geometric waypoints)</li>
<li><strong>trajectory</strong> (sequence of spatiotemporal states in
the free space, i.e. time-varying waypoints)<br />
</li>
<li><strong>maneuver</strong> (predefined motion, considered as a
subspace of paths or trajectories, i.e. motion primitives)<br />
</li>
<li><strong>generation</strong>, which builds sequences of paths,
trajectories, maneuvers, or actions<br />
</li>
<li><strong>planning</strong>, meaning the selection of one sequence
among the generated motions<br />
</li>
<li><strong>prediction horizon</strong> denotes the space or/and time
horizon limit for the simulation of motion</li>
</ul></li>
</ul>
<h4 id="b.-motion-planning-scheme">B. Motion Planning Scheme</h4>
<ul>
<li>分层的自动驾驶算法框架</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220731-1.png"
alt="A hierarchical scheme of Autonomous Ground Vehicle systems." />
<figcaption aria-hidden="true">A hierarchical scheme of Autonomous
Ground Vehicle systems.</figcaption>
</figure>
<ul>
<li>行为规划包括：(i) route planning, (ii) prediction, (iii) decision
making, (iv) generation, and (v) deformation.</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220731-2.png"
alt="Motion planning functions. Motion planning acts as a global, local, and reactive motion strategy." />
<figcaption aria-hidden="true">Motion planning functions. Motion
planning acts as a global, local, and reactive motion
strategy.</figcaption>
</figure>
<p>其中，decision making, generation, and
deformation是核心。参考[32][33]两篇文章，总结的方法如下：</p>
<ul>
<li><p><strong>A high-level predictive planning</strong> built around
three objectives: risk evaluation, criteria minimization, and constraint
ubmission (see II-D). Those are used for decision making (iii), i.e. to
select the best solution out of the candidates’ generation (iv). One
either generates a set of motions and then makes a decision on the
behavior motion, or, defines the behavior to adopt and then fits a set
of motions. This high-level stage benefits from a longer predicted
motion but is time-consuming.</p></li>
<li><p><strong>A low-level reactive planning</strong> deforming the
generated motion from the high-level planning according to a reactive
approach, i.e. the deformation function (v). This acts on a shorter
range of actions and thus has faster computation.</p></li>
</ul>
<p>[32]. L. Claussmann, A. Carvalho, and G. Schildbach, “A path planner
for autonomous driving on highways using a human mimicry approach with
binary decision diagrams,” in IEEE European Control Conference (ECC),
2015. [33]. X. Li, Z. Sun, Q. Zhu, and D. Liu, “A unified approach to
local trajectory planning and control for autonomous driving along a
reference path,” in IEEE Int. Conf. on Mechatronics and Automation
(ICMA), 2014.</p>
<ul>
<li>空间和时间约束：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220731-3.png"
alt="Motion planning functions. Motion planning acts as a global, local, and reactive motion strategy." />
<figcaption aria-hidden="true">Motion planning functions. Motion
planning acts as a global, local, and reactive motion
strategy.</figcaption>
</figure>
<h4 id="c.-specificities-of-highway-driving">C. Specificities of Highway
Driving</h4>
<ul>
<li><p>特点：</p>
<ul>
<li>单向车流</li>
<li>高速a dynamic speed over 60km/h</li>
<li>道路形状简单：直线道路（straight
lines），回旋曲线道路（clothoids），小曲率的环形道路（circles with small
curvature）</li>
</ul></li>
<li><p>障碍车的行为预测分为以下几个：</p>
<ul>
<li>one-direction</li>
<li>two-lane changes – right or left</li>
<li>and to accelerate, maintain speed, or brake</li>
</ul></li>
<li><p>高速路的一些通常境况：</p>
<ul>
<li><p><strong>Lane keeping</strong></p>
<p>纵向安全的情况下保持期望的速度行驶</p></li>
<li><p><strong>Car following</strong></p>
<p>跟随自己前方的车辆，保持安全距离</p></li>
<li><p><strong>Lane changing</strong></p>
<p>受到方向和障碍物的约束，规划需要保证目标车道由充足的空间和合适的行驶速度</p></li>
<li><p><strong>Lateral-most lane changing</strong></p>
<p>一些情况下的交规要求只能在最左/右的车道行驶，因此agent会一直寻求变道的机会，直到到达目标车道</p></li>
<li><p><strong>Passing</strong></p>
<p>在侧向有障碍物的时候遵守lane keeping或者car
following决策的情况，需要保证侧向的安全距离</p></li>
<li><p><strong>Overtaking</strong></p>
<p>超车上复杂的机动动作，包括变道、pass、变道三个过程</p></li>
<li><p><strong>Merging</strong></p>
<p>两个车道合并为一个车道</p></li>
<li><p><strong>Highway toll</strong></p>
<p>高速收费站，先并入虚拟的车道线，进入收费站，之后再加速驶出，并入实际的车道线</p></li>
<li><p>高速场景特点总结：The main differences between highway, except
for platooning, and city driving consist in a further look-ahead time,
with a stronger focus towards the ahead direction of the road, whereas
city driving involves a closer range but in all directions. The highway
vehicle dynamics is also simpler with lower turn-angle, no reverse, and
less braking/acceleration, but higher and more constant speed. Thus,
even if there are less hazards, the risk due to high speed is stronger.
Moreover, the higher distances imply poorer sensors capacities. Finally,
less traffic insures more stable scenario. The algorithms which consider
all these specificities in real-time will be favored for a practical
application on highways.</p></li>
<li><p>[34]. L. Claussmann, M. Revilloud, S. Glaser, and D. Gruyer, “A
study on ai-based approaches for high-level decision making in highway
autonomous driving,” in IEEE. Int. Conf. on Systems, Man, and
Cybernetics (SMC), 2017.</p></li>
</ul></li>
</ul>
<h4 id="d.-constraints-on-highway-driving">D. Constraints on Highway
Driving</h4>
<ul>
<li><p><strong>硬约束</strong>（hard
constraints）：环境约束、交规、安全约束、避免碰撞。</p></li>
<li><p><strong>软约束</strong>（soft
constraints）：时间/距离/能耗最小化，舒适性最大化等乘坐优化约束。</p></li>
<li><p>其他可行性约束依赖于车辆的运动学限制，即非完整动力学，即车辆在只有两个自由度的三维空间中发展，平滑路径，即轨迹应该可微分且其曲率连续，以及车辆的动力学限制。</p></li>
<li><p>作者在文献[27]中认为，生成运动的质量要求应该为：可行、安全、最优、可用、自适应、高效、渐进和交互（feasible,
safe, optimal, usable, adaptive, efficient, progressive, and
interactive）</p></li>
<li><p>[27]. M. Rodrigues, A. McGordon, G. Gest, and J. Marco, “Adaptive
tactical behaviour planner for autonomous ground vehicle,” in IEEE Int.
Conf. on Control, 2016.</p></li>
</ul>
<h3 id="state-of-the-art">3 State of the art</h3>
<ul>
<li><p>运动规划包含了5个方面：</p></li>
<li><ol type="i">
<li>state estimation</li>
</ol></li>
<li><ol start="2" type="i">
<li>time evolution</li>
</ol></li>
<li><ol start="3" type="i">
<li>actions planning</li>
</ol></li>
<li><ol start="4" type="i">
<li>criteria optimization</li>
</ol></li>
<li><ol start="22" type="a">
<li>compliance with constraints</li>
</ol></li>
<li><p>一个争论：是否要区分驾驶模式（distinguishing and not
distinguishing the driving
modes），不同模式下使用不同的数据进行学习。</p></li>
<li><p><strong>分类1：空间构造算法</strong>（Space Configuration）</p>
<ul>
<li><p>总述：sampling points, connected cells, and
lattice这些方法的核心思想在于：（1）对进化空间进行采样或离散化；（2）排除与障碍物冲突或不可行的点、单元或网格；（3）将这些空间分解作为自由空间约束发送，或者用寻路算法(见III-B2)或曲线规划器(见III-B4)求解结果空间配置，以直接将路点、连接单元集或点阵集发送到控制块。</p></li>
<li><p>Sampling-Based Decomposition：</p>
<ul>
<li>Probabilistic RoadMap (PRM) [41]（The most popular random
method），配合Dijkstra算法[42]，先选择路径点，再分配速度曲线</li>
<li>spatiotemporal sampling points predictive
algorithm[43]，采样5维的车辆状态点（车的位置、速度、角度、到达时间），考虑空间分辨率的因素，还可以结合自适应分辨率采样方法[44]</li>
</ul></li>
<li><p>Connected Cells
Decomposition：网格化道路，赋予格子随机权重，然后避障寻路，该方法的缺点在于要求大的记忆容量、较高的计算速度、具有移动障碍物的虚假指示性占领，以及随空间和时间变化的分辨率。</p>
<p>依据不同的速度和形状，障碍物通常表示为凸多边形（convex polygons）
、矩形、 三角形、圆形、椭圆形。</p>
<ul>
<li><strong>非基于障碍物表征的方法</strong>，格子组织可以离线觉得在线填充，网格可以快速获得但没有使用环境特性。eg：exact
decomposition（正在淘汰）</li>
<li><strong>基于障碍物表征的方法</strong>，考虑动态变化的环境，建立在线的网格，更加方便计算和重规划。</li>
</ul></li>
<li><p>Lattice Representation（晶格表征）构建reachability graph of
maneuvers，多用于predictive planning。calculated offline for a quick
replanning [76]. Unfortunately, their application to reactive planning
is mostly limited due to the fixed structure.</p>
<p>经典的晶格表征算法基于maximum turn strategy [13,
76]，只变化车的角度，调整路径。引入速度后的改进方法curvature velocity
method[77],[72], [78]
。方法的缺点在于需要预先定义的运动集，以及高密度的运动图。</p></li>
</ul></li>
<li><p><strong>分类2：路径搜索算法</strong>（Pathfinding
Algorithms）</p>
<p>这类算法属于图论中的一部分，代表算法为Dijkstra and
A*等，主要问题在于图的尺寸和复杂度，以及进一步的动态环境的处理上，总之这些方法不是太适应高速的环境条件。</p></li>
<li><p><strong>分类3：吸引力和排斥力</strong>（Attractive and Repulsive
Forces）</p>
<p>目的地是吸引力，障碍物产生排斥力，由此构建引力图产生规划的轨迹。其优点是方便适应动态的环境。其问题在于局部最优和震荡现象。</p>
<p>Virtual Force Field (VFF) [56]</p>
<p>elastic band algorithm [102]</p></li>
<li><p><strong>分类4：参数化和半参数化曲线</strong>（Parametric and
Semi-parametric Curves）</p>
<p>考虑（1）高速路本省就是由简单的曲线构成的；（2）预先定义的曲线几何很容易实施和测试；曲线路径和速度可以解耦考虑。这里介绍两类算法：</p>
<ul>
<li><p><strong>point-free
curves算法</strong>：首先构建运动学上可行的轨迹，作为一组候选解；然后基于点的子类使用曲线来拟合一组选择的路点(采样点或单元)</p>
<p>该方法也可以参考基于晶格的方法，用一些基本曲线构建可能的运动路径，形成“触手”，以加快求解的速度。但是这些简单曲线的二阶导数不连续</p></li>
<li><p><strong>point-based
curves算法</strong>：能很好适应约束环境的几何特征，各种曲线的选择依赖对环境的认知。</p></li>
</ul></li>
<li><p><strong>分类5：数值优化</strong>（Numerical Optimization）</p>
<p>数值优化方法在运动规划中被广泛使用。一类算法简化求解的复杂度，提高效率；一类算法探索数学上的性质，以在受限的空间（restrictive
space）中推断出预测解。对于第二类方法，其基础的算法是the Linear
Programming (LP) （most popular one: Simplex algorithm[81]）</p>
<p>具体的预测上，使用Model Predictive Control (MPC)，Dynamic Programming
(DP)等</p></li>
<li><p><strong>分类6：人工智能方法</strong>（Artificial
Intelligence）</p>
<p>需要复制并模拟司机的推断和学习能力。本文将这些方法分为两类：<strong>cognitive/rational</strong>
and <strong>rules/learning</strong>，– based on [125]’s distinction
between thinking and acting humanly or rationally</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220801-1.png"
alt="A map of AI-based algorithms." />
<figcaption aria-hidden="true">A map of AI-based
algorithms.</figcaption>
</figure>
<ul>
<li><p>人工智能基于逻辑的方法（AI Logic-Based Approach）</p>
<p>依赖专家知识库和规则的专家推理系统。主要缺点在于处理循环推理和枚举所有规则</p>
<ul>
<li><p>决策树：不确定性和近似值增加了计算困难，行为必须被解释为安全合法的。</p></li>
<li><p>有限状态机（Finite State Machine (FSM)）：
只能在已知的知识范围内运行，不能在未知的环境中做生成。</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220808-1.png"
alt="Illustration of an FSM for highway" />
<figcaption aria-hidden="true">Illustration of an FSM for
highway</figcaption>
</figure></li>
<li><p>Bayesian networks：依赖马尔科夫模型的状态转移因果链。The authors
in [50] develop a Markov Decision Process (MDP) on the <strong>choice of
tentacle trajectories</strong>, and the one in [130] for a
<strong>lane-staying or -changing decision.</strong></p>
<ul>
<li>[50]. H. Mouhagir, R. Talj, V. Cherfaoui, F. Aioun, and F.
Guillemard, “Integrating safety distances with trajectory planning by
modifying the occupancy grid for autonomous vehicle navigation,” in IEEE
Int. Conf. on Intelligent Transportation Systems (ITSC), 2016.<br />
</li>
<li>[130]. S. Zhou, Y. Wang, M. Zheng, and M. Tomizuka, “A hierarchical
planning and control framework for structured highway driving,”
IFACPapersOnLine, vol. 50, no. 1, pp. 9101–9107, 2017.</li>
</ul>
<p>POMDP：</p>
<ul>
<li>[131] S. Ulbrich and M. Maurer, “Towards tactical lane change
behavior planning for automated vehicles,” in IEEE Int. Conf. on
Intelligent Transportation Systems (ITSC), 2015.</li>
<li>[132] E. Galceran, A. G. Cunningham, R. M. Eustice, and E. Olson,
“Multipolicy decision-making for autonomous driving via
changepoint-based behavior prediction: Theory and experiment,”
Autonomous Robots, vol. 41, no. 6, pp. 1367–1382, 2017.</li>
<li>[133] N. Li, D. W. Oyler, M. Zhang, Y. Yildiz, I. Kolmanovsky, and
A. R. Girard, “Game theoretic modeling of driver and vehicle
interactions for verification and validation of autonomous vehicle
control systems,” IEEE Trans. on Control Systems Technology, vol. 26,
no. 5, pp. 1782– 1797, 2018.</li>
</ul></li>
<li><p>人工智能启发式算法（AI Heuristic Algorithms）</p>
<p>优势在于相对快速高效，但是具有启发式算法的通病，即陷入局部最优，无法保证得到全局最优。</p>
<ul>
<li>基于Support Vector Machines (SVM)的个性化变道决策：
<ul>
<li>C. Vallon, Z. Ercan, A. Carvalho, and F. Borrelli, “A machine
learning approach for personalized autonomous lane change initiation and
control,” in IEEE Intelligent Vehicles Symposium (IV), 2017.<br />
</li>
</ul></li>
<li>演化算法（Evolutionary
methods）。在高速场景，最优解并不是必须的，演化算法的高计算效率，获得近似次优解，足以满足要求。</li>
</ul></li>
</ul></li>
<li><p>人工智能近似推理（AI Approximate Reasoning）</p>
<p>该方法与logic approach的区别在于其知识是非boolean形式表示的。
方法的优势在于其灵活，可拓展到非确定性数据中。缺点在于缺少可追踪性和系统性的设计方法</p>
<ul>
<li><p>Artificial Neural Networks (ANN)</p>
<p>主要可分为三类方法：supervised, unsupervised, and reinforcement
learning。他们的缺点在于缺少因果解释</p>
<p>使用卷积网络变道</p>
<ul>
<li>E. Rehder, J. Quehl, and C. Stiller, “Driving like a human:
Imitation learning for path planning using convolutional neural
networks,” in IEEE Int. Conf. on Intelligent Robots and Systems (IROS)
Workshops, 2017.</li>
</ul>
<p>multi-goal overtaking maneuvers[144]</p>
<ul>
<li>D. C. K. Ngai and N. H. C. Yung, “A multiple-goal reinforcement
learning method for complex vehicle overtaking maneuvers,” IEEE Trans.
on Intelligent Transportation Systems, vol. 12, no. 2, pp. 509–522,
2011.</li>
</ul>
<p>automated lane change maneuvers[145]</p>
<ul>
<li>P. Wang, C.-Y. Chan, and A. de La Fortelle, “A reinforcement
learning based approach for automated lane change maneuvers,” IEEE
Intelligent Vehicles Symposium (IV), 2018.</li>
</ul></li>
</ul></li>
<li><p>人工智能类人的方法（AI Human-Like Methods）</p>
<p>类人的驾驶模型的完整描述，参考这篇文章</p>
<ul>
<li>D. D. Salvucci, “Modeling driver behavior in a cognitive
architecture,” Human factors, vol. 48, no. 2, pp. 362–380, 2006</li>
</ul>
<p>可以分解为风险、任务和博弈三类：</p>
<ul>
<li><strong>Risk
estimators</strong>：执行可接受风险和目标安全之间的权衡。</li>
<li><strong>Taxonomic models</strong>：场景和状况细分分类。<br />
</li>
<li><strong>Game
theory</strong>：把所有车辆看做agent参与博弈，缺点是假设所有人准守规则，一些文章进行了改进：
<ul>
<li>N. Li, D. W. Oyler, M. Zhang, Y. Yildiz, I. Kolmanovsky, and A. R.
Girard, “Game theoretic modeling of driver and vehicle interactions for
verification and validation of autonomous vehicle control systems,” IEEE
Trans. on Control Systems Technology, vol. 26, no. 5, pp. 1782–1797,
2018.</li>
</ul></li>
</ul>
<p>人工智能类似人类的方法非常适合在高速公路场景中进行决策，由于这种环境的基本规则，司机的行为更容易预测。也易于理解并与驾驶员分享。但目前还没有统一的处理框架。</p></li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<p>这篇文章介绍的范围太大，涵盖的研究方向和方法过多，受篇幅限制，讲的东西又很浅显，价值不大。</p>
<p>​</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/07/23/2022-07-23-%E4%BD%BF%E7%94%A8VSCode%E8%B0%83%E8%AF%95%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84Python%E8%84%9A%E6%9C%AC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/23/2022-07-23-%E4%BD%BF%E7%94%A8VSCode%E8%B0%83%E8%AF%95%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84Python%E8%84%9A%E6%9C%AC/" class="post-title-link" itemprop="url">使用VSCode调试带参数的Python脚本</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-23 00:00:00" itemprop="dateCreated datePublished" datetime="2022-07-23T00:00:00+08:00">2022-07-23</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>3.3k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>3 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="使用vscode调试带参数的python脚本">使用VSCode调试带参数的Python脚本</h2>
<h3 id="问题描述">1 问题描述</h3>
<p>linux系统上执行带参数的python程序直接添加-arg
xxx即可。但在VSCode调试模式（Debug）下该执行方式不可行。那么是否有办法在VSCode上调试带参数的python脚本呢？</p>
<p>这里提供三种方案：</p>
<ul>
<li>2.1 方案1 pbd命令的Debug</li>
<li>2.2 方案2 在launch.json中设置参数的Debug</li>
<li>2.3 方案3 终端命令行中写入参数的Debug</li>
</ul>
<h3 id="解决方案">2 解决方案</h3>
<h4 id="方案1-pbd命令的debug">2.1 方案1 pbd命令的Debug</h4>
<p>终端窗口执行命令</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pdb xxx.py</span><br></pre></td></tr></table></figure>
<p>开启Debug模式，在断点处暂停，可输入以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">h：（<span class="built_in">help</span>）帮助</span><br><span class="line">w：（<span class="built_in">where</span>）打印当前执行堆栈</span><br><span class="line">d：（down）执行跳转到在当前堆栈的深一层（个人没觉得有什么用处）</span><br><span class="line">u：（up）执行跳转到当前堆栈的上一层</span><br><span class="line">b：（<span class="built_in">break</span>）添加断点</span><br><span class="line">	b 列出当前所有断点，和断点执行到统计次数</span><br><span class="line">	b line_no：当前脚本的line_no行添加断点</span><br><span class="line">	b filename:line_no：脚本filename的line_no行添加断点</span><br><span class="line">	b <span class="keyword">function</span>：在函数<span class="keyword">function</span>的第一条可执行语句处添加断点</span><br><span class="line">tbreak：（temporary <span class="built_in">break</span>）临时断点</span><br><span class="line">	在第一次执行到这个断点之后，就自动删除这个断点，用法和b一样</span><br><span class="line">cl：（clear）清除断点</span><br><span class="line">	cl 清除所有断点</span><br><span class="line">	cl bpnumber1 bpnumber2… 清除断点号为bpnumber1,bpnumber2…的断点</span><br><span class="line">	cl lineno 清除当前脚本lineno行的断点</span><br><span class="line">	cl filename:line_no 清除脚本filename的line_no行的断点</span><br><span class="line"><span class="built_in">disable</span>：停用断点，参数为bpnumber，和cl的区别是，断点依然存在，只是不启用</span><br><span class="line"><span class="built_in">enable</span>：激活断点，参数为bpnumber</span><br><span class="line">s：（step）执行下一条命令</span><br><span class="line">	如果本句是函数调用，则s会执行到函数的第一句</span><br><span class="line">n：（next）执行下一条语句</span><br><span class="line">	如果本句是函数调用，则执行函数，接着执行当前执行语句的下一条。</span><br><span class="line">r：（<span class="built_in">return</span>）执行当前运行函数到结束</span><br><span class="line">c：（<span class="built_in">continue</span>）继续执行，直到遇到下一条断点</span><br><span class="line">l：（list）列出源码</span><br><span class="line">	l 列出当前执行语句周围11条代码</span><br><span class="line">	l first 列出first行周围11条代码</span><br><span class="line">	l first second 列出first–second范围的代码，如果second&lt;first，second将被解析为行数</span><br><span class="line">a：（args）列出当前执行函数的函数</span><br><span class="line">p expression：（<span class="built_in">print</span>）输出expression的值</span><br><span class="line">pp expression：好看一点的p expression</span><br><span class="line">run：重新启动debug，相当于restart</span><br><span class="line">q：（quit）退出debug</span><br><span class="line">j lineno：（jump）设置下条执行的语句函数</span><br><span class="line">	只能在堆栈的最底层跳转，向后重新执行，向前可直接执行到行号</span><br><span class="line">unt：（until）执行到下一行（跳出循环），或者当前堆栈结束</span><br><span class="line">condition bpnumber conditon，给断点设置条件，当参数condition返回True的时候bpnumber断点有效，否则bpnumber断点无效</span><br></pre></td></tr></table></figure>
<p><strong>Note：</strong>
<code>1：直接输入Enter，会执行上一条命令；</code>
<code>2：输入PDB不认识的命令，PDB会把他当做Python语句在当前环境下执行；</code></p>
<p>但这种方式不依赖VSCode，并且是在命令行中调试，并不方便。</p>
<h4 id="方案2-在launch.json中设置参数的debug">2.2 方案2
在launch.json中设置参数的Debug</h4>
<p><strong>Step1：</strong>VSCode菜单栏-运行-打开配置，出现launch.json文件。</p>
<p><strong>Step2：</strong>在configurations中添加”args”参数形式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">      <span class="string">&quot;version&quot;</span>: <span class="string">&quot;0.2.0&quot;</span>,</span><br><span class="line">      <span class="string">&quot;configurations&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                  <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Python: Current File&quot;</span></span><br><span class="line">                  <span class="string">&quot;type&quot;</span>: <span class="string">&quot;python&quot;</span>,</span><br><span class="line">                  <span class="string">&quot;request&quot;</span>: <span class="string">&quot;launch&quot;</span>,</span><br><span class="line">                  <span class="string">&quot;program&quot;</span>: <span class="string">&quot;<span class="variable">$&#123;file&#125;</span>&quot;</span>,</span><br><span class="line">                  <span class="string">&quot;console&quot;</span>: <span class="string">&quot;integratedTerminal&quot;</span>,</span><br><span class="line">                  <span class="string">&quot;args&quot;</span>: [</span><br><span class="line">                        <span class="string">&quot;arg1&quot;</span>, <span class="string">&quot;xxx&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;arg2&quot;</span>, <span class="string">&quot;xxx&quot;</span>,</span><br><span class="line">                   ]</span><br><span class="line">            &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Step3：</strong>添加完成后，设置断点，按F5执行即可开始调试。</p>
<p>该方式需要预先写入参数，在实际项目中参数数量和名称可能常常发生变化，这种方式在调整时还不够灵活，效率不够高。</p>
<h4 id="方案3-终端命令行中写入参数的debug">2.3 方案3
终端命令行中写入参数的Debug</h4>
<p><strong>Step1：</strong>安装debugpy库，<code>pip install debugpy</code></p>
<p><strong>Step2：</strong>打开本地终端，找到一个未占用的端口号。输入<code>netstat -a</code>
State显示为LISTEN即为未占用，记该端口号为xxxx</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关于State的说明：</span><br><span class="line">- LISTEN: The socket is listening for incoming connections. Foreign address is not relevant for this line</span><br><span class="line">- ESTABLISHED: The socket has an established connection. Foreign address in the address of the remote end point of the socket.</span><br><span class="line">- CLOSE_WAIT: The remote end has shut down, waiting for the socket to close.</span><br></pre></td></tr></table></figure>
<p><strong>Step3：</strong>修改launch.json中内容为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	&quot;version&quot;: &quot;0.2.0&quot;,</span><br><span class="line">	&quot;configurations&quot;: [</span><br><span class="line">		&#123;</span><br><span class="line">			&quot;name&quot;: &quot;Python: Attach&quot;,</span><br><span class="line">			&quot;type&quot;: &quot;python&quot;,</span><br><span class="line">			&quot;request&quot;: &quot;attach&quot;,</span><br><span class="line">			&quot;connect&quot;: &#123;</span><br><span class="line">				&quot;host&quot;: &quot;localhost&quot;,</span><br><span class="line">				&quot;port&quot;: xxxx</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中，xxxx表示使用的端口号。</p>
<p><strong>Step4：</strong>假设run
该Python脚本的命令为：<code>python xxx.py -arg1 ARG1 -arg2 ARG2</code>。即脚本有两个参数输入。在进行调试之前，在VSCode终端命令行中键入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m debugpy --listen xxxx --wait-for-client xxx.py -arg1 ARG1 -arg2 ARG2</span><br></pre></td></tr></table></figure>
<p>注意这里监听的端口xxxx即为上一步找到的未占用端口（这里输入的端口号需要和launch.json中设置的端口号一致）</p>
<p>执行上述命令，终端处于执行中，没有任何返回。接下来在程序中设置断点，按下F5键，即可进入VSCode的调试模式。调试方式与不带参数的情况相同。</p>
<h3 id="ref">Ref:</h3>
<p>https://blog.csdn.net/weixin_37251044/article/details/107471459</p>
<p>https://www.cnblogs.com/JavicxhloWong/p/14356814.html（重要参考，感谢）</p>
<p>https://blog.csdn.net/qq_37837061/article/details/124561317</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/30/">30</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Txing"
      src="/images/my_photo.jpg">
  <p class="site-author-name" itemprop="name">Txing</p>
  <div class="site-description" itemprop="description">泛用人形决战型机器人博士</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">233</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/txing-casia" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;txing-casia" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.uomi.moe/" title="https:&#x2F;&#x2F;blog.uomi.moe" rel="noopener" target="_blank">驱逐舰患者</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://m.mepai.me/photographyer/u_5a68085ba15aa.html?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;m.mepai.me&#x2F;photographyer&#x2F;u_5a68085ba15aa.html?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">隐之-INF</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Txing</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">568k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">8:36</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
