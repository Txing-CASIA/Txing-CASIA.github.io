<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"txing-casia.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="泛用人形决战型机器人博士">
<meta property="og:type" content="website">
<meta property="og:title" content="Txing">
<meta property="og:url" content="https://txing-casia.github.io/page/4/index.html">
<meta property="og:site_name" content="Txing">
<meta property="og:description" content="泛用人形决战型机器人博士">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Txing">
<meta property="article:tag" content="Txing">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://txing-casia.github.io/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Txing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Txing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到 | 伽蓝之堂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/20/2022-08-20-Autonomous%20Driving%20-%20Off-Policy%20Deep%20Reinforcement%20Learning%20without%20Exploration/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/20/2022-08-20-Autonomous%20Driving%20-%20Off-Policy%20Deep%20Reinforcement%20Learning%20without%20Exploration/" class="post-title-link" itemprop="url">Autonomous Driving | Off-Policy Deep Reinforcement Learning without Exploration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-20 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-20T00:00:00+08:00">2022-08-20</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>类似DQN和DDPG的off-policy RL算法在被禁止探索，并在没有数据策略分布修正的的情况下，难以取得好的效果。本文通过限制off-policy agent的行为空间，使其行为类似与on-policy算法，最后提出了一个较为通用的，针对连续控制的deep reinforcement learning algorithm。</p>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/18/2022-08-18-Autonomous%20Driving%20-%20[U]%20Goal-driven%20Self-Attentive%20Recurrent%20Networks%20for%20Trajectory%20Prediction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/18/2022-08-18-Autonomous%20Driving%20-%20%5BU%5D%20Goal-driven%20Self-Attentive%20Recurrent%20Networks%20for%20Trajectory%20Prediction/" class="post-title-link" itemprop="url">Autonomous Driving | Goal-driven Self-Attentive Recurrent Networks for Trajectory Prediction</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-18 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-18T00:00:00+08:00">2022-08-18</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>224</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="goal-driven-self-attentive-recurrent-networks-for-trajectory-prediction">Goal-driven
Self-Attentive Recurrent Networks for Trajectory Prediction</h2>
<p>提出了一种基于U-Net
architecture和注意力的循环网络框架，增加了语义信息来预测轨迹终点。提出的算法在公开数据集SDD,
inD, ETH/UCY上部分取得了SOTA的成绩，还简化了模型。</p>
<h3 id="introduction">1 Introduction</h3>
<figure>
<img src="\images\20220817-2.png"
alt="Levels of multi-agent learning in autonomous driving" />
<figcaption aria-hidden="true">Levels of multi-agent learning in
autonomous driving</figcaption>
</figure>
<h3 id="总结">总结</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/17/2022-08-17-Autonomous%20Driving%20-%20SMARTS%20Scalable%20Multi-Agent%20Reinforcement%20Learning%20Training%20School%20for%20Autonomous%20Driving/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/17/2022-08-17-Autonomous%20Driving%20-%20SMARTS%20Scalable%20Multi-Agent%20Reinforcement%20Learning%20Training%20School%20for%20Autonomous%20Driving/" class="post-title-link" itemprop="url">Autonomous Driving | SMARTS Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving (Huawei)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-17 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-17T00:00:00+08:00">2022-08-17</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.6k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="smarts-scalable-multi-agent-reinforcement-learning-training-school-for-autonomous-driving">SMARTS:
Scalable Multi-Agent Reinforcement Learning Training School for
Autonomous Driving</h2>
<p>顾名思义，SMARTS是针对多智能体算法的自动驾驶强化学习仿真平台。</p>
<p>开源了基准任务和代码：https://github.com/huawei-noah/SMARTS</p>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>自动驾驶仿真的挑战之一是天气问题；绝大部分数据是在好天气（fair
weather）下采集的；当前的L4自动驾驶面对复杂的交互情况时。倾向于减速等待，而不是提前主动找办法通过；</p></li>
<li><p>Waymo的California2018的自动驾驶事故数据中, 57%是发生了追尾（rear
endings），29%是发生了侧面碰撞（sideswipes），并且事故都是由于他车造成的，因此说明过于保守的驾驶策略</p></li>
<li><p>waymo的汽车相比人类驾驶员经常过分刹车，导致乘客晕车</p></li>
<li><p>多智能体交互的分级标准：“multi-agent learning levels”, or
“M-levels“</p></li>
<li><p>double
merge道路场景（即&gt;--&lt;形道路）是多智能体交互的难点，车辆需考虑是继续走还是等待；在间隙不够大的时候是否需要变道；其他车开到了自车车道上，是否和它交换位置？等</p></li>
<li><p>平台设计目标：</p>
<ul>
<li><p>Bootstrapping Realistic Interaction</p>
<ul>
<li><ol type="1">
<li>physics,</li>
</ol></li>
<li><ol start="2" type="1">
<li>behavior of road users,</li>
</ol></li>
<li><ol start="3" type="1">
<li>road structure &amp; regula-tions,</li>
</ol></li>
<li><ol start="4" type="1">
<li>background traffic flow.</li>
</ol></li>
</ul></li>
<li><p>Heterogeneous Agent Computing（异构智能体计算）</p></li>
<li><p>Simulation Providers</p></li>
<li><p>Interaction Scenarios</p></li>
<li><p>Distributed Computing</p></li>
</ul></li>
<li><p>Key Features：</p></li>
</ul>
<figure>
<img src="\images\20220817-1.png"
alt="Levels of multi-agent learning in autonomous driving" />
<figcaption aria-hidden="true">Levels of multi-agent learning in
autonomous driving</figcaption>
</figure>
<ul>
<li>场景：</li>
</ul>
<figure>
<img src="\images\20220817-2.png"
alt="Levels of multi-agent learning in autonomous driving" />
<figcaption aria-hidden="true">Levels of multi-agent learning in
autonomous driving</figcaption>
</figure>
<ul>
<li><p><strong>Observation.</strong> The observation is a stack of three
consecutive frames, which covers the dynamic objects and key events. For
each frame, it contains: 1) relative position of goal; 2) distance to
the center of lane; 3) speed; 4) steering; 5) a list of heading errors;
6) at most eight neighboring vehicles’ driving states (relative
distance, speed and position); 7) a bird’s-eye view gray-scale image
with the agent at the center.</p></li>
<li><p><strong>Action.</strong> The action used here is a
four-dimensional vector of discrete values, for longitudinal
control—keep lane and slow down—and lateral control—turn right and turn
left.</p></li>
<li><p><strong>Reward.</strong> The reward is a weighted sum of the
reward components shaped according to ego vehicle states, interactions
involving surrounding vehicles, and key traffic events. More details can
be found in our implementation code.</p></li>
</ul>
<h3 id="总结">总结</h3>
<p>华为推出的针对多智能体的自动驾驶仿真环境，设计相对灵活，支持场景和他车的编辑，以及Waymo的真实数据，后续可以继续看看</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/12/2022-08-12-Autonomous%20Driving%20-%20An%20Optimistic%20Perspective%20on%20Offline%20Reinforcement%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/2022-08-12-Autonomous%20Driving%20-%20An%20Optimistic%20Perspective%20on%20Offline%20Reinforcement%20Learning/" class="post-title-link" itemprop="url">Autonomous Driving | An Optimistic Perspective on Offline Reinforcement Learning (Google)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.3k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="an-optimistic-perspective-on-offline-reinforcement-learning-google">An
Optimistic Perspective on Offline Reinforcement Learning (Google)</h2>
<p>作者用online DQN在60款 Atari
2600游戏上获取数据样本，然后用这些样本(fixed
dataset)训练offline强化学习算法，一些offline的算法性能可以超过online的算法。本文提出的Random
Ensemble Mixture
(REM)算法在离线回放数据上的表现超过了强的基准算法。因此作者认为在离线样本足够多，多样化充分的情况下，使用鲁棒的RL算法可以获得高质量的策略。</p>
<p>代码： github.com/google-research/batch_rl</p>
<h3 id="introduction">1 Introduction</h3>
<p>离线强化学习的设定是不与真实环境的主动交互，而是通过对收集的离线回放数据中学习策略，在评估模型中生成新的交互数据。相应的情景在以下场景均会面临：</p>
<ul>
<li><strong>robotics</strong>
<ul>
<li>Cabi, S., Colmenarejo, S. G., Novikov, A., Konyushkova, K., Reed,
S., Jeong, R., Zołna, K., Aytar, Y., Budden, D., Vecerik, M., et al. A
framework for data-driven robotics. arXiv preprint arXiv:1909.12200,
2019.</li>
<li>Dasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper,
K., Singh, S., Levine, S., and Finn, C. Robonet: Large-scale multi-robot
learning. CoRL, 2019.</li>
</ul></li>
<li><strong>autonomous driving</strong>
<ul>
<li>Yu, F., Xian, W., Chen, Y., Liu, F., Liao, M., Madhavan, V., and
Darrell, T. Bdd100k: A diverse driving video database with scalable
annotation tooling. CVPR, 2018.</li>
</ul></li>
<li><strong>recommendation systems</strong>
<ul>
<li>Strehl, A. L., Langford, J., Li, L., and Kakade, S. Learning from
logged implicit exploration data. NeurIPS, 2010.</li>
<li>Bottou, L., Peters, J., Quiñonero-Candela, J., Charles, D. X.,
Chickering, D. M., Portugaly, E., Ray, D., Simard, P., and Snelson, E.
Counterfactual reasoning and learning systems: The example of
computational advertising. JMLR, 2013.</li>
</ul></li>
<li><strong>healthcare</strong>
<ul>
<li>Shortreed, S. M., Laber, E., Lizotte, D. J., Stroup, T. S., Pineau,
J., and Murphy, S. A. Informing sequential clinical decisionmaking
through reinforcement learning: an empirical study. Machine learning,
2011.</li>
</ul></li>
</ul>
<p>面对离线强化学习问题时，off-policy算法普遍表现不好，设计大的replay
buffer反而会损害off-policy算法的性能（由于算法的off-policyness）</p>
<ul>
<li>对比的方法包括：
<ul>
<li><strong>offline QR-DQN</strong>：Dabney, W., Rowland, M., Bellemare,
M. G., and Munos, R. Distributional reinforcement learning with quantile
regression. AAAI, 2018.</li>
<li><strong>DQN</strong>：（Nature）</li>
<li><strong>Random Ensemble
Mixture</strong>（REM，随机集成混合）：为本文提出方法</li>
<li><strong>online C51</strong>： 分布式DQN算法。Bellemare, M. G.,
Dabney, W., and Munos, R. A distributional perspective on reinforcement
learning. ICML, 2017.</li>
<li><strong>distributional QR-DQN (SOTA)</strong>：Dabney, W., Rowland,
M., Bellemare, M. G., and Munos, R. Distributional reinforcement
learning with quantile regression. AAAI, 2018</li>
</ul></li>
</ul>
<h3 id="off-policy-reinforcement-learning">2 Off-policy Reinforcement
Learning</h3>
<ul>
<li><p>DQN算法介绍</p>
<ul>
<li>Huber loss：介于MSE和MAE之间的，对数据异常值更不敏感的loss</li>
</ul>
<p><span class="math display">\[
l_{\lambda}(u)=
\begin{align}
\begin{cases}
\frac{1}{2}u^2,&amp;\mid u \mid \leq \lambda\\
\lambda(\mid u\mid-\frac{1}{2}\lambda),&amp; \text{otherwise}
\end{cases}
\end{align}
\]</span></p></li>
<li><p>baseline方法：分布式RL（Distributional RL）</p>
<ul>
<li>C51</li>
<li>QR-DQN</li>
</ul></li>
</ul>
<h3 id="offline-reinforcement-learning">3 Offline Reinforcement
Learning</h3>
<ul>
<li><p>offline的模式分离了模型对经验的利用、生成能力（exploit） vs
探索效率（explore）</p></li>
<li><p>offline RL面临的挑战是<strong>distribution
mismatch</strong>：错误匹配当前使用的策略和固定的离线数据集。例如，当采取了数据集中不存在的行为时，并不知道响应的奖励是多少。</p></li>
<li><p>本文尝试在不解决distribution
mismatch的基础上，训练高性能的agent</p></li>
</ul>
<h3 id="developing-robust-offline-rl-algorithms">4 Developing Robust
Offline RL Algorithms</h3>
<ul>
<li>采用集成（Ensemble）可以提高模型的泛化能力，本文使用了Ensemble
DQN和REM两个采取该思想的方法。</li>
</ul>
<h4 id="ensemble-dqn">4.1 Ensemble-DQN</h4>
<ul>
<li><p>该方法是对DQN算法的简单扩展，使用集成多个参数化的Q函数来近视Q值。</p>
<ul>
<li>Faußer, S. and Schwenker, F. Neural network ensembles in
reinforcement learning. Neural Processing Letters, 2015</li>
<li>Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep
exploration via bootstrapped DQN. NeurIPS, 2016.</li>
<li>Anschel, O., Baram, N., and Shimkin, N. Averaged-dqn: Variance
reduction and stabilization for deep reinforcement learning. ICML,
2017.</li>
</ul></li>
<li><p>每个参数化Q函数的优化目标是近似真实的Q值，参考下面这篇文章：</p>
<ul>
<li><strong>Bootstrapped-DQN</strong>：Osband, I., Blundell, C.,
Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped DQN.
NeurIPS, 2016.</li>
</ul></li>
<li><figure>
<img src="\images\20220812-2.png" alt="损失函数" />
<figcaption aria-hidden="true">损失函数</figcaption>
</figure>
<p>其中，<span
class="math display">\[l_{\lambda}\]</span>是Huber损失。算法使用所有Q函数的均值作为输出。</p></li>
</ul>
<h4 id="random-ensemble-mixture-rem">4.2 Random Ensemble Mixture
(REM)</h4>
<ul>
<li>引入了dropout：
<ul>
<li>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and
Salakhutdinov, R. Dropout: a simple way to prevent neural networks from
overfitting. JMLR, 2014</li>
</ul></li>
<li>不同于Ensemble-DQN，REM构造一个包含多个Q函数近似器的凸组合（convex
combination），将多个Q 函数近似器作为1个近似器使用。使用(K −
1)-simplex计算混合的概率。</li>
</ul>
<figure>
<img src="\images\20220812-3.png" alt="模型结构" />
<figcaption aria-hidden="true">模型结构</figcaption>
</figure>
<ul>
<li><p>对于每个mini-batch，随机产生一个分类分布（categorical
distribution）<span
class="math display">\[\alpha\]</span>，它定义了一个逼近最优Q-函数的K个估计器的凸组合</p></li>
<li><figure>
<img src="\images\20220817-3.png" alt="REM的loss形式" />
<figcaption aria-hidden="true">REM的loss形式</figcaption>
</figure>
<p>其中，<span
class="math display">\[P_{\Delta}\]</span>表示标准的(K-1)-simplex的概率分布，<span
class="math display">\[\Delta^{K-1}=\{\alpha\in R^K:
\alpha_1+\alpha_2+...+\alpha_K=1,\alpha_k\geq0,k=1,...,K
\}\]</span></p></li>
<li><p>对于<span class="math display">\[P_{\Delta}\]</span>：先从Uniform
(0,1)分布中采样K个独立同分布的值，然后归一化它们获得有效的分类分布（<span
class="math display">\[a&#39;_k \sim U(0,1),a_k=a&#39;_k/\sum_k
a&#39;_i\]</span>）</p></li>
<li><p>对于Q值的求解，使用<span
class="math display">\[Q(s,a)=\frac{1}{K}\sum_k Q_{\theta}^k
(s,a)\]</span></p></li>
</ul>
<h3 id="offline-rl-on-atari-2600-games">5 Offline RL on Atari 2600
Games</h3>
<ul>
<li>将Nature DQN在60个Atari游戏上的行为数据用来构建DQN
replay数据集，每个游戏2亿帧（200 million frames）</li>
<li>每个游戏训练5个智能体，因此60个游戏一共有60个数据集</li>
<li>ofline RL算法性能对比：</li>
</ul>
<figure>
<img src="\images\20220812-1.png" alt="Offline RL on Atari 2600." />
<figcaption aria-hidden="true">Offline RL on Atari 2600.</figcaption>
</figure>
<ul>
<li><figure>
<img src="\images\20220818-1.png"
alt="Offline QR-DQN vs. DQN (Nature)" />
<figcaption aria-hidden="true">Offline QR-DQN vs. DQN
(Nature)</figcaption>
</figure></li>
<li><figure>
<img src="\images\20220818-2.png"
alt="Offline Agents on DQN Replay Dataset" />
<figcaption aria-hidden="true">Offline Agents on DQN Replay
Dataset</figcaption>
</figure></li>
<li><figure>
<img src="\images\20220818-3.png"
alt="Asymptotic performance of offline agents" />
<figcaption aria-hidden="true">Asymptotic performance of offline
agents</figcaption>
</figure></li>
<li><p>文中提到offline连续强化学习方法，实验了offline DDPG，offline
TD3，offline BCQ等算法</p>
<ul>
<li><p>Fujimoto, S., Meger, D., and Precup, D. Off-policy deep
reinforcement learning without exploration. ICML, 2019b.</p>
<figure>
<img src="\images\20220818-4.png"
alt="Asymptotic performance of offline agents" />
<figcaption aria-hidden="true">Asymptotic performance of offline
agents</figcaption>
</figure></li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<p>总的来说，文章本身目的更倾向于提供一种乐观的横向对比，证明offline
RL在一些情况下可以获取SOTA的性能，甚至超过online RL。提供的几个offline
Q-learning变体和offline连续情况的RL算法可以再看看。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/04/2022-08-04-Autonomous%20Driving%20-%20UMBRELLA%20Uncertainty-Aware%20Model-Based%20Offline%20Reinforcement%20Learning%20Leveraging%20Planning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/04/2022-08-04-Autonomous%20Driving%20-%20UMBRELLA%20Uncertainty-Aware%20Model-Based%20Offline%20Reinforcement%20Learning%20Leveraging%20Planning/" class="post-title-link" itemprop="url">Autonomous Driving | UMBRELLA: Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging Planning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-04 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-04T00:00:00+08:00">2022-08-04</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>5.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="umbrella-uncertainty-aware-model-based-offline-reinforcement-learning-leveraging-planning">UMBRELLA:
Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging
Planning</h2>
<ul>
<li>在学习过程中考虑了随机不确定性的影响，提高了模型的可迁移性和可解释性</li>
</ul>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>目前的模型考虑到多智能体之间的交互和复杂行为，多采用工程设计的驾驶策略，但这样难以适用更复杂的任务。强化学习通过试错学习的方式避免了这些手工设计，但需要大量的试错机会。相比于模仿学习习得次优行为，强化学习可以提升不同类型数据的质量</p></li>
<li><p>主要贡献：</p>
<ul>
<li>提出了一个基于模型的离线规划算法UMBRELLA，在观测上考虑了认知和偶然的不确定性（epistemic
和 aleatoric）</li>
<li>引入了一个消融实验，优化最坏情况模型</li>
<li>实验中，在稠密车流（with dense
traffic）的城市和高速场景中，胜过了BC行为克隆方法和MBOP算法</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<ul>
<li><p><strong>Model-based Offline Reinforcement Learning</strong>：</p>
<p>其主要的问题是the distributional
shift。MOReL和MBOP方法将动态模型的认知不确定性估计结合到奖励函数中，以惩罚未被行为分布覆盖的状态</p>
<ul>
<li>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline
reinforcement learning: Tutorial, review, and perspectives on open
problems. CoRR, 2020. arXiv:2005.01643.</li>
</ul>
<p>这些方法在多智能体环境中测试，没考虑到由行人和其它车辆行为造成的偶然不确定性（aleatoric
uncertainty）的影响。[Henaff et al.,
2019]通过由条件变分自动编码器（conditional variational
autoencoder，CVAE）表示的随机动力学模型解决了这个问题[Kingma and
Welling,
2014]。然而，他们的方法依赖于策略学习，这与基于模型的离线规划相比，可解释性和控制灵活性有所降低。</p>
<ul>
<li>Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive
policy learning with uncertainty regularization for driving in dense
traffic. In International Conference on Learning Representations,
\2019.</li>
<li>Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes.
In International Conference on Learning Representations, 2014.</li>
</ul></li>
<li><p><strong>Interaction-aware Motion Prediction and
Planning</strong>：</p>
<p>单纯通过规划安全的pass进行自动驾驶忽略了车辆之间包含规划和预测（planning
and prediction）的行为交互</p>
<p>引入博弈论的方法考虑了多智能体的动力学过程，但带来了计算上的开销。</p>
<p>一些基于学习的模型可以生成更多的场景，但是没有考虑认知不确定性</p>
<ul>
<li>Jerry Liu, Wenyuan Zeng, Raquel Urtasun, and Ersin Yumer. Deep
structured reactive planning. In IEEE International Conference on
Robotics and Automation, 2021.</li>
<li>Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A. Wright,
Rowan McAllister, Joseph E. Gonzalez, and Sergey Levine. Contingencies
from observations: Tractable contingency planning with learned behavior
models. In IEEE International Conference on Robotics and Automation,
2021.</li>
</ul></li>
</ul>
<h3 id="the-umbrella-framework">3 The UMBRELLA Framework</h3>
<ul>
<li><p>UMBRELLA 是 MBOP 算法的扩展</p>
<ul>
<li>Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline
planning. In International Conference on Learning Representations,
2021.</li>
</ul></li>
<li><p>问题形式化，用MDP的<span class="math display">\[(S; A; p; r;
\gamma)\]</span>
表示，在offline设定中，智能体不与环境直接交互，而是从数据集中学习策略<span
class="math display">\[\pi_d\]</span>。如果观测值<span
class="math display">\[O\]</span>并不是完全可得，那么问题就变化为（partially
observable MDP，POMDP），使用 <span
class="math display">\[M_{PO}=(S;A;O;p;r;\gamma)\]</span>，处理该问题的常用方法是使用nth-order
history
方法，可以近似得到状态估计，然后将其转变为MDP，用标准的RL方法处理。</p></li>
<li><p>UMBRELLA使用连续的潜在变量（continuous latent variable）<span
class="math display">\[z_t\in Z\]</span>
，并枚举自车所有的可能行为（行为采样通过学到的BC
policy）。预测N条长度界限为H的轨迹。还使用了一个return-weighted
trajectory optimizer，处理POMDP问题，用持续观测状态从<span
class="math display">\[o_{t-_c:t}\]</span>直到时间步t估计缺失的观测状态。（相关解释参考VAE算法）</p>
<ul>
<li><span
class="math display">\[z_t=\mu_{\phi}+\sigma_{\phi}*\epsilon\]</span></li>
<li><span
class="math display">\[(\mu_{\phi},\sigma_{\phi})=q_{\phi}(s_t,s_{t+1})\]</span></li>
<li><span class="math display">\[\epsilon \sim
\mathcal{N}(0,1)\]</span></li>
</ul></li>
<li><p>随机动力学模型：</p>
<ul>
<li><p>为了建模不同的未来可能情况，学习随机前向动力学模型<span
class="math display">\[f_{m,\theta}:S\times A\times Z \rightarrow S
\times \mathbb{R}\]</span></p></li>
<li><p>该模型采用CVAE架构，预测下一时刻的状态</p>
<ul>
<li>Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes.
In International Conference on Learning Representations, 2014.</li>
</ul></li>
<li><p><span
class="math display">\[\hat{s}_{t+1}=f_m(s_t,a_t,z_t)_s\]</span></p></li>
<li><p><span
class="math display">\[\hat{r}_{t}=f_m(s_t,a_t,z_t)_r\]</span></p></li>
<li><p>其中潜在变量<span
class="math display">\[z_t\]</span>建模了不同的未来预测，并确保了输出对于输入是非确定性的。在训练过程中，该潜在变量从后验分布<span
class="math display">\[q_{\phi}(z\mid s_t,s_{t+1})\]</span>中采样，<span
class="math display">\[\phi\]</span>是参数。由于实际采样只能从先验分布中采，因此使用Kullback-Leibler
(KL) divergence度量后验分布和先验分布<span
class="math display">\[p(z)\]</span>并最小化距离。</p>
<ul>
<li>潜变量用于区分细分情况，精细化建模</li>
</ul></li>
<li><p>定义Evidence Lower BOund (ELBO)目标训练VAEs。</p>
<ul>
<li>Evidence Lower BOund： https://zhuanlan.zhihu.com/p/400322786</li>
</ul></li>
<li><p>损失函数为： <span class="math display">\[
L(\theta,\phi;s_t,s_{t+1},a_t,r_t)=\mid\mid s_{t+1} -
f_{m,\theta}(s_t,a_t,z_t)_s\mid\mid_2^2 + \mid\mid
r_t-f_{m,\theta}(s_t,a_t,z_t)_r\mid\mid^2_2 \\
\zeta D_{KL}(q_{\phi}(z_t\mid s_t,s_{t+1})\mid\mid p(z_t))
\]</span></p></li>
<li><p>BC policies <span class="math display">\[f_{b,\psi}:S\times
A^{n_c} \rightarrow A \]</span> ，<span
class="math display">\[f_b(s_t,a_{(t-n_c):(t-1)})\]</span>，该函数使用当前的状态和<span
class="math display">\[n_c\]</span>个之前的行为作为输入，输出行为<span
class="math display">\[a_t\]</span>。通过将之前的行为串联考虑，可以使得输出的行为更加平滑。</p></li>
<li><p>训练BC Policy使用最小化损失函数：</p>
<p><span
class="math display">\[L(\psi;s_t,a_{(t-n_c):(t-1)},a_t)=\mid\mid a_t -
f_{b,\psi}(s_t,a_{(t-n_c):(t-1)})\mid\mid^2_2\]</span></p></li>
<li><p>截断价值函数<span class="math display">\[f_{R,\xi}:S\times
A^{n_c}\rightarrow \mathbb{R}\]</span> 估计H个时间步后的期望回报<span
class="math display">\[\hat{R}_H\]</span></p></li>
</ul></li>
<li><figure>
<img src="\images\20220804-3.png"
alt="Simplified network architecture of the stochastic forward dynamics model during training" />
<figcaption aria-hidden="true">Simplified network architecture of the
stochastic forward dynamics model during training</figcaption>
</figure></li>
<li><figure>
<img src="\images\20220804-4.png"
alt="Behavior-cloned policy network architecture" />
<figcaption aria-hidden="true">Behavior-cloned policy network
architecture</figcaption>
</figure></li>
<li><figure>
<img src="\images\20220804-5.png"
alt="Truncated value function network architecture" />
<figcaption aria-hidden="true">Truncated value function network
architecture</figcaption>
</figure></li>
<li><figure>
<img src="\images\20220804-6.png"
alt="Signal flow of the stochastic forward dynamics model" />
<figcaption aria-hidden="true">Signal flow of the stochastic forward
dynamics model</figcaption>
</figure></li>
<li><p><strong>UMBRELLA-Planning</strong>：采用了MPC，在每一步规划未来H步的最优控制轨迹，然后只执行第一步行为，之后再次进行规划，并不断迭代。通过这样的迭代优化可以降低模型误差带来的影响。</p></li>
<li><p><strong>UMBRELLA Trajectory Optimizer</strong>：</p>
<p>最终的轨迹输出使用MPPI framework</p>
<ul>
<li>Grady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model
predictive path integral control: From theory to parallel computation.
Journal of Guidance, Control, and Dynamics, 40(2): 344–357, 2017.</li>
</ul></li>
<li><p>最后，模型通过re-weighting获得最优轨迹：</p></li>
</ul>
<p><span class="math display">\[
T^*_t = \frac{\sum_{n=1}^N e^{kR_n} A_{n,t+1}}{\sum_{n=1}^N e^{kR_n}}
\]</span></p>
<figure>
<img src="\images\20220804-1.png" alt="UMBRELLA Planning" />
<figcaption aria-hidden="true">UMBRELLA Planning</figcaption>
</figure>
<ul>
<li><strong>Pessimistic Trajectory Optimizer</strong>：
面对认知的不确定性，UMBRELLA-P对最坏情况的结果进行优化，并采取悲观的行动</li>
</ul>
<h3 id="experimental-evaluation">4 Experimental Evaluation</h3>
<ul>
<li>环境：
<ul>
<li><strong>NGSIM</strong>：多智能体自动驾驶环境</li>
<li><strong>CARLA</strong>：城市多智能体自动驾驶场景</li>
</ul></li>
<li>基线方法：
<ul>
<li><strong>1-step IL</strong>：模仿专家行为的BC policy</li>
<li><strong>MBOP</strong>：当前最好的基于模型的离线RL方法
<ul>
<li>Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline
planning. In International Conference on Learning Representations,
2021.</li>
</ul></li>
<li><strong>MPUR</strong>：最好的基于模型策略学习方法
<ul>
<li>Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive
policy learning with uncertainty regularization for driving in dense
traffic. In International Conference on Learning Representations,
2019.</li>
</ul></li>
</ul></li>
<li>指标：
<ul>
<li>Success rate (SR)：The rate of collision-free
episodes，并要求在时间内到达目标位置</li>
<li>Mean distance (MD)：NGSIM中的纵向行驶距离</li>
<li>mean successful time (MST)</li>
<li>mean proximity reward <span
class="math display">\[\overline{r}_{prox}\]</span></li>
<li>mean lane reward <span
class="math display">\[\overline{r}_{lane}\]</span></li>
<li>mean final reward <span
class="math display">\[\overline{r}\]</span></li>
</ul></li>
<li>实验结果：</li>
</ul>
<figure>
<img src="\images\20220804-2.png" alt="性能对比" />
<figcaption aria-hidden="true">性能对比</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>奖励函数误设计仍然是一个较大的问题，也是自动驾驶策略不像人的原因之一。</p>
<ul>
<li>W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt,
and Peter Stone. Reward (mis)design for autonomous driving. 2021.
arxiv:2104.13906.</li>
</ul>
<p>模型考虑偶然不确定性：使用潜变量<span
class="math display">\[z\]</span>，枚举可能的轨迹</p>
<p>模型考虑认知不确定性：在reward中进行re-weighting和使用参数<span
class="math display">\[\beta\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/03/2022-08-03-Autonomous%20Driving%20-%20Urban%20Driver%20Learning%20to%20Drive%20from%20Real-world%20Demonstrations%20Using%20Policy%20Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/03/2022-08-03-Autonomous%20Driving%20-%20Urban%20Driver%20Learning%20to%20Drive%20from%20Real-world%20Demonstrations%20Using%20Policy%20Gradients/" class="post-title-link" itemprop="url">Autonomous Driving | Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-03 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-03T00:00:00+08:00">2022-08-03</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="urban-driver-learning-to-drive-from-real-world-demonstrations-using-policy-gradients">Urban
Driver: Learning to Drive from Real-world Demonstrations Using Policy
Gradients</h2>
<ul>
<li>取得了城市驾驶场景中最好的效果（Urban driving scenarios）</li>
<li>数据：使用100小时的城市道路专家示教数据</li>
<li>不必添加复杂的状态扰动；</li>
<li>不必在训练中收集额外的同策略数据；</li>
</ul>
<h3 id="introduction">Introduction</h3>
<figure>
<img src="\images\20220803-1.png" alt="本文的闭环训练算法概览" />
<figcaption aria-hidden="true">本文的闭环训练算法概览</figcaption>
</figure>
<ul>
<li>工业界最好轨迹规划器文献：
<ul>
<li>H. Fan, F. Zhu, C. Liu, L. Zhang, L. Zhuang, D. Li, W. Zhu, J. Hu,
H. Li, and Q. Kong. Baidu apollo em motion planner. ArXiv, 2018.</li>
</ul></li>
<li>本文主要贡献：
<ul>
<li>复杂城市驾驶场景中，第一个证明了用策略梯度学习，可以从大量真实世界演示数据中学习模仿驾驶策略；</li>
<li>一个新的可微分仿真器，可基于过去的数据进行闭环仿真，并通过时间的反向传播计算策略梯度，实现快速学习；</li>
<li>单纯在仿真器中训练可在真实世界中控制自动驾驶车辆，优于其他方法；</li>
<li>源码可得：https://planning.l5kit.org.</li>
</ul></li>
</ul>
<h3 id="related-work">Related work</h3>
<ul>
<li><p><strong>Trajectory-based optimization</strong>：</p>
<ul>
<li><p>这是当前工业界的主流方法（a dominant approach）</p></li>
<li><p>依赖手工定义的损失和奖励</p></li>
<li><p>损失的优化可结合一系列经典的算法：</p>
<ul>
<li>A* [11]</li>
<li>RRTs [12]</li>
<li>POMDP with solver [13]</li>
<li>dynamic programming [14]</li>
</ul></li>
<li><p>整体上是依赖human engineering，而不是数据驱动</p></li>
</ul></li>
<li><p><strong>Reinforcement learning（RL）</strong>：</p>
<ul>
<li>依赖仿真器的构造、精确编码和优化的奖励信号
<ul>
<li>S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent,
reinforcement learning for autonomous driving. ArXiv, 2016.</li>
</ul></li>
<li>手工编程的仿真器，不能还原真实的长尾场景</li>
<li>本文直接通过 mid-level representations 从真实世界的 log
中构建仿真环境</li>
</ul></li>
<li><p><strong>Imitation learning (IL) and Inverse Reinforcement
Learning (IRL)</strong>：</p>
<ul>
<li>原始的行为克隆（Naive behavioral
cloning）面临协变量偏移问题（covariate shift）</li>
<li>Adversarial Imitation Learning [31, 32,
33]，还没有在自动驾驶场景使用</li>
</ul></li>
<li><p><strong>Neural Motion Planners</strong>：</p>
<ul>
<li>在[34]中，原始感觉输入和高清地图被用于估计未来可能的SDV位置的成本量。基于这些成本量，可以对轨迹进行采样，并且选择最低成本的轨迹来执行。这些方法目前没有在实车测试。
<ul>
<li>W. Zeng, W. Luo, S. Suo, A. Sadat, B. Yang, S. Casas, and R.
Urtasun. End-to-end interpretable neural motion planner. Int. Conference
on Computer Vision and Pattern Recognition (CVPR), \2019.</li>
<li>S. Casas, A. Sadat, and R. Urtasun. Mp3: A unified model to map,
perceive, predict and plan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 14403–14412, 2021.</li>
</ul></li>
</ul></li>
<li><p><strong>Mid-representations and the availability of large-scale
real-world AD datasets</strong>：</p>
<ul>
<li>J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V.
Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving
motion prediction dataset. Conference on Robot Learning (CoRL),
2020.</li>
<li>M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
P. C. De Wang, S. Lucey, D. Ramanan, and J. Hays. Argoverse: 3d tracking
and forecasting with rich maps supplementary material. Int. Conf. on
Computer Vision and Pattern Recognition (CVPR).</li>
<li>state-of-the-art solutions for motion forecasting [8, 9]
<ul>
<li>[8] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C.
Schmid. Vectornet: Encoding hd maps and agent dynamics from vectorized
representation. In Int. Conf. on Computer Vision and Pattern Recognition
(CVPR), 2020.</li>
<li>[9] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R.
Urtasun. Learning lane graph representations for motion forecasting.
2020.</li>
</ul></li>
</ul></li>
<li><p><strong>Data-driven simulation</strong>：</p>
<ul>
<li>[23] created a photo-realistic simulator for training an end-to-end
RL policy.</li>
<li>[5] simulated a bird’s-eye view of dense traffic on a highway.</li>
<li>Finally, two recent works [39, 40] developed data-driven simulators
and showed their usefulness for training and validating ML
planners.</li>
</ul></li>
</ul>
<h3
id="differentiable-traffic-simulator-from-real-world-driving-data">Differentiable
Traffic Simulator from Real-world Driving Data</h3>
<ul>
<li>真实世界的经验轨迹：<span
class="math display">\[\overline{\tau}=\{\overline{s}_1,\overline{s}_2,...,\overline{s}_T\}\]</span></li>
<li>仿真的目标是迭代地生成观测状态序列<span
class="math display">\[\tau=\{s_1,s_2,...,s_T\}\]</span>，然后计算车辆轨迹<span
class="math display">\[p_t\]</span>，包括<span
class="math display">\[(x;y;\theta)\]</span></li>
<li><span class="math display">\[s_{t+1}=S(s_t,a_t)\]</span>，<span
class="math display">\[p_{t+1}=f(p_t,a_t)\]</span></li>
</ul>
<h3 id="imitation-learning-using-a-differentiable-simulator">Imitation
Learning Using a Differentiable Simulator</h3>
<ul>
<li><span class="math display">\[L(s_t,a_t)=\mid\mid \overline{p}_t -
p_t\mid\mid_1\]</span></li>
<li><span
class="math display">\[J(\pi)=\mathbb{E}_{\overline{\tau}\sim\pi_E}\mathbb{E}_{\tau\sim\pi}
\sum_{t}\gamma^t L(s_t,a_t)\]</span>，<span
class="math display">\[\pi_E\]</span>是专家策略，<span
class="math display">\[\pi\]</span>是模型的策略，希望两个策略接近</li>
</ul>
<figure>
<img src="\images\20220803-2.png"
alt="Imitation learning from expert demonstrations" />
<figcaption aria-hidden="true">Imitation learning from expert
demonstrations</figcaption>
</figure>
<blockquote>
<p>P.S.：由于轨迹的开始阶段均来自专家策略，会引入bias，在策略更新的时候，在运动开始的第K步之后才计算梯度，以此避免bias</p>
</blockquote>
<ul>
<li><p>策略梯度的计算（用下标表示偏微分，<span
class="math display">\[\theta\]</span>是策略参数）： <span
class="math display">\[
J_s^t = L_s+L_a\pi_s+\gamma J^{t+1}_{\theta}(S_s+S_a\pi_{s})\\
J_{\theta}^t =
L_a\pi_{\theta}+\gamma(J_s^{t+1}S_a\pi_{\theta}+J_{\theta}^{t+1})
\]</span> &gt; Ref: N. Heess, G. Wayne, D. Silver, T. Lillicrap, T.
Erez, and Y. Tassa. Learning continuous control policies by stochastic
value gradients. In Advances in Neural Information Processing Systems,
\2015. ### Experiments</p></li>
<li><p>Lyft Motion Prediction Dataset
[6]：数据采集自加利福尼亚州帕洛阿尔托的复杂城市路线。数据集捕捉各种真实世界的情况，例如在多车道交通中驾驶、转弯、在十字路口与车辆互动等。</p>
<ul>
<li>J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V.
Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving
motion prediction dataset. Conference on Robot Learning (CoRL),
2020.</li>
</ul></li>
<li><p>模型在100小时子集上训练，并在25小时子集上测试。</p></li>
<li><p>three state-of-the-art baselines：</p>
<ul>
<li>Naive Behavioral Cloning (BC)</li>
<li>Behavioral Cloning + Perturbations (BC-perturb)
<ul>
<li>M. Bansal, A. Krizhevsky, and A. Ogale. Chauffeurnet: Learning to
drive by imitating the best and synthesizing the worst. 12 2018.</li>
</ul></li>
<li>Multi-step Prediction (MS Prediction)
<ul>
<li>A. Venkatraman, M. Hebert, and J. Bagnell. Improving multi-step
prediction of learned time series models. In AAAI, 2015.</li>
</ul></li>
</ul></li>
</ul>
<figure>
<img src="\images\20220803-3.png" alt="性能对比" />
<figcaption aria-hidden="true">性能对比</figcaption>
</figure>
<blockquote>
<p>指标值越小越好，本文模型取得最好的表现以及最低的l1K指标（综合其它指标，每1000英里干预次数）</p>
</blockquote>
<ul>
<li>评价指标：
<ul>
<li><strong>L2</strong>: L2 distance to the underlying expert position
in the driving log in meters.</li>
<li><strong>Off-road events</strong>: we report a failure if the planner
deviates more than 2m laterally from the reference trajectory – this
captures events such as running off-road and into opposing traffic.</li>
<li><strong>Collisions</strong>: collisions of the SDV with any other
agent, broken down into front, side and rear collisions w.r.t. the
SDV.</li>
<li><strong>Comfort</strong>: we monitor the absolute value of
acceleration, and raise a failure should this exceed 3 m/s2.</li>
<li><strong>I1K</strong>: we accumulate safety-critical failures
(collisions and off-road events) into one key metric for ease of
comparison, namely Interventions per 1000 Miles (I1K)</li>
</ul></li>
</ul>
<figure>
<img src="\images\20220803-4.png" alt="仿真结果" />
<figcaption aria-hidden="true">仿真结果</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>策略梯度的推导部分可以继续看看，本文有仿真和实车实验，但方法对比上，对其它算法进行了修改，因此并不完整。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/01/2022-08-01-Autonomous%20Driving%20-%20DriverGym%20Democratising%20Reinforcement%20Learning%20for%20Autonomous%20Driving/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/01/2022-08-01-Autonomous%20Driving%20-%20DriverGym%20Democratising%20Reinforcement%20Learning%20for%20Autonomous%20Driving/" class="post-title-link" itemprop="url">Autonomous Driving | DriverGym Democratising Reinforcement Learning for Autonomous Driving</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-01 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-01T00:00:00+08:00">2022-08-01</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="drivergym-democratising-reinforcement-learning-for-autonomous-driving">DriverGym:
Democratising Reinforcement Learning for Autonomous Driving</h2>
<ul>
<li>现状：目前缺少open-source platform来用real-world
data训练和高效地验证RL算法。</li>
<li>DriverGym 平台的特点：
<ul>
<li>开源（opensource）</li>
<li>与Gym兼容（OpenAI Gym-compatible environment specifically
tailored）</li>
<li>超过1000小时专家记录数据（more than 1000 hours of expert logged
data）</li>
<li>灵活的闭环评估协议（flexible closed-loop evaluation protocol）</li>
<li>提供行为克隆baselines（provide behavior cloning baselines using
supervised learning and RL）</li>
<li>代码：https://lyft.github.io/l5kit/ （已失效）</li>
</ul></li>
<li>环境结构：</li>
</ul>
<figure>
<img src="\images\20220801-2.png" alt="DriverGym" />
<figcaption aria-hidden="true">DriverGym</figcaption>
</figure>
<blockquote>
<p>DriverGym: an open-source gym environment that enables training RL
driving policies on real-world data. The RL policy can access rich
semantic maps to control the ego (<strong>red</strong>). Other agents
(<strong>blue</strong>) can either be simulated from the data logs or
controlled using a dedicated policy pre-trained on real-world data. We
provide an extensible evaluation system (<strong>purple</strong>) with
easily configurable metrics to evaluate the idiosyncrasies of the
trained policies.</p>
</blockquote>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li>自动驾驶开源RL仿真环境的对比</li>
</ul>
<figure>
<img src="\images\20220801-3.png" alt="自动驾驶开源RL仿真环境的对比" />
<figcaption aria-hidden="true">自动驾驶开源RL仿真环境的对比</figcaption>
</figure>
<ul>
<li><p>对仿真平台的几个需求：</p>
<ul>
<li><ol type="1">
<li>易于训练RL策略；be used to easily train RL policies using real-world
logs,</li>
</ol></li>
<li><ol start="2" type="1">
<li>可仿真实际的和反应式的周围智能体行为反应；simulate surrounding agent
behavior that is both realistic and reactive to the ego policy,</li>
</ol></li>
<li><ol start="3" type="1">
<li>可高效评估模型；effectively evaluate the trained models,</li>
</ol></li>
<li><ol start="4" type="1">
<li>设计灵活可调；be flexible in its design,</li>
</ol></li>
<li><ol start="5" type="1">
<li>包容整个相关研究社区；inclusive to the entire research
community,</li>
</ol></li>
</ul></li>
<li><p>使用了最大的公开数据集：Level 5 Prediction Dataset</p>
<ul>
<li>J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V.
Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving
motion prediction dataset. https://level-5.global/level5/data/,
2020.</li>
</ul></li>
<li><p>支持反应式的行为仿真</p>
<ul>
<li>Luca Bergamini, Y. Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del
Pero, Blazej Osinski, Hugo Grimmett, and Peter Ondruska. Simnet:
Learning reactive self-driving simulations from real-world observations.
ArXiv, abs/2105.12332, 2021.</li>
</ul></li>
<li><p>闭环评估系统中提供了自动驾驶相关的评估指标，指标支持扩展和合并，应用于策略的训练</p></li>
<li><p>主要贡献：</p>
<ul>
<li>An open-source and OpenAI gym-compatible environment for autonomous
driving task;</li>
<li>Support for more than 1000 hours of real-world expert data;</li>
<li>Support for logged agents replay or data-driven realistic agent
trajectory simulations;</li>
<li>Configurable and extensible evaluation protocol;</li>
<li>Provide pre-trained models and the corresponding reproducible
training code.</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<ul>
<li><strong>赛车仿真环境（Racing simulators）：</strong>
<ul>
<li><strong>TORCS</strong>：提供了受限的驾驶场景
<ul>
<li>E. Espié, Christophe Guionneau, Bernhard Wymann, Christos
Dimitrakakis, Rémi Coulom, and Andrew Sumner. Torcs, the open racing car
simulator. 2005.</li>
</ul></li>
<li><strong>Highway-Env</strong>：环境与Gym兼容，但缺少交通灯、评估协议和专家数据
<ul>
<li>Edouard Leurent. An environment for autonomous driving
decision-making. https://github.com/eleurent/highway-env, 2018.</li>
</ul></li>
</ul></li>
<li><strong>交通仿真环境（Traffic simulators）</strong>：
<ul>
<li><strong>CARLA</strong>：支持多变的交通情况训练和测试，但周围车辆使用手写规则（hand-coded
rules），真实性有限。
<ul>
<li>A. Dosovitskiy, G. Ros, Felipe Codevilla, Antonio M. López, and V.
Koltun. Carla: An open urban driving simulator. ArXiv, abs/1711.03938,
2017.</li>
</ul></li>
<li><strong>SUMO</strong>：支持多变的交通情况训练和测试，但周围车辆使用手写规则（hand-coded
rules），真实性有限。
<ul>
<li>Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob
Erdmann, Yun-Pang Flötteröd, Robert Hilbrich, Leonhard Lücken, Johannes
Rummel, Peter Wagner, and Evamarie Wießner. Microscopic traffic
simulation using sumo. In The 21st IEEE International Conference on
Intelligent Transportation Systems. IEEE, 2018. URL
https://elib.dlr.de/124092/.</li>
</ul></li>
<li><strong>SMARTS</strong>
：有<code>Social Agent Zoo</code>支持数据驱动周围车辆行为。
<ul>
<li>Ming Zhou, Jun Luo, Julian Villela, Yaodong Yang, David Rusu, Jiayu
Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, Aurora
Chongxi Huang, Ying Wen, Kimia Hassanzadeh, Daniel Graves, Dong Chen,
Zhengbang Zhu, Nhat M. Nguyen, Mohamed Elsayed, Kun Shao, Sanjeevan
Ahilan, Baokuan Zhang, Jiannan Wu, Zhengang Fu, Kasra Rezaee, Peyman
Yadmellat, Mohsen Rohani, Nicolas Perez Nieves, Yihan Ni, Seyedershad
Banijamali, Alexander Cowen Rivers, Zheng Tian, Daniel Palenicek,
Haitham Ammar, Hongbo Zhang, Wulong Liu, Jianye Hao, and Jintao Wang.
Smarts: Scalable multi-agent reinforcement learning training school for
autonomous driving. ArXiv, abs/2010.09776, 2020.</li>
</ul></li>
<li><strong>CRTS</strong>：提供了logs数据接口，使用64小时的真实驾驶数据（real-world
logs）训练周围车辆的行为。集成在Carla中。
<ul>
<li>Blazej Osinski, Piotr Milos, Adam Jakubowski, Pawel Ziecina, Michal
Martyniak, Christopher Galias, Antonia Breuer, Silviu Homoceanu, and
Henryk Michalewski. Carla real traffic scenarios - novel training ground
and benchmark for autonomous driving. ArXiv, abs/2012.11329, 2020.</li>
</ul></li>
<li><strong>DriverGym</strong>：支持反应式的agent使用数据驱动模型学习真实世界的数据，并提供了1000小时真实的驾驶记录可用于仿真agent</li>
</ul></li>
</ul>
<h3 id="drivergym">3 DriverGym</h3>
<ul>
<li><p>模型兼容两个流行的框架训练策略：</p>
<ul>
<li><strong>SB3</strong>：<a
target="_blank" rel="noopener" href="https://github.com/DLR-RM/stable-baselines3">Stable Baselines3
(SB3)</a>是 PyTorch 中强化学习算法的一组可靠实现。它是<a
target="_blank" rel="noopener" href="https://github.com/hill-a/stable-baselines">Stable
Baselines</a>的下一个主要版本。
<ul>
<li>Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi
Kanervisto, and Noah Dormann. Stable baselines3.
https://github.com/DLR-RM/stable-baselines3, 2019.</li>
</ul></li>
<li><strong>RLlib</strong>：RLlib是一个开源强化学习库,提供了高度可扩展能力和不同应用的统一的<a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=API&amp;spm=1001.2101.3001.7020">API</a>。RLlib原生支持Tensorflow，Tensorflow
Eager，以及PyTorch，但其内部与这些框架无关。
<ul>
<li>Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox,
Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica.
Rllib: Abstractions for distributed reinforcement learning. In ICML,
2018.</li>
</ul></li>
</ul></li>
<li><p>例子场景：绿线是策略预测的轨迹</p>
<figure>
<img src="\images\20220801-4.png"
alt="Visualization of an episode rollout (ego in red, agents in blue) in DriverGym. The policy prediction (green line) is scaled by factor of 10 and shown at 2 second intervals for better viewing" />
<figcaption aria-hidden="true">Visualization of an episode rollout (ego
in red, agents in blue) in DriverGym. The policy prediction (green line)
is scaled by factor of 10 and shown at 2 second intervals for better
viewing</figcaption>
</figure></li>
<li><p>action space：行为为 <span class="math display">\[(x; y;
yaw)\]</span> (yaw是朝向)，用于更新ego-agent行为；环境输出 <span
class="math display">\[(acceleration; steer)\]</span>
用于计算下一时刻的observation</p></li>
<li><p>reactive agent：允许周围车辆使用两种方式控制：</p>
<ul>
<li>log replay：回放真实世界中收集的数据；</li>
<li>reactive simulation：可使用真实世界数据训练neural-network-based
agent models，用于控制车辆行为</li>
</ul></li>
<li><p>reward：支持不同的自动驾驶评价指标，在每一帧进行评价计算，指标可以整合为奖励函数。</p></li>
</ul>
<h3 id="总结">总结</h3>
<p>整体来看，支持Gym环境大大方便了仿真和调试，一些细节问题由于没有实际使用该环境还不清楚，比如车辆密度、速度、周围车辆的观测质量、轨迹质量等。</p>
<p>本文作者正对更多细粒度场景设计评估方案，例如或绿灯路口重新启动等。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/01/2022-08-01-Autonomous%20Driving%20-%20Model-based%20offline%20planning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/01/2022-08-01-Autonomous%20Driving%20-%20Model-based%20offline%20planning/" class="post-title-link" itemprop="url">Autonomous Driving | Model-based offline planning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-01 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-01T00:00:00+08:00">2022-08-01</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="model-based-offline-planning">Model-based offline planning</h2>
<ul>
<li>由于成本、安全性等因素，很多情况下不能够直接与系统交互来学习控制策略，因此，只能从记录的log数据中学习控制策略（offline
reinforcement
learning）。本文介绍了一种从log数据中学到超越成圣log数据的原策略的新策略的方法，命名为
model-based ofline planning (MBOP)。</li>
</ul>
<h3 id="introduction">Introduction</h3>
<ul>
<li>Offline reinforcement learning包括：
<ul>
<li>model-free方法：
<ul>
<li>Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized
offline reinforcement learning. CoRR, abs/1911.11361, 2019. URL
http://arxiv.org/abs/1911.11361.</li>
<li>Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
Advantage-weighted regression: Simple and scalable off-policy
reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.</li>
<li>Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep
reinforcement learning without exploration. In International Conference
on Machine Learning, pp. 2052–2062, 2019.</li>
<li>Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost
Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar
Gulcehre, Nicolas Heess, et al. Critic regularized regression. Advances
in Neural Information Processing Systems, 33, 2020.</li>
</ul></li>
<li>model-based方法：MOPO,
MoREL学习一个模型，然后用于训练一个无模型策略，这种模式和Dyna模式类似。
<ul>
<li><strong>MOPO</strong>: Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano
Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo:
Model-based offline policy optimization. arXiv preprint
arXiv:2005.13239, 2020.</li>
<li><strong>MoREL</strong>: Rahul Kidambi, Aravind Rajeswaran, Praneeth
Netrapalli, and Thorsten Joachims. Morel: Modelbased offline
reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.</li>
<li><strong>Dyna</strong>: Richard S Sutton and Andrew G Barto.
Reinforcement learning: An introduction. MIT press, 2018.</li>
</ul></li>
</ul></li>
<li>本文的算法属于model-based，利用model-predictive control (MPC)
，扩展MPPI轨迹规划器，并使用实时规划，产生目标或满足奖励条件的策略。
<ul>
<li>Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M
Rehg, Byron Boots, and Evangelos A Theodorou. Information theoretic mpc
for model-based reinforcement learning. In 2017 IEEE International
Conference on Robotics and Automation (ICRA), pp. 1714–1721. IEEE,
2017b.</li>
</ul></li>
<li>本文模型MBOP包含三个要素：
<ul>
<li>a learnt <strong>world model</strong>,</li>
<li>a learnt <strong>behavior-cloning policy</strong>,</li>
<li>a learnt <strong>fixed-horizon value-function</strong>.</li>
</ul></li>
<li>MBOP的核心优势是<strong>数据高效</strong>和<strong>自适应</strong>。只需仅100秒就可以训练出一个和奖励函数、目标状态、基于状态的约束相适应的策略。</li>
<li>MBOP能够对非平稳目标和约束执行zero-shot自适应，但是没有处理非平稳动力学特性的机制。</li>
</ul>
<h3 id="model-based-offline-planning-1">Model-based offline
planning</h3>
<ul>
<li>描述问题为Markov Decision Process (MDP)，<span
class="math display">\[(S,A,p,r,\gamma)\]</span>
<ul>
<li><span class="math display">\[s\]</span>是系统状态</li>
<li><span class="math display">\[a\]</span>是行为</li>
<li><span class="math display">\[p(s_{t+1}\mid
s_t,a_t)\]</span>是状态转移概率</li>
<li><span class="math display">\[r(s_t,a_t,s_{t+1})\]</span>是奖励</li>
<li><span class="math display">\[\gamma=1\]</span>是时间折扣系数</li>
</ul></li>
<li>MBOP包括三个函数近似器：
<ul>
<li><span
class="math display">\[f_m\]</span>：环境动力学的单步模型，<span
class="math display">\[(\hat{r}_t,\hat{s}_{t+1})=f_m(s_t,a_t)\]</span>，本文使用<span
class="math display">\[f_m(s_t,a_t)_s\]</span>表示状态预测，使用<span
class="math display">\[f_m(s_t,a_t)_r\]</span>表示奖励预测。</li>
<li><span
class="math display">\[f_b\]</span>：表示一个行为克隆网络，<span
class="math display">\[a_t=f_b(s_t,a_{t-1})\]</span>，被规划算法用来引导轨迹采样的先验。</li>
<li><span
class="math display">\[f_R\]</span>：是一个阉割的值函数，提供在状态s中采取行为a后，在固定界限<span
class="math display">\[R_H\]</span>上的收益。<span
class="math display">\[\hat{R}_H = f_R(s_t,a_{t-1})\]</span></li>
</ul></li>
<li>MBOP-POLICY
<ul>
<li>使用MPC输出每个新状态下的行为（<span
class="math display">\[a_t=\pi(s_t)\]</span>）。MPC在每一时间步执行一个固定长度的规划，返回长度为H的轨迹T。选择该轨迹的第一个行为<span
class="math display">\[a_t\]</span>并返回。</li>
</ul></li>
</ul>
<figure>
<img src="\images\20220802-1.png" alt="High-Level MBOP-Policy" />
<figcaption aria-hidden="true">High-Level MBOP-Policy</figcaption>
</figure>
<ul>
<li>MBOP-TRAJOPT
<ul>
<li>在PDDM的基础上增加一个策略先验<span
class="math display">\[f_b\]</span>和价值预测<span
class="math display">\[f_R\]</span></li>
</ul></li>
</ul>
<figure>
<img src="\images\20220802-2.png" alt="MBOP-Trajopt" />
<figcaption aria-hidden="true">MBOP-Trajopt</figcaption>
</figure>
<blockquote>
<p>P.S.：第11行在<span
class="math display">\[f_b\]</span>给出的行为上加权了采样轨迹的行为，其含义可能是希望在网络没有收敛时，记录下来的行为也不要偏差太大，都保持在采样轨迹附近，参数<span
class="math display">\[\beta\]</span>可被视为学习率。第17行给出多条轨迹中奖励最大的作为输出（re-weighting）</p>
</blockquote>
<h3 id="experimental-results">Experimental results</h3>
<ul>
<li><p>首先，在非常少的数据中心训练，其次，再迁移到基于相同系统动力学的两种novel
tasks中：</p>
<ul>
<li><strong>goal-conditioned tasks</strong> (that ignore the original
reward function)</li>
<li><strong>constrained tasks</strong> (that require optimising for the
original reward under some state constraint)</li>
</ul></li>
<li><p>使用的数据集RL Unplugged (RLU) 和 D4RL</p>
<ul>
<li><strong>RL Unplugged (RLU)</strong>：Caglar Gulcehre, Ziyu Wang,
Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Kon- ´rad
Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru,
et al. Rl unplugged: Benchmarks for offline reinforcement learning.
arXiv preprint arXiv:2006.13888, 2020.
<ul>
<li>cartpole-swingup</li>
<li>walker</li>
<li>quadruped</li>
</ul></li>
<li><strong>D4RL</strong>：Justin Fu, Aviral Kumar, Ofir Nachum, George
Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven
reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
<ul>
<li>halfcheetah</li>
<li>hopper</li>
<li>walker2d</li>
<li>Adroit</li>
</ul></li>
</ul></li>
<li><p>对于 RLU 中的 Quadruped 和 Walker
任务，由于数据集中性能高方差，在训练 <span
class="math display">\[f_b\]</span> 和 <span
class="math display">\[f_R\]</span>
的过程中，通过设定阈值，舍弃了性能不好的数据。 使用未过滤的数据来训练
<span class="math display">\[f_s\]</span></p></li>
<li><p>对于所有的数据集，90%用于训练，10%用于测试验证</p></li>
<li><p>性能：For the RLU datasets (Fig. 1), we observe that MBOP is able
to find a near-optimal policy on most dataset sizes in Cartpole and
Quadruped with as little as <strong>5000 steps</strong>, which
corresponds to <strong>5 episodes</strong>, or approximately 50 seconds
on Cartpole and 100 seconds on Quadruped. On the Walker datasets MBOP
requires 23 episodes (approx. 10 minutes) before it finds a reasonable
policy, and with sufficient data converges to a score of 900 which is
near optimal. On most tasks, MBOP is able to generate a policy
significantly better than the behavior data as well as the the BC
prior.</p></li>
<li><p>MBOP模型容易适应新的目标函数，例如添加新的子目标函数<span
class="math display">\[R&#39;_n\]</span>时， <span
class="math display">\[
R&#39;_n = \sum_t f_{obj}(s_t)
\]</span> 其中，<span
class="math display">\[f_{obj}\]</span>是用户自定义的目标函数。只需要将轨迹更新规则改为：
<span class="math display">\[
T_t=\frac{\sum_{n=1}^N e^{kR_n+k_{obj}R&#39;_n}A_{n,t}}{\sum_{n=1}^N
e^{kR_n+k_{obj}R&#39;_n}}
\]</span></p></li>
<li><p>为了验证上述模型的适应能力，进行了两个实验：</p>
<ul>
<li><strong>goal-conditioned control</strong>（忽略原始奖励，<span
class="math display">\[k=0\]</span>，学习新奖励）</li>
<li><strong>constrained control</strong>（增加了state-based
constraint，然后探索合适的 <span class="math display">\[k\]</span> 和
<span class="math display">\[k_{obj}\]</span> ）</li>
</ul></li>
</ul>
<figure>
<img src="\images\20220802-3.png" alt="ZERO-SHOT TASK ADAPTATION" />
<figcaption aria-hidden="true">ZERO-SHOT TASK ADAPTATION</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>MBOP为策略生成提供了一种易于实施、数据高效、稳定且灵活的算法。</p>
<p>由于使用了在线规划，使其能够应对变化的目标、成本和环境限制。</p>
<p>但是算法没有在更复杂的场景和约束条件下测试，因此适用范围和效果还缺少验证。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/30/">30</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Txing"
      src="/images/my_photo.jpg">
  <p class="site-author-name" itemprop="name">Txing</p>
  <div class="site-description" itemprop="description">泛用人形决战型机器人博士</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">235</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/txing-casia" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;txing-casia" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.uomi.moe/" title="https:&#x2F;&#x2F;blog.uomi.moe" rel="noopener" target="_blank">驱逐舰患者</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://m.mepai.me/photographyer/u_5a68085ba15aa.html?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;m.mepai.me&#x2F;photographyer&#x2F;u_5a68085ba15aa.html?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">隐之-INF</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Txing</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">577k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">8:45</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
