<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"txing-casia.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="泛用类人决战型机器人博士">
<meta property="og:type" content="website">
<meta property="og:title" content="Txing">
<meta property="og:url" content="https://txing-casia.github.io/page/9/index.html">
<meta property="og:site_name" content="Txing">
<meta property="og:description" content="泛用类人决战型机器人博士">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Txing">
<meta property="article:tag" content="Txing">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://txing-casia.github.io/page/9/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Txing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Txing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到 | 伽蓝之堂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2021/07/18/2021-07-18-Q688-%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87-%E4%B8%AD%E7%AD%89-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/18/2021-07-18-Q688-%E9%A9%AC%E5%9C%A8%E6%A3%8B%E7%9B%98%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87-%E4%B8%AD%E7%AD%89-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" class="post-title-link" itemprop="url">Q688-马在棋盘上的概率-中等-动态规划</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-18T00:00:00+08:00">2021-07-18</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>3k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>3 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="马在棋盘上的概率"><a
target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/knight-probability-in-chessboard/">688.
“马”在棋盘上的概率</a></h4>
<h2 id="question">Question</h2>
<blockquote>
<p>已知一个 NxN 的国际象棋棋盘，棋盘的行号和列号都是从 0
开始。即最左上角的格子记为 (0, 0)，最右下角的记为 (N-1, N-1)。</p>
<p>现有一个 “马”（也译作 “骑士”）位于 (r, c) ，并打算进行 K 次移动。</p>
<p>如下图所示，国际象棋的 “马” 每一步先沿水平或垂直方向移动 2
个格子，然后向与之相垂直的方向再移动 1 个格子，共有 8 个可选的位置。</p>
<p>现在 “马”
每一步都从可选的位置（包括棋盘外部的）中独立随机地选择一个进行移动，直到移动了
K 次或跳到了棋盘外面。</p>
<p>求移动结束后，“马” 仍留在棋盘上的概率。</p>
</blockquote>
<blockquote>
<p><strong>Example 1:</strong></p>
<p>输入: 3, 2, 0, 0 输出: 0.0625 解释: 输入的数据依次为 N, K, r, c 第 1
步时，有且只有 2 种走法令 “马”
可以留在棋盘上（跳到（1,2）或（2,1））。对于以上的两种情况，各自在第2步均有且只有2种走法令
“马” 仍然留在棋盘上。 所以 “马” 在结束后仍在棋盘上的概率为 0.0625。</p>
</blockquote>
<blockquote>
<p><strong>Note:</strong></p>
<ul>
<li><code>N</code> 的取值范围为 [1, 25]</li>
<li><code>K</code> 的取值范围为 [0, 100]</li>
<li>开始时，“马” 总是位于棋盘上</li>
</ul>
</blockquote>
<h3 id="approach-1-动态规划">Approach 1: 动态规划</h3>
<ul>
<li>令 <code>f[r][c][steps]</code> 代表马在位置 <code>(r, c)</code>
移动了 <code>steps</code>
次以后还留在棋盘上的概率，根据马的移动方式，我们有以下递归：</li>
</ul>
<p><span class="math display">\[
f[r][c][steps]=\sum_{dr,dc} f[r+dr][c+dc][steps-1]\cdot \frac{1}{8}
\]</span></p>
<ul>
<li>根据题目我们可以知道 (dr, dc) 的可能数据对是 (2, 1), (2, -1), (-2,
1), (-2, -1), (1, 2), (1, -2), (-1, 2), (-1, -2)。</li>
<li>我们将使用二维的 dp 和 dp2 来存储我们的数据，而不是使用三维数组 f。
<ul>
<li><code>dp2</code> 代表 <code>f[][][steps]</code>；</li>
<li><code>dp</code> 代表 <code>f[][][steps-1]</code>；</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">knightProbability</span>(<span class="params">self, N, K, r, c</span>):</span><br><span class="line">        dp = [[<span class="number">0</span>] * N <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">        dp[r][c] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(K): <span class="comment"># 走k步还在棋盘内的概率</span></span><br><span class="line">            dp2 = [[<span class="number">0</span>] * N <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">            <span class="keyword">for</span> r, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(dp):</span><br><span class="line">                <span class="keyword">for</span> c, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(row):</span><br><span class="line">                    <span class="keyword">for</span> dr, dc <span class="keyword">in</span> ((<span class="number">2</span>,<span class="number">1</span>),(<span class="number">2</span>,-<span class="number">1</span>),(-<span class="number">2</span>,<span class="number">1</span>),(-<span class="number">2</span>,-<span class="number">1</span>),</span><br><span class="line">                                   (<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,-<span class="number">2</span>),(-<span class="number">1</span>,<span class="number">2</span>),(-<span class="number">1</span>,-<span class="number">2</span>)):</span><br><span class="line">                        <span class="comment"># 走了一步之后还在棋盘内</span></span><br><span class="line">                        <span class="keyword">if</span> <span class="number">0</span> &lt;= r + dr &lt; N <span class="keyword">and</span> <span class="number">0</span> &lt;= c + dc &lt; N:</span><br><span class="line">                            dp2[r+dr][c+dc] += val / <span class="number">8.0</span></span><br><span class="line">            dp = dp2</span><br><span class="line">        <span class="comment"># map()函数把sum依次作用在dp的每一项上，在map()外再使用sum()完成二维数组的求和</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">map</span>(<span class="built_in">sum</span>, dp))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    N, K, r, c = <span class="number">3</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    solution=Solution().solve(N, K, r, c)</span><br><span class="line">    <span class="built_in">print</span>(solution)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p><strong>复杂度分析</strong></p>
<ul>
<li>时间复杂度：<span class="math display">\[O(N^2 K)\]</span>。其中 N,
K 为题目中的定义。我们对 <span class="math display">\[N^2\]</span>
元素的每一层 dp 进行 O(1) 工作，并且考虑了 K 层。</li>
<li>空间复杂度：<span class="math display">\[O(N^2)\]</span>，dp 和 dp2
的大小。</li>
</ul>
<h3 id="approach-2-矩阵求幂">Approach 2: 矩阵求幂</h3>
<p>方法 1 中表示的状态重复表达了过渡到其他的线性组合的状态。
任何情况下，我们都可以将整个转换表示为这些线性组合的矩阵。然后，这个矩阵的第
n 次方代表了 n 移动的转换，因此我们可以将问题简化为矩阵求幂问题。</p>
<p><strong>算法：</strong></p>
<ul>
<li>首先，我们可以利用棋盘上的对称性。马可能有 n^2
的状态（假设它在板上）。由于横轴、纵轴和对角线的对称性，我们可以假设骑士在棋盘的左上象限，并且列数等于或大于行数。对于任何一个位置，通过满足条件通过轴反射得到位置将是该位置的标准索引。</li>
<li>这将使状态数从 <span class="math display">\[N^2\]</span> 减少到大约
<span class="math display">\[\frac{N^2}{8}\]</span> ，这使得在这个 <span
class="math display">\[O(\frac{N^2}{8}) \times O(\frac{N^2}{8})\]</span>
矩阵上下求幂大约快 <span class="math display">\[8^3\]</span> 倍。</li>
<li>现在，如果我们知道每一个状态在一次移动后都会变成某种状态的线性组合，那么让我们写一个过渡矩阵
<span class="math display">\[\mathcal{T}\]</span>，其中 <span
class="math display">\[\mathcal{T}\]</span> 的第 i 行代表了第 i
个状态的线性组合。然后，<span
class="math display">\[\mathcal{T}^n\]</span> 表示 n
次移动的转换，我们需要第 i 行的总和，其中 i 是起始位置的索引。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">knightProbability2</span>(<span class="params">self, N, K, sr, sc</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">canonical</span>(<span class="params">r, c</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="number">2</span> * r &gt; N: r = N - <span class="number">1</span> - r</span><br><span class="line">            <span class="keyword">if</span> <span class="number">2</span> * c &gt; N: c = N - <span class="number">1</span> - c</span><br><span class="line">            <span class="keyword">if</span> r &gt; c: r, c = c, r</span><br><span class="line">            <span class="keyword">return</span> r*N + c</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">matrix_mult</span>(<span class="params">A, B</span>):</span><br><span class="line">            ZB = <span class="built_in">zip</span>(*B)</span><br><span class="line">            <span class="keyword">return</span> [[<span class="built_in">sum</span>(a * b <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(row, col))</span><br><span class="line">                    <span class="keyword">for</span> col <span class="keyword">in</span> ZB] <span class="keyword">for</span> row <span class="keyword">in</span> A]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">matrix_expo</span>(<span class="params">A, K</span>):</span><br><span class="line">            <span class="keyword">if</span> K == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> [[+(i==j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="built_in">len</span>(A))]</span><br><span class="line">                        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="built_in">len</span>(A))]</span><br><span class="line">            <span class="keyword">if</span> K == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> A</span><br><span class="line">            <span class="keyword">elif</span> K % <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">return</span> matrix_mult(matrix_expo(A, K-<span class="number">1</span>), A)</span><br><span class="line">            B = matrix_expo(A, K/<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> matrix_mult(B, B)</span><br><span class="line"></span><br><span class="line">        index = [<span class="number">0</span>] * (N*N)</span><br><span class="line">        t = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> xrange(N):</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> xrange(N):</span><br><span class="line">                <span class="keyword">if</span> r*N + c == canonical(r, c):</span><br><span class="line">                    index[r*N + c] = t</span><br><span class="line">                    t += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    index[r*N + c] = index[canonical(r, c)]</span><br><span class="line"></span><br><span class="line">        T = []</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> xrange(N):</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> xrange(N):</span><br><span class="line">                <span class="keyword">if</span> r*N + c == canonical(r, c):</span><br><span class="line">                    row = [<span class="number">0</span>] * t</span><br><span class="line">                    <span class="keyword">for</span> dr, dc <span class="keyword">in</span> ((<span class="number">2</span>,<span class="number">1</span>),(<span class="number">2</span>,-<span class="number">1</span>),(-<span class="number">2</span>,<span class="number">1</span>),(-<span class="number">2</span>,-<span class="number">1</span>),</span><br><span class="line">                                    (<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,-<span class="number">2</span>),(-<span class="number">1</span>,<span class="number">2</span>),(-<span class="number">1</span>,-<span class="number">2</span>)):</span><br><span class="line">                        <span class="keyword">if</span> <span class="number">0</span> &lt;= r+dr &lt; N <span class="keyword">and</span> <span class="number">0</span> &lt;= c+dc &lt; N:</span><br><span class="line">                            row[index[(r+dr)*N + c+dc]] += <span class="number">0.125</span></span><br><span class="line">                    T.append(row)</span><br><span class="line"></span><br><span class="line">        Tk = matrix_expo(T, K)</span><br><span class="line">        i = index[sr * N + sc]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(Tk[i])</span><br></pre></td></tr></table></figure>
<p><strong>复杂度分析</strong></p>
<ul>
<li><p>时间复杂度：<span class="math display">\[O(N^6
\log(K))\]</span>。其中 N, K 为题目中的定义。大约有 <span
class="math display">\[\frac{N^2}{8}\]</span>
规范状态，这使得我们的矩阵乘法 <span
class="math display">\[O(N^6)\]</span>。为了找到这个矩阵的第 K
次幂，我们做了 <span class="math display">\[O(\log(K))\]</span>
matrix乘法。</p></li>
<li><p>空间复杂度：<span
class="math display">\[O(N^4)\]</span>，矩阵有大约 <span
class="math display">\[\frac{N^4}{64}\]</span> 个元素。</p></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2021/07/16/2021-07-16-Reinforcement%20Learning%20-%20On%20Learning%20Intrinsic%20Rewards%20for%20Policy%20Gradient%20Methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/16/2021-07-16-Reinforcement%20Learning%20-%20On%20Learning%20Intrinsic%20Rewards%20for%20Policy%20Gradient%20Methods/" class="post-title-link" itemprop="url">Reinforcement Learning | On Learning Intrinsic Rewards for Policy Gradient Methods</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-16 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-16T00:00:00+08:00">2021-07-16</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>5.4k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="on-learning-intrinsic-rewards-for-policy-gradient-methods">On
Learning Intrinsic Rewards for Policy Gradient Methods</h1>
<p>论文链接：https://dl.acm.org/doi/pdf/10.5555/3327345.3327375</p>
<h2 id="主要内容">主要内容</h2>
<ul>
<li>本文提出基于 <strong>optimal reward framework</strong> [Singh et
al., 2010] 的本质奖励学习方法，并和 PPO 算法（Mujoco）结合</li>
</ul>
<p>Satinder Singh, Richard L Lewis, Andrew G Barto, and Jonathan Sorg.
Intrinsically motivated reinforcement learning: An evolutionary
perspective. IEEE Transactions on Autonomous Mental Development,
2(2):70–82, 2010.</p>
<ul>
<li>在一些游戏场景中缺少清晰的奖励函数选择方式。有时候存在多个目标，最小化能量消耗，最大化吞吐量，最小化延时等（minimizing
energy consumption and maximizing throughput and minimizing
latency）</li>
<li>早期的工作证明了最优策略的奖励函数并不唯一，例如使用一个势能函数（potential-based
reward [Ng et al., 1999]）的奖励函数不会改变最优策略。</li>
</ul>
<p>Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance
under reward transformations: Theory and application to reward shaping.
In Proceedings of the Sixteenth International Conference on Machine
Learning, pages 278–287. Morgan Kaufmann Publishers Inc., 1999.</p>
<ul>
<li><p>另一方面，由于实际任务中的各种限制（e.g., inadequate memory,
representational capacity, computation, training data,
etc.），智能体很难学习到最优策略。因此，在解决奖励设计问题时，希望设计能改变最优策略的奖励函数变换方式。</p></li>
<li><p>构造奖励函数的好处：</p>
<ul>
<li>比原始的奖励函数更少的稀疏性，加快学习过程 [Rajeswaran et al.,
2017]；</li>
<li>帮助探索状态边界的区域，例如count-based reward
鼓励探索很少探索到的区域 [Bellemare et al., 2016, Ostrovski et al.,
2017, Tang et al., 2017]；</li>
</ul></li>
<li><p>相关的方法：</p>
<ul>
<li>preference elicitation</li>
<li>inverse RL</li>
<li>intrinsically motivated RL</li>
<li>optimal rewards</li>
<li>potentialbased shaping rewards</li>
<li>more general reward shaping</li>
<li>mechanism design</li>
</ul></li>
<li><p>主要贡献：</p>
<ul>
<li>基于策略梯度的方法推导intrinsic rewards学习方法，并与
task-specifying (hereafter extrinsic) reward
相结合，最大化外部奖励。</li>
<li>内部和外部奖励之和的参数通过策略梯度方法更新训练</li>
</ul></li>
<li><p>使用本质奖励的主要原因是对RL
agent的训练都是有限的（表征能力，可得到的数据，计算能力限制等等），通过集合内部奖励可以环节这些限制。The
main intuition is that in practice all RL agents are bounded
(computationally, representationally, in terms of data availability,
etc.) and the optimal intrinsic reward can help mitigate these
bounds.</p></li>
</ul>
<h3 id="background-and-related-work">Background and Related Work</h3>
<ul>
<li><p><strong>Optimal rewards and reward design</strong></p>
<p>Sorg et al.
[2010]提出了PGRD算法，这是一种可扩展的算法，仅适用于基于前瞻搜索(例如，UCT)的规划agent(因此agent本身不是基于学习的agent；只学习与固定规划者一起使用的奖励)。Sorg
et al. [2010] introduced PGRD (Policy Gradient for Reward Design), a
scalable algorithm that only works with lookahead-search (e.g., UCT)
based planning agents (and hence the agent itself is not a
learning-based agent; only the reward to use with the fixed planner is
learned).</p></li>
<li><p><strong>Reward shaping and Auxiliary rewards</strong></p>
<ul>
<li><p>Reward shaping [Ng et al.,
1999]提供了一个一般性的答案，说明什么样的奖励函数修改空间不会改变最优策略，特别是基于势能的奖励函数。Reward
shaping [Ng et al., 1999] provides a general answer to what space of
reward function modifications do not change the optimal policy,
specifically potential-based rewards.</p></li>
<li><p>The <strong>UNREAL</strong> agent <strong>[Jaderberg et al.,
2016]</strong> used pseudo-reward computed from unsupervised auxiliary
tasks to refine its internal representations.</p>
<p>Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul,
Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement
learning with unsupervised auxiliary tasks. arXiv preprint
arXiv:1611.05397, 2016.</p></li>
<li><p>In <strong>Bellemare et al. [2016]</strong>, <strong>Ostrovski et
al. [2017]</strong>, and <strong>Tang et al. [2017]</strong>, a
pseudo-count based reward bonus was given to the agent to encourage
exploration</p></li>
<li><p><strong>Pathak et al. [2017]</strong> used self-supervised
prediction errors as intrinsic rewards to help the agent
explore.</p></li>
<li><p>本文的主要出发点是学习本质奖励，使其能够映射高维的观测和行为到奖励中。The
main departure point in this paper is that we learn the parameters of an
intrinsic reward function that maps high-dimensional observations and
actions to rewards.</p></li>
</ul></li>
<li><p><strong>Hierarchical RL</strong></p>
<ul>
<li>另一类的的本质奖励在分层强化学习中使用，比如FeUdal
Network通过manager和worker在不同时间尺度尽心学习，manager学习抽象的目标指挥worker最大化外部奖励</li>
<li>Finally, another difference is that hierarchical RL typically treats
the lower-level learner as a black box while we train the intrinsic
reward using gradients through the policy module in our
architecture.</li>
</ul></li>
<li><p><strong>Meta Learning for RL</strong></p>
<ul>
<li>这篇工作也可以被当做meta-RL方向的工作</li>
<li>与之前工作的区别：在单个任务中元学习到内部奖励，而不是快速适应新的任务。However,
a key distinction from the prior work on meta learning for RL [Finn et
al., 2017, Duan et al., 2017, Wang et al., 2016, Duan et al., 2016b] is
that our method aims to meta-learn intrinsic rewards within a single
task, whereas much of the prior work is designed to quickly adapt to new
tasks in a few-shot learning scenario.</li>
</ul></li>
</ul>
<h3
id="gradient-based-learning-of-intrinsic-rewards-a-derivation">Gradient-Based
Learning of Intrinsic Rewards: A Derivation</h3>
<h4 id="policy-gradient-based-rl">Policy Gradient based RL</h4>
<ul>
<li><p>The value of a policy <span
class="math display">\[J(\theta)=E[\sum^{\infty}_{t=0}
\gamma^tr_t]\]</span></p></li>
<li><p>the gradient of the value <span
class="math display">\[J(\theta)\]</span> is<br />
<span class="math display">\[
\nabla_{\theta} J(\theta)=E_{\theta}[G(s_t,a_t) \nabla_{\theta} \log
\pi_{\theta}(a_t|s_t)]\\
G(s_t,a_t)=\sum^{\infty}_{i=t} \gamma^{i-t}r_i
\]</span> G是到终点为止的return</p></li>
</ul>
<h4 id="lirpg-learning-intrinsic-rewards-for-policy-gradient">LIRPG:
Learning Intrinsic Rewards for Policy Gradient</h4>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210719-1.png"
alt="Learning Intrinsic Rewards for Policy Gradient" />
<figcaption aria-hidden="true">Learning Intrinsic Rewards for Policy
Gradient</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210719-2.png"
alt="Algorithm 1 LIRPG: Learning Intrinsic Reward for Policy Gradient" />
<figcaption aria-hidden="true">Algorithm 1 LIRPG: Learning Intrinsic
Reward for Policy Gradient</figcaption>
</figure>
<ul>
<li><p><strong>Updating Policy Parameters (θ)</strong></p>
<p>使用内部和外部奖励之和来更新 <span
class="math display">\[\theta\]</span><br />
<span class="math display">\[
\begin{align}
\theta&#39;&amp;=\theta+\alpha\nabla_{\theta} J^{ex+in}(\theta)\\
&amp;\approx \theta +\alpha G^{ex+in}(s_t,a_t) \nabla_{\theta} \log
\pi_{\theta}(a_t|s_t)
\end{align}
\]</span></p></li>
<li><p><strong>Updating Intrinsic Reward Parameters
(η)</strong></p></li>
</ul>
<p>Given an episode and the updated policy parameters <span
class="math display">\[\theta&#39;\]</span> , we update intrinsic reward
parameters.<br />
<span class="math display">\[
\begin{align}
\nabla_{\eta}
J^{ex}&amp;=\nabla_{\theta&#39;}J^{ex}\nabla_{\eta}\theta&#39;\\
\nabla_{\theta&#39;}J^{ex} &amp;\approx G^{ex}(s_t,a_t)
\nabla_{\theta&#39;}\log \pi_{\theta&#39;}(a_t \mid s_t)\\
\nabla_{\eta} \theta&#39; &amp; =\nabla_{\eta}(\theta+\alpha
G^{ex+in}(s_t,a_t)\nabla_{\theta}\log \pi_{\theta}(a_t|s_t))\\
&amp;= \nabla_{\eta}(\alpha G^{ex+in}(s_t,a_t) \nabla_{\theta}\log
\pi_{\theta}(a_t|s_t)))\\
&amp;= \nabla_{\eta}(\alpha \lambda G^{in}(s_t,a_t) \nabla_{\theta}\log
\pi_{\theta}(a_t|s_t)))\\
&amp;= \alpha \lambda \sum^{\infty}_{i=t} \gamma^{i-t}\nabla_{\eta}
r^{in}_{\eta}(s_i,a_i)\nabla_{\theta} \log \pi_{\theta}(a_t\mid s_t)
\end{align}  
\]</span> The importance sampling:<br />
<span class="math display">\[
\nabla_{\theta&#39;}
J^{ex}=G^{ex}(s_t,a_t)\frac{\nabla_{\theta&#39;}\pi_{\theta&#39;}(a_t\mid
s_t)}{\pi_{\theta}(a_t\mid s_t)}\\  
\]</span>
重要性采样就是假设数据属于p(x)分布，但是实际采样是以q(x)分布采样的，需要乘以一个权重，把数据转换到p(x)分布，也就是重要性权重。</p>
<h2 id="总结">总结</h2>
<p>整个工作还是很漂亮的，尤其的这两层的梯度推导（需要结合途中公式和文本中的公式来看）。后面考虑再看几篇本质reward的文章，看看还有什么构造方式。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2021/07/14/2021-07-14-Reinforcement%20Learning%20-%20Hierarchical%20Deep%20Reinforcement%20Learning%20Integrating%20Temporal%20Abstraction%20and%20Intrinsic%20Motivation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/14/2021-07-14-Reinforcement%20Learning%20-%20Hierarchical%20Deep%20Reinforcement%20Learning%20Integrating%20Temporal%20Abstraction%20and%20Intrinsic%20Motivation/" class="post-title-link" itemprop="url">Reinforcement Learning | Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-14 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-14T00:00:00+08:00">2021-07-14</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.6k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1
id="hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation">Hierarchical
Deep Reinforcement Learning: Integrating Temporal Abstraction and
Intrinsic Motivation</h1>
<p>论文链接：https://proceedings.neurips.cc/paper/2016/file/f442d33fa06832082290ad8544a8da27-Paper.pdf</p>
<h2 id="主要内容">主要内容</h2>
<ul>
<li><p>稀疏反馈下，不充分的探索使得agent很难学习到鲁棒的行为策略。本文通过设计本质动机来驱动行为探索，并结合分层的结构来完成行为，最后在奖励稀疏和延迟的任务中进行了实验。</p></li>
<li><p>结构分为两层，顶层的meta-controller学习定义子目标，底层的controller学习根据状态和子目标执行行动，直到目标完成或者情节结束。</p></li>
<li><p>这项工作是基于 option 框架完成的。相关的分层强化学习框架还有 MAXQ
等。这篇文章没有对每个option单独使用Q函数。这一点和 [21]
是一致的，这会带来两个好处：（1）不同选项之间存在共享学习；（2）该模型可扩展到大量options的情况；</p>
<p>[21]. T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value
function approximators. In Proceedings of the 32nd International
Conference on Machine Learning (ICML-15), pages 1312–1320,
2015.</p></li>
<li><p>In another paper, Singh et al. [26] take an evolutionary
perspective to optimize over the space of reward functions for the
agent, leading to a notion of extrinsically and intrinsically motivated
behavior.</p></li>
<li><p>Cognitive Science and Neuroscience The nature and origin of
intrinsic goals in humans is a thorny issue but there are some notable
insights from existing literature. There is converging evidence in
developmental psychology that human infants, primates, children, and
adults in diverse cultures base their core knowledge on certain
cognitive systems including – entities, agents and their actions,
numerical quantities, space, social-structures and intuitive theories
[29]. During curiositydriven activities, toddlers use this knowledge to
generate intrinsic goals such as building physically stable block
structures. In order to accomplish these goals, toddlers seem to
construct subgoals in the space of their core knowledge. Knowledge of
space can also be utilized to learn a hierarchical decomposition of
spatial environments. This has been explored in Neuroscience with the
successor representation, which represents value functions in terms of
the expected future state occupancy. Decomposition of the successor
representation have shown to yield reasonable subgoals for spatial
navigation problems [5, 30].</p></li>
<li><p>MDP过程中的高效探索是一个难题。<span
class="math display">\[\epsilon-greedy\]</span>
在局部探索中表现还好，但是对于整个状态空间不同区域的探索比较失败。这里通过设计本质目标
<span class="math display">\[g\]</span> 和本质奖励intrinsic
rewards</p></li>
</ul>
<p><img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210714-1.png" /></p>
<ul>
<li><p>agent设置：</p>
<ul>
<li>meta-controller: 接收状态 <span class="math display">\[s_t\]</span>
然后选择子目标 <span class="math display">\[g_t\]</span>.</li>
<li>controller: 根据 <span class="math display">\[s_t\]</span> 和 <span
class="math display">\[g_t\]</span> 选择行为 <span
class="math display">\[a_t\]</span>.</li>
</ul></li>
<li><p>一个内部的critic用来为 controller 评估 <span
class="math display">\[r_t(g)\]</span>
（这篇文章使用的是一个二值的奖励）。controller
的目标就是最大化累积的本质奖励intrinsic reward <span
class="math display">\[R_t(g)=\sum_{t&#39;=t}^{\infty}
\gamma^{t&#39;-t}r_{t&#39;}(g)\]</span></p></li>
<li><p>meta-controller 的目标是最大化累积的外部奖励 extrinsic reward
<span class="math display">\[F_t=\sum_{t&#39;=t}^{\infty}
\gamma^{t&#39;-t}f_{t&#39;}\]</span>，其中<span
class="math display">\[f_{t&#39;}\]</span>是从环境中收到的奖励信号，并且<span
class="math display">\[F_{t}\]</span>和<span
class="math display">\[R_{t}\]</span>的时间尺度是不同的。</p></li>
<li><p>Controller: <span class="math display">\[
Q_1^*(s,a;g)=\max_{\pi_{a,g}} \mathbb{E} \Big[r_t+\gamma
\max_{a_{t+1}}Q_1^*(s_{t+1},a_{t+1};g)\Big]
\]</span></p></li>
<li><p>meta-controller: <span class="math display">\[
Q_2^*(s,g)=\max_{\pi_{g}} \mathbb{E} \Big[\sum_{t&#39;=t}^{t+N}
f_{t&#39;}+\gamma \max_{g&#39;}Q_2^*(s_{t+N},g&#39;)\Big]
\]</span> N 代表当控制器停止时所经过的步数。</p></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210714-2.png"
alt="算法步骤" />
<figcaption aria-hidden="true">算法步骤</figcaption>
</figure>
<h2 id="总结">总结</h2>
<p>很简单直接的思路，直接在DQN上做出了分层的改进，但是感觉和option的结合并不是很紧密，比如终止函数没有单独设计，时间缩放步数N可能需要预设等等。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2021/07/13/2021-07-13-Q130-%E8%A2%AB%E5%9B%B4%E7%BB%95%E7%9A%84%E5%8C%BA%E5%9F%9F-%E4%B8%AD%E7%AD%89-%E6%B7%B1%E5%BA%A6and%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/13/2021-07-13-Q130-%E8%A2%AB%E5%9B%B4%E7%BB%95%E7%9A%84%E5%8C%BA%E5%9F%9F-%E4%B8%AD%E7%AD%89-%E6%B7%B1%E5%BA%A6and%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" class="post-title-link" itemprop="url">Q130-被围绕的区域-中等-DFS/BFS</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-13 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-13T00:00:00+08:00">2021-07-13</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>3.4k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>3 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="被围绕的区域"><a
target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/surrounded-regions/">130.
被围绕的区域</a></h4>
<h2 id="question">Question</h2>
<blockquote>
<p>给你一个 <code>m x n</code> 的矩阵 <code>board</code> ，由若干字符
<code>'X'</code> 和 <code>'O'</code> ，找到所有被 <code>'X'</code>
围绕的区域，并将这些区域里所有的 <code>'O'</code> 用 <code>'X'</code>
填充。</p>
<p>x x x x <span class="math display">\[\rightarrow\]</span> x x x x</p>
<p>x 0 0 x <span class="math display">\[\rightarrow\]</span> x x x x</p>
<p>x x 0 x <span class="math display">\[\rightarrow\]</span> x x x x</p>
<p>x 0 x x <span class="math display">\[\rightarrow\]</span> x 0 x x</p>
</blockquote>
<blockquote>
<p><strong>Example 1:</strong></p>
<p>输入：board =
[["X","X","X","X"],["X","O","O","X"],["X","X","O","X"],["X","O","X","X"]]
输出：[["X","X","X","X"],["X","X","X","X"],["X","X","X","X"],["X","O","X","X"]]
解释：被围绕的区间不会存在于边界上，换句话说，任何边界上的 'O'
都不会被填充为 'X'。 任何不在边界上，或不与边界上的 'O' 相连的 'O'
最终都会被填充为
'X'。如果两个元素在水平或垂直方向相邻，则称它们是“相连”的。</p>
</blockquote>
<blockquote>
<p><strong>Example 2:</strong></p>
<p>输入：board = [["X"]] 输出：[["X"]]</p>
</blockquote>
<blockquote>
<p><strong>Note:</strong></p>
<ul>
<li><code>m == board.length</code></li>
<li><code>n == board[i].length</code></li>
<li><code>1 &lt;= m, n &lt;= 200</code></li>
<li><code>board[i][j]</code> 为 <code>'X'</code> 或
<code>'O'</code></li>
</ul>
</blockquote>
<h3 id="approach-1-深度优先搜索">Approach 1: 深度优先搜索</h3>
<p>写在前面 本题给定的矩阵中有三种元素：</p>
<ul>
<li><p>字母 X；</p></li>
<li><p>被字母 X 包围的字母 O；</p></li>
<li><p>没有被字母 X 包围的字母 O。</p></li>
</ul>
<p>本题要求将所有被字母 X 包围的字母 O都变为字母 X ，但很难判断哪些 O
是被包围的，哪些 O 不是被包围的。</p>
<p>注意到题目解释中提到：任何边界上的 O 都不会被填充为 X。
我们可以想到，所有的不被包围的 O 都直接或间接与边界上的 O
相连。我们可以利用这个性质判断 O 是否在边界上，具体地说：</p>
<ul>
<li>对于每一个边界上的
O，我们以它为起点，标记所有与它直接或间接相连的字母 O；</li>
<li>最后我们遍历这个矩阵，对于每一个字母：
<ul>
<li>如果该字母被标记过，则该字母为没有被字母 X 包围的字母
O，我们将其还原为字母 O；</li>
<li>如果该字母没有被标记过，则该字母为被字母 X 包围的字母
O，我们将其修改为字母 X。</li>
</ul></li>
</ul>
<p><strong>深度优先搜索：</strong></p>
<p>我们可以使用深度优先搜索实现标记操作。在下面的代码中，我们把标记过的字母
<code>O</code> 修改为字母 <code>A</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">self, board</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Do not return anything, modify board in-place instead.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> board:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        n, m = <span class="built_in">len</span>(board), <span class="built_in">len</span>(board[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">x, y</span>):</span><br><span class="line">            <span class="comment"># 如果board[x][y]不是位于矩阵边框，且字符为&#x27;x&#x27;，则返回为空</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0</span> &lt;= x &lt; n <span class="keyword">or</span> <span class="keyword">not</span> <span class="number">0</span> &lt;= y &lt; m <span class="keyword">or</span> board[x][y] != <span class="string">&#x27;O&#x27;</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="comment"># 对于位于矩阵边框，且字符为&#x27;0&#x27;的元素</span></span><br><span class="line">            board[x][y] = <span class="string">&quot;A&quot;</span></span><br><span class="line">            dfs(x + <span class="number">1</span>, y)</span><br><span class="line">            dfs(x - <span class="number">1</span>, y)</span><br><span class="line">            dfs(x, y + <span class="number">1</span>)</span><br><span class="line">            dfs(x, y - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 检测边框</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dfs(i, <span class="number">0</span>)</span><br><span class="line">            dfs(i, m - <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 检测边框</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m - <span class="number">1</span>):</span><br><span class="line">            dfs(<span class="number">0</span>, i)</span><br><span class="line">            dfs(n - <span class="number">1</span>, i)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">                <span class="keyword">if</span> board[i][j] == <span class="string">&quot;A&quot;</span>:</span><br><span class="line">                    board[i][j] = <span class="string">&quot;O&quot;</span></span><br><span class="line">                <span class="keyword">elif</span> board[i][j] == <span class="string">&quot;O&quot;</span>:</span><br><span class="line">                    board[i][j] = <span class="string">&quot;X&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># [[&quot;X&quot;]] </span></span><br><span class="line">    <span class="comment"># [[&quot;X&quot;,&quot;X&quot;,&quot;X&quot;,&quot;X&quot;],[&quot;X&quot;,&quot;O&quot;,&quot;O&quot;,&quot;X&quot;],[&quot;X&quot;,&quot;X&quot;,&quot;O&quot;,&quot;X&quot;],[&quot;X&quot;,&quot;O&quot;,&quot;X&quot;,&quot;X&quot;]]</span></span><br><span class="line">    board = [[<span class="string">&quot;X&quot;</span>,<span class="string">&quot;X&quot;</span>,<span class="string">&quot;X&quot;</span>,<span class="string">&quot;X&quot;</span>],[<span class="string">&quot;X&quot;</span>,<span class="string">&quot;O&quot;</span>,<span class="string">&quot;O&quot;</span>,<span class="string">&quot;X&quot;</span>],[<span class="string">&quot;X&quot;</span>,<span class="string">&quot;X&quot;</span>,<span class="string">&quot;O&quot;</span>,<span class="string">&quot;X&quot;</span>],[<span class="string">&quot;X&quot;</span>,<span class="string">&quot;O&quot;</span>,<span class="string">&quot;X&quot;</span>,<span class="string">&quot;X&quot;</span>]]</span><br><span class="line">    solution=Solution().solve(board)</span><br><span class="line">    <span class="built_in">print</span>(board)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p><strong>复杂度分析</strong></p>
<ul>
<li><strong>时间复杂度</strong>：<span class="math display">\[O(n \times
m)\]</span>，其中 n 和 m
分别为矩阵的行数和列数。深度优先搜索过程中，每一个点至多只会被标记一次。</li>
<li><strong>空间复杂度</strong>：<span class="math display">\[O(n \times
m)\]</span>，其中 n 和 m
分别为矩阵的行数和列数。主要为深度优先搜索的栈的开销。</li>
</ul>
<h3 id="approach-2-广度优先搜索">Approach 2: 广度优先搜索</h3>
<p>我们可以使用广度优先搜索实现标记操作。在下面的代码中，我们把标记过的字母
<code>O</code> 修改为字母 <code>A</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solve_BFS</span>(<span class="params">self, board </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Do not return anything, modify board in-place instead.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> board:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        n, m = <span class="built_in">len</span>(board), <span class="built_in">len</span>(board[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 边框有&#x27;0&#x27;就放入队列</span></span><br><span class="line">        que = collections.deque()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> board[i][<span class="number">0</span>] == <span class="string">&quot;O&quot;</span>:</span><br><span class="line">                que.append((i, <span class="number">0</span>))</span><br><span class="line">            <span class="keyword">if</span> board[i][m - <span class="number">1</span>] == <span class="string">&quot;O&quot;</span>:</span><br><span class="line">                que.append((i, m - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m - <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> board[<span class="number">0</span>][i] == <span class="string">&quot;O&quot;</span>:</span><br><span class="line">                que.append((<span class="number">0</span>, i))</span><br><span class="line">            <span class="keyword">if</span> board[n - <span class="number">1</span>][i] == <span class="string">&quot;O&quot;</span>:</span><br><span class="line">                que.append((n - <span class="number">1</span>, i))</span><br><span class="line">        <span class="comment"># 队列中的每个元素标记为&#x27;A&#x27;</span></span><br><span class="line">        <span class="keyword">while</span> que:</span><br><span class="line">            x, y = que.popleft()</span><br><span class="line">            board[x][y] = <span class="string">&quot;A&quot;</span></span><br><span class="line">        <span class="comment"># 检测周围的4个点是否为非边框，且值为&#x27;0&#x27;，如果是，则加入队列</span></span><br><span class="line">            <span class="keyword">for</span> mx, my <span class="keyword">in</span> [(x - <span class="number">1</span>, y), (x + <span class="number">1</span>, y), (x, y - <span class="number">1</span>), (x, y + <span class="number">1</span>)]:</span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt;= mx &lt; n <span class="keyword">and</span> <span class="number">0</span> &lt;= my &lt; m <span class="keyword">and</span> board[mx][my] == <span class="string">&quot;O&quot;</span>:</span><br><span class="line">                    que.append((mx, my))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">                <span class="keyword">if</span> board[i][j] == <span class="string">&quot;A&quot;</span>:</span><br><span class="line">                    board[i][j] = <span class="string">&quot;O&quot;</span></span><br><span class="line">                <span class="keyword">elif</span> board[i][j] == <span class="string">&quot;O&quot;</span>:</span><br><span class="line">                    board[i][j] = <span class="string">&quot;X&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>复杂度分析</strong></p>
<ul>
<li><strong>时间复杂度</strong>：<span class="math display">\[O(n \times
m)\]</span>，其中 n 和
分别为矩阵的行数和列数。广度优先搜索过程中，每一个点至多只会被标记一次。</li>
<li><strong>空间复杂度</strong>：<span class="math display">\[O(n \times
m)\]</span>，其中 n 和 m
分别为矩阵的行数和列数。主要为广度优先搜索的队列的开销。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2021/07/12/2021-07-12-Reinforcement%20Learning%20-%20DRN-A%20Deep%20Reinforcement%20Learning%20Framework%20for%20News%20Recommendation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/12/2021-07-12-Reinforcement%20Learning%20-%20DRN-A%20Deep%20Reinforcement%20Learning%20Framework%20for%20News%20Recommendation/" class="post-title-link" itemprop="url">Reinforcement Learning | DRN: A Deep Reinforcement Learning Framework for News Recommendation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-12 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-12T00:00:00+08:00">2021-07-12</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.6k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1
id="drn-a-deep-reinforcement-learning-framework-for-news-recommendation">DRN:
A Deep Reinforcement Learning Framework for News Recommendation</h1>
<p>论文链接：http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf</p>
<h2 id="主要内容">主要内容</h2>
<ul>
<li><p>针对新闻和用户动态特征的问题，目前的方法主要存在三个问题：</p>
<ul>
<li>模型<strong>只考虑当前奖励</strong>（例如CTR，点击转化率）</li>
<li>很少有模型考虑<strong>使用用户的反馈</strong>而不是点击/不点击的标签（例如用户反馈频率）</li>
<li>模型总是<strong>推荐相似的东西</strong>，会使得用户感到无聊</li>
</ul></li>
<li><p>一些典型方法：</p>
<ul>
<li>content based methods [19, 22, 33],</li>
<li>collaborative filtering based methods [11, 28, 34]</li>
<li>hybrid methods [12, 24, 25].</li>
<li>deep learning models [8, 45, 52]</li>
</ul></li>
<li><p>三个挑战：</p>
<ul>
<li>First, the dynamic changes in news recommendations are difficult to
handle.
<ul>
<li>First, news become outdated very fast.</li>
<li>Second, users’ interest on different news might evolve during
time.</li>
</ul></li>
<li>Second, current recommendation methods [23, 35, 36, 43] usually only
consider the click / no click labels or ratings as users’ feedback.</li>
<li>The third major issue of current recommendation methods is its
tendency to keep recommending similar items to users, which might
decrease users’ interest in similar topics.</li>
</ul></li>
<li><p>一般的强化学习推荐方法使用 <span
class="math display">\[\epsilon-greedy\]</span> 或者 Upper Confidence
Bound (UCB) 方法增加推荐的探索能力。</p>
<p>但是 <span class="math display">\[\epsilon-greedy\]</span>
会损害当前推荐的性能，而UCB在一件物品尝试了几次之后才能得到相对准确的奖励评估。因此需要更高效的探索方式。</p></li>
<li><p>通过使用 Dueling Bandit Gradient Descent (DBGD)
算法作为探索策略，它会随机从候选项的邻域或者候选项中选择推荐对象。这个探索策略可以避免推荐完全无关的信息，因此可以保持当前策略的精度。</p></li>
<li><p>模型框图：</p>
<p><img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-1.png" /></p></li>
<li><p>算法每1小时更新一次。Every one hour, the agent will use the log
in the memory to update its recommendation algorithm.</p></li>
<li><p>主要贡献：</p>
<ul>
<li>我们认为用户的活跃性有助于提高推荐的准确性，这可以提供额外的信息，而不是简单地使用用户点击标签。</li>
<li>应用了更有效的探索方法，避免了由经典探索方法(例如，<span
class="math display">\[\epsilon\]</span>-greedy和置信上限)引起的推荐准确度下降。</li>
<li>我们的系统已经在线部署在商业新闻推荐应用程序中。大量的离线和在线实验显示了我们方法的优越性能。</li>
</ul></li>
<li><p>一些推荐方法的特点：</p>
<ul>
<li>Content-based methods：统计内容的频率，推荐相似的内容；</li>
<li>Collaborative filtering
methods：使用过去的转化率或者其他类似用户的转化率，或者结合两者来预测；</li>
<li>Hybrid methods：提升用户画像模型；</li>
<li>deep learning models：建模复杂的用户-物品关系</li>
</ul></li>
</ul>
<h3 id="算法模型">算法模型：</h3>
<ul>
<li><p>算法步骤：</p>
<ol type="1">
<li><strong>PUSH</strong>: In each timestamp (t1, t2, t3, t4, t5, ...),
when a user sends a news request to the system, the recommendation agent
G will take the feature representation of the current user and news
candidates as input, and generate a top-k list of news to recommend L. L
is generated by combining the exploitation of current model (will be
discussed in Section 4.3) and exploration of novel items (will be
discussed in Section 4.5).</li>
<li><strong>FEEDBACK</strong>: User u who has received recommended news
L will give their feedback B by his clicks on this set of news.</li>
<li><strong>MINOR UPDATE</strong>: After each timestamp (e.g., after
timestamp t1), with the feature representation of the previous user u
and news list L, and the feedback B, agent G will update the model by
comparing the recommendation performance of exploitation network Q and
exploration network Q˜ (will be discussed in Section 4.5). If Q˜ gives
better recommendation result, the current network will be updated
towards Q˜ . Otherwise, Q will be kept unchanged. Minor update can
happen after every recommendation impression happens.</li>
<li><strong>MAJOR UPDATE</strong>: After certain period of timeTR(e.g.,
after timestamp t3), agent G will use the user feedback B and user
activeness stored in the memory to update the network Q. Here, we use
the experience replay technique [31] to update the network.
Specifically, agent G maintains a memory with recent historical click
and user activeness records. When each update happens, agent G will
sample a batch of records to update the model. Major update usually
happens after a certain time interval, like one hour, during which
thousands of recommendation impressions are conducted and their
feedbacks are collected.</li>
<li>Repeat step (1)-(4).</li>
</ol></li>
<li><p>特征设计：</p>
<ul>
<li><p><strong>News features</strong> includes 417 dimension one hot
features that describe whether certain property appears in this piece of
news, including headline, provider, ranking, entity name, category,
topic category, and click counts in last 1 hour, 6 hours, 24 hours, 1
week, and 1 year respectively.</p></li>
<li><p><strong>User features</strong> mainly describes the features
(i.e., headline, provider, ranking, entity name, category, and topic
category) of the news that the user clicked in 1 hour, 6 hours, 24
hours, 1 week, and 1 year respectively. There is also a total click
count for each time granularity. Therefore, there will be totally 413 ×
5 = 2065 dimensions.</p></li>
<li><p><strong>User news features</strong>. These 25-dimensional
features describe the interaction between user and one certain piece of
news, i.e., the frequency for the entity (also category, topic category
and provider) to appear in the history of the user’s readings.</p></li>
<li><p><strong>Context features</strong>. These 32-dimensional features
describe the context when a news request happens, including time,
weekday, and the freshness of the news (the gap between request time and
news publish time).</p></li>
</ul></li>
<li><p>模型框架，包含离线和在线部分</p></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-3.png"
alt="Model framework" />
<figcaption aria-hidden="true">Model framework</figcaption>
</figure>
<p><img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-4.png"
alt="Q network" /> <span class="math display">\[
y_{s,a}=Q(s,a)=r_{immediate}+\gamma r_{future}\\
y_{s,a,t}=r_{a,t+1}+\gamma Q\big(s_{a,t+1},\arg \max_{a&#39;}
Q(s_{a,t+1},a&#39;;W_t);W_t&#39;\big)
\]</span></p>
<h3 id="user-activeness">User Activeness</h3>
<p><img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-3.png"
alt="User activeness estimation" /> <span class="math display">\[
\lambda(t)=\lim_{dt\rightarrow 0} \frac{\Pr\{t \leq T&lt;t+dt\mid T\geq
t \}}{d t}\\
S(t)=e^{-\int_0^\infty \lambda(x)dx}\\
T_0=\int_0^{\infty}S(t)dt
\]</span> the maximum user activeness is truncated to 1.</p>
<p>The click / no click label <span
class="math display">\[r_{click}\]</span> and the user activeness <span
class="math display">\[r_{active}\]</span> are combined as: <span
class="math display">\[
r_{total}=r_{click}+\beta r_{active}
\]</span></p>
<ul>
<li>网络框架</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-2.png"
alt="Exploration by Dueling Bandit Gradient Descent" />
<figcaption aria-hidden="true">Exploration by Dueling Bandit Gradient
Descent</figcaption>
</figure>
<p><span class="math display">\[\hat{L}\]</span> 首先使用概率交错方法从
<span class="math display">\[L\]</span> 和 <span
class="math display">\[\tilde{L}\]</span> 。大致就是首先随机从 <span
class="math display">\[L\]</span> 和 <span
class="math display">\[\tilde{L}\]</span>
选择推荐项，之后按照概率值更新网络 <span
class="math display">\[\tilde{Q}\]</span></p>
<p><span class="math display">\[
\Delta W=\alpha \cdot rand(-1,1)\cdot W\\
W&#39;=W+\eta \tilde{W}
\]</span></p>
<h2 id="总结">总结</h2>
<p>感觉整体框架还是很好理解的，针对任务的改进比较多，对强化学习框架的改动比较小。</p>
<p>两个网络的构造也类似于DQN中的target
network，输入的高维特征类比于输入的图像信息。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2021/07/08/2021-07-08-Reinforcement%20Learning%20-%20Option-critic%20architecture/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/08/2021-07-08-Reinforcement%20Learning%20-%20Option-critic%20architecture/" class="post-title-link" itemprop="url">Reinforcement Learning | Option-critic Architecture</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-08 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-08T00:00:00+08:00">2021-07-08</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>5.5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="option-critic-architecture">Option-critic Architecture</h1>
<p>论文链接：www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14858/14328</p>
<h2 id="主要内容">主要内容</h2>
<ul>
<li><p>分层强化学习自动学习时间抽象是很重要的</p></li>
<li><p>设计了基于策略梯度理论的option-critic方法来学习内部策略和option的终点约束。We
derive policy gradient theorems for options and propose a new
option-critic architecture capable of learning both the internal
policies and the termination conditions of options</p></li>
<li><p>在离散和连续的任务中都取得了好的表现。 discrete and continuous
environments</p></li>
<li><p>时间抽象允许表示关于发生在不同时间尺度上的行动过程的知识。Temporal
abstraction allows representing knowledge about courses of action that
take place at different time scales.</p></li>
<li><p>Discovering temporal abstractions autonomously
成为了一个持续了15年的研究主题</p></li>
<li><p>大部分的工作致力于发现子目标，并在后续的策略学习中实现它们。这个其实就是分层结构，并且和肌肉骨骼的运动控制很像。The
majority of the existing work has focused on finding subgoals (useful
states that an agent should reach) and subsequently learning policies to
achieve them</p></li>
<li><p>问题在于求解子目标时的探索时间和计算开销非常大。Additionally,
learning policies associated with subgoals can be expensive in terms of
data and computation time; in the worst case, it can be as expensive as
solving the entire task.</p></li>
<li><p>本文的方法模糊了发现option和学习option的界限，</p></li>
<li><p>In contrast, we show that our approach is capable of successfully
learning options within a single task without incurring any slowdown and
while still providing benefits for <strong>transfer
learning</strong>.（看着有点像我的Speed-accuracy
Trade-off那篇文章，但它的是直接说在抢一学习上有好处的）</p></li>
<li><p>core ideas：the <strong>intra-option policy</strong> and
<strong>termination gradient theorems</strong></p></li>
<li><p>方法需要的特殊条件少。As opposed to other methods, we only need
to specify <strong>the number of desired options</strong>; it is
<strong>not</strong> necessary to have <strong>subgoals</strong>,
<strong>extra rewards</strong>, <strong>demonstrations</strong>,
<strong>multiple problems</strong> or any other <strong>special
accommodations</strong> (however, the approach can take advantage of
pseudo-reward functions if desired)</p></li>
</ul>
<h3 id="option-framework">Option Framework</h3>
<ul>
<li>option框架假设对于option <span class="math display">\[\omega\in
\Omega\]</span> 由三元组 <span
class="math display">\[(I_{\omega},\pi_{\omega},\beta_{\omega})\]</span>
组成。
<ul>
<li><span class="math display">\[I_{\omega} \subseteq S\]</span>
是初始的状态集合</li>
<li><span class="math display">\[\pi_{\omega}\]</span>
是intra-option策略</li>
<li><span class="math display">\[\beta_{\omega}: S \rightarrow
[0,1]\]</span> 是终止函数</li>
</ul></li>
<li>当使用option框架时， MDP就变成了Semi-MDP，它有相应的最优值函数<span
class="math display">\[V_{\Omega}(s)\]</span> 和option-value函数 <span
class="math display">\[Q_{\Omega}(s,\omega)\]</span>
。所谓的Semi-MDP就是指从状态s到下一个状态s‘要经过 <span
class="math display">\[\tau\]</span>
步的MDP，大致就是状态之间存在时间上的不连续。</li>
<li>通过一些针对MDP的算法实现多个option并行地学习，这就是
<strong>intra-option learning</strong> 的主要思路</li>
<li>接下来就是要实现两个关键任务：learning <strong>option
policies</strong> and <strong>termination functions</strong></li>
<li>算法流程：一个外部的策略 <span
class="math display">\[\pi_{\Omega}\]</span> 选择option来执行控制，
intra-option policy <span class="math display">\[\pi_{\omega}\]</span>
开始执行，直到终点（用终点函数来判断停止执行）</li>
<li>定义option-value function can be written as:</li>
</ul>
<p><span class="math display">\[
Q_{\Omega}(s,\omega)=\sum_a \pi_{\omega,\theta}(a \mid
s)Q_{U}(s,\omega,a)
\]</span></p>
<p>其中 <span class="math display">\[Q_{U}(s,\omega,a)\]</span>
是在state-option对下执行行为的评估值。</p>
<p><span class="math display">\[
Q_{U}(s,\omega,a)=r(s,a)+\gamma\sum_{s&#39;}P(s&#39; \mid
s,a)U(\omega,s&#39;)
\]</span></p>
<p>注意 <span class="math display">\[(s,\omega)\]</span>
组导致了一个扩张的状态空间 an augmented state space, cf. (Levy and
Shimkin 2011)</p>
<p><span class="math display">\[U: \Omega \times S\rightarrow
\mathbb{R}\]</span> 是 the option-value function upon arrival，The value
of executing <span class="math display">\[\omega\]</span> upon entering
a state <span class="math display">\[s&#39;\]</span> is given by:</p>
<p><span class="math display">\[
U(\omega,s&#39;)=(1-\beta_{\omega,\vartheta}(s&#39;))Q_{\Omega}(s&#39;,\omega)+\beta_{\omega,\vartheta}(s&#39;)V_{\Omega}(s&#39;)
\]</span></p>
<p>如果option <span class="math display">\[\omega_t\]</span> 在时刻 t
被初始化并被执行，此时状态为 <span class="math display">\[s_t\]</span>
，然后在1步之后，状态转移到 <span
class="math display">\[(s_{t+1},\omega_{t+1})\]</span> 的概率是：</p>
<p><span class="math display">\[
P(s_{t+1},\omega_{t+1} \mid s_t,\omega_t)=\sum_a
\pi_{\omega_t,\theta}(a|s_t)P(s_{t+1} \mid s_t,a)\\
\big((1-\beta_{\omega_t,\vartheta}(s_{t+1}))1_{\omega_t=\omega_{t+1}}+\beta_{\omega_t,\vartheta}(s_{t+1})\pi_{\Omega}(\omega_{t+1}
\mid s_{t+1})\big)
\]</span></p>
<p>显然，给出计算过程是均匀的。在温和的条件下，由于选项到处可用，它实际上是遍历的，并且在
state-option 对上存在唯一的平稳分布。</p>
<p>接下来计算期望累积回报关于intra-option策略的参数 <span
class="math display">\[\theta\]</span>
的梯度，假设它是随机的可微分的，我们得到</p>
<p><span class="math display">\[
\frac{\partial Q_{\Omega}(s,\omega)}{\partial \theta}=\Bigg(\sum_a
\frac{\partial \pi_{\omega,\theta}(a \mid s)}{\partial
\theta}Q_{U}(s,\omega,a)\Bigg)\\
+\sum_a\pi_{\omega,\theta}(a \mid s)\sum_{s&#39;} \gamma
P(s&#39;|s,a)\frac{\partial U(\omega,s&#39;)}{\partial \theta}
\]</span></p>
<p><strong>Theorem 1</strong> (<strong>Intra-Option Policy Gradient
Theorem</strong>). Given a set of Markov options with stochastic
intra-option policies differentiable in their parameters <span
class="math display">\[\theta\]</span>, the gradient of the expected
discounted return with respect to <span
class="math display">\[\theta\]</span> and initial condition <span
class="math display">\[(s_0,\omega_0)\]</span> is:</p>
<p><span class="math display">\[
\sum_{s,\omega} \mu_{\Omega}(s,\omega \mid s_0,\omega_0)\sum_a
\frac{\partial \pi_{\omega,\theta}(a \mid s)}{\partial \theta}
Q_U(s,\omega,a)
\]</span></p>
<p>其中 <span class="math display">\[\mu_{\Omega}\]</span>
是状态-选项对的折扣权重</p>
<p><span class="math display">\[
\mu_{\Omega}(s,\omega \mid s_0,\omega_0)=\sum_{t=0}^{\infty}\gamma^t
P(s_t=s,\omega_t=\omega \mid s_0,\omega_0)
\]</span></p>
<p>下面计算终点函数的梯度，假设时间是随机化的并且可微的关于参数 <span
class="math display">\[\vartheta\]</span></p>
<p><span class="math display">\[
\frac{\partial Q_{\Omega}(s,\omega)}{\partial \vartheta} = \sum_a
\pi_{\omega,\theta}(a \mid s)\sum_{s&#39;}\gamma P(s&#39; \mid s,a)
\frac{\partial U(\omega,s&#39;)}{\partial \vartheta}
\]</span></p>
<p>关于 <span class="math display">\[U\]</span> 的梯度，可以结合又是函数
<span class="math display">\[A_{\Omega}\]</span> 来计算：</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial U(\omega,s&#39;)}{\partial \vartheta} =&amp;
-\frac{\partial \beta_{\omega,\vartheta}(s&#39;)}{\partial \vartheta}
A_{\Omega}(s&#39;,\omega) \\
&amp;+ \gamma\sum_{\omega&#39;} \sum_{s&#39;&#39;}
P(s&#39;&#39;,\omega&#39; \mid s&#39;,\omega)\frac{\partial
U(\omega&#39;,s&#39;&#39;)}{\partial \vartheta}
\end{align}
\]</span></p>
<p>其中，优势函数 <span class="math display">\[A_{\Omega}(s&#39;,\omega)
= Q_{\Omega}(s&#39;,\omega)-V_{\Omega}(s&#39;)\]</span> .</p>
<p><strong>Theorem 2</strong> (<strong>Termination Gradient
Theorem</strong>). Given a set of Markov options with stochastic
termination functions differentiable in their parameters <span
class="math display">\[\vartheta\]</span>, the gradient of the expected
discounted return objective with respect to <span
class="math display">\[\vartheta\]</span> and the initial condition
<span class="math display">\[(s_1, \omega_0)\]</span> is:</p>
<p><span class="math display">\[
-\sum_{s&#39;,\omega}\mu_{\Omega}(s&#39;,\omega \mid
s_1,\omega_0)\frac{\partial \beta_{\omega,\vartheta}(s&#39;)}{\partial
\vartheta}A_{\Omega}(s&#39;,\omega)
\]</span></p>
<p>其中 <span class="math display">\[\mu_{\Omega}(s&#39;,\omega \mid
s_1,\omega_0)\]</span> 是状态-选项对 <span
class="math display">\[(s_1,\omega_0)\]</span> 的折扣权重</p>
<p><span class="math display">\[
\mu_{\Omega}(s,\omega \mid s_1,\omega_0)=\sum_{t=0}^{\infty} \gamma^t
P(s_{t+1}=s,\omega_t=\omega|s_1,\omega_0)
\]</span></p>
<ul>
<li>算法结构：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210709-1.png"
alt="算法结构" />
<figcaption aria-hidden="true">算法结构</figcaption>
</figure>
<ul>
<li>算法伪代码：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210709-2.png"
alt="算法伪代码" />
<figcaption aria-hidden="true">算法伪代码</figcaption>
</figure>
<ul>
<li>算法的问题：假设了所有的options适用于所有的地方。Perhaps the biggest
remaining limitation of our work is the assumption that all options
apply everywhere.</li>
</ul>
<h2 id="总结">总结</h2>
<p>option-critic算法框架是典型的HRL实现方式之一，上下两层都使用策略梯度优化的证明过程看着还是很不错的。具体公式和推导可以参考原文以及附件。相关代码可以在paper
with code找出来看看，估计会比FuN的结构的HRL更容易复现吧。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2021/07/06/2021-07-06-Reinforcement%20Learning%20-%20Option%20Discovery%20in%20Hierarchical%20Reinforcement%20Learning%20using%20Spatio-Temporal%20Clustering%20-%20%E5%89%AF%E6%9C%AC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/06/2021-07-06-Reinforcement%20Learning%20-%20Option%20Discovery%20in%20Hierarchical%20Reinforcement%20Learning%20using%20Spatio-Temporal%20Clustering%20-%20%E5%89%AF%E6%9C%AC/" class="post-title-link" itemprop="url">Reinforcement Learning | Option Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal Clustering</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-06 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-06T00:00:00+08:00">2021-07-06</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1
id="option-discovery-in-hierarchical-reinforcement-learning-using-spatio-temporal-clustering">Option
Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal
Clustering</h1>
<p>论文链接：https://arxiv.org/pdf/1605.05359v3.pdf</p>
<h2 id="主要内容">主要内容</h2>
<ul>
<li><p>本文旨在提出一个自动的技能获取框架，可以分层描述任务，对状态抽象以及在抽象状态之间拓展行为。这样的分层结构可以加速学习过程</p></li>
<li><p>使用动态系统中寻找状态空间中亚稳态区域和抽象状态的思想。We use
ideas from dynamical systems to find metastable regions in the state
space and associate them with abstract states</p></li>
<li><p>使用空间聚类算法PCCA+，做状态抽象，形成抽象的状态量</p></li>
<li><p>These skills are independent of the learning task and can be
efficiently reused across a variety of tasks defined over the same
model</p></li>
<li><p>The core idea of hierarchical reinforcement learning is to break
down the reinforcement learning problem into subtasks through a
hierarchy of abstractions.</p></li>
<li><p>自动发现不同的技能，这是一个小领域。Our focus in this paper is to
present our framework on automated discovery of skills</p>
<ul>
<li>定义状态空间中的瓶颈状态，这些状态被划分为集合，两组罕见状态之间的转换会在这种转换的相应点引入瓶颈状态。达到这种瓶颈状态的策略被缓存为选项
(McGovern and Barto 2001).</li>
<li>使用因式分解来表示状态空间中存在的结构，识别很少变化的动作序列，并把这些序列作为选项被缓存起来
(Hengst 2004)</li>
<li>使用智能体与环境交互的图表征，并使用中间中心性度量来识别子任务</li>
<li>使用聚类方法(光谱或其他方法)分离出MDP的不同强关联成分，并识别连接不同聚类的瓶颈
(Menache, Mannor, and Shimkin 2002).</li>
</ul></li>
<li><p>文章的目标是在没有任何先验知识的情况下获得技能和发现抽象</p></li>
<li><p>使用SMDP Q-Learning，Intra Option Q-Learning</p></li>
<li><p>主要贡献：</p>
<ul>
<li>使用PCCA+抽象MDP的state</li>
<li>设计一种组合option的方式</li>
<li>大状态空间下，不好使用，设计一种聚合的状态空间</li>
</ul></li>
<li><p>Option框架：The option framework is one of the formalisations
used to represent hierarchies in RL (Sutton, Precup, and Singh 1999).
Formally, an option is a tuple <span class="math inline">\(O = (I, \mu,
\beta)\)</span> here:</p>
<ul>
<li><span class="math inline">\(I\)</span> is the initiation set: a set
of states from which the action can be activated</li>
<li><span class="math inline">\(\mu\)</span> is a policy function where
µ(s, a) represents the preference value given to action a in state s
following option O.</li>
<li><span class="math inline">\(\beta\)</span> is the termination
function: When an agent enters a state s while following option O, the
option could be terminated with a probability <span
class="math inline">\(\beta(s)\)</span>.</li>
</ul></li>
<li><p>Semi-Markov Decision Process (SMDP)</p></li>
</ul>
<p><span class="math display">\[
Q(s,o)\leftarrow Q(s,o)+\alpha[r+\gamma^k \max_{a\in O} Q(s,a)-Q(s,o)]
\]</span></p>
<ul>
<li><p>SMDP学习方法的一个主要缺点是，在了解其结果之前，选项需要完全执行到终止。</p></li>
<li><p>Perron Cluster Analysis (PCCA+)</p></li>
</ul>
<h2 id="总结">总结</h2>
<p>主要工作是利用一个谱聚类算法自动地寻找option，算是options类算法的变体。option可以赋予策略语义特征，可以看看原始文章。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2021/07/05/2021-07-05-HJ26%20%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8E%92%E5%BA%8F-%E4%B8%AD%E7%AD%89-%E5%AD%97%E7%AC%A6%E4%B8%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/05/2021-07-05-HJ26%20%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8E%92%E5%BA%8F-%E4%B8%AD%E7%AD%89-%E5%AD%97%E7%AC%A6%E4%B8%B2/" class="post-title-link" itemprop="url">HJ26 字符串排序-中等-字符串</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-05 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-05T00:00:00+08:00">2021-07-05</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>700</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="hj26-字符串排序"><a
target="_blank" rel="noopener" href="https://www.nowcoder.com/practice/5190a1db6f4f4ddb92fd9c365c944584?tpId=37&amp;&amp;tqId=21249&amp;rp=1&amp;ru=/ta/huawei&amp;qru=/ta/huawei/question-ranking">HJ26
字符串排序</a></h4>
<h2 id="question">Question</h2>
<blockquote>
<p><strong>规则 1</strong> ：英文字母从 A 到 Z 排列，不区分大小写。</p>
<p>如，输入： Type 输出： epTy</p>
<p><strong>规则 2</strong>
：同一个英文字母的大小写同时存在时，按照输入顺序排列。</p>
<p>如，输入： BabA 输出： aABb</p>
<p><strong>规则 3</strong> ：非英文字母的其它字符保持原来的位置。</p>
<p>如，输入： By?e 输出： Be?y</p>
<p>注意有多组测试数据，即输入有多行，每一行单独处理（换行符隔开的表示不同行）</p>
<p>输入描述： - 输入字符串</p>
<p>输出描述： - 输出字符串</p>
<p>示例1 输入：A Famous Saying: Much Ado About Nothing (2012/8). 输出：A
aaAAbc dFgghh: iimM nNn oooos Sttuuuy (2012/8).</p>
</blockquote>
<h3 id="approach">Approach</h3>
<ul>
<li>sorted(iterable,str.upper)就可以实现：
<ul>
<li>1）字符由A到Z的排序</li>
<li>2）能够实现同字母（A与a算同字母），由输入先后书序排列。</li>
</ul></li>
<li>str.isalpha实现的是如果字符串至少有一个字符并且所有字符都是字母则返回
True，否则返回 False。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        s = <span class="built_in">input</span>()</span><br><span class="line">        a = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> i.isalpha():</span><br><span class="line">                a += i</span><br><span class="line">        b = <span class="built_in">sorted</span>(a, key=<span class="built_in">str</span>.upper)</span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        d = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            <span class="keyword">if</span> s[i].isalpha():</span><br><span class="line">                d += b[index]</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                d += s[i]</span><br><span class="line">        <span class="built_in">print</span>(d)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p><strong>复杂度分析</strong></p>
<ul>
<li>时间复杂度：<span
class="math display">\[O(N)\]</span>，N是序列长度。</li>
<li>空间复杂度：<span
class="math display">\[O(1)\]</span>。常数个变量。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Txing"
      src="/images/my_photo.jpg">
  <p class="site-author-name" itemprop="name">Txing</p>
  <div class="site-description" itemprop="description">泛用类人决战型机器人博士</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">229</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/txing-casia" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;txing-casia" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.uomi.moe/" title="https:&#x2F;&#x2F;blog.uomi.moe" rel="noopener" target="_blank">驱逐舰患者</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://m.mepai.me/photographyer/u_5a68085ba15aa.html?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;m.mepai.me&#x2F;photographyer&#x2F;u_5a68085ba15aa.html?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">隐之-INF</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Txing</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">538k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">8:09</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
