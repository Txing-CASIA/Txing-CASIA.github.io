<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"txing-casia.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="泛用类人决战型机器人博士">
<meta property="og:type" content="website">
<meta property="og:title" content="Txing">
<meta property="og:url" content="https://txing-casia.github.io/page/2/index.html">
<meta property="og:site_name" content="Txing">
<meta property="og:description" content="泛用类人决战型机器人博士">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Txing">
<meta property="article:tag" content="Txing">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://txing-casia.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Txing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Txing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到 | 伽蓝之堂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/30/2022-09-30-Autonomous%20Driving%20-%20From%20smart%20parking%20towards%20autonomous%20valet%20parking%20A%20survey%20challenges%20and%20future%20Works/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/30/2022-09-30-Autonomous%20Driving%20-%20From%20smart%20parking%20towards%20autonomous%20valet%20parking%20A%20survey%20challenges%20and%20future%20Works/" class="post-title-link" itemprop="url">Autonomous Driving | From smart parking towards autonomous valet parking: A survey, challenges and future Works</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-30 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-30T00:00:00+08:00">2022-09-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-05 19:15:44" itemprop="dateModified" datetime="2023-04-05T19:15:44+08:00">2023-04-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这是一篇停车场管理的综述，不是我想找的Autonomous parking算法的。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/09/30/2022-09-30-Autonomous%20Driving%20-%20From%20smart%20parking%20towards%20autonomous%20valet%20parking%20A%20survey%20challenges%20and%20future%20Works/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/30/2022-09-30-Autonomous%20Driving%20-%20Parallel%20Angular%20and%20Perpendicular%20Parking%20for%20Self-Driving%20Cars%20using%20Deep%20Reinforcement%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/30/2022-09-30-Autonomous%20Driving%20-%20Parallel%20Angular%20and%20Perpendicular%20Parking%20for%20Self-Driving%20Cars%20using%20Deep%20Reinforcement%20Learning/" class="post-title-link" itemprop="url">Autonomous Driving | Parallel, Angular and Perpendicular Parking for Self-Driving Cars using Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-30 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-30T00:00:00+08:00">2022-09-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-05 19:15:40" itemprop="dateModified" datetime="2023-04-05T19:15:40+08:00">2023-04-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>parking是所有驾驶行为的最终步骤，主要分为三类：垂直、平行、斜向泊车。本文使用DDPG控制汽车完成三种场景的泊车。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/09/30/2022-09-30-Autonomous%20Driving%20-%20Parallel%20Angular%20and%20Perpendicular%20Parking%20for%20Self-Driving%20Cars%20using%20Deep%20Reinforcement%20Learning/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/29/2022-09-29-Autonomous%20Driving%20-%20Reinforcement%20Learning-Based%20Motion%20Planning%20for%20Automatic%20Parking%20System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/29/2022-09-29-Autonomous%20Driving%20-%20Reinforcement%20Learning-Based%20Motion%20Planning%20for%20Automatic%20Parking%20System/" class="post-title-link" itemprop="url">Autonomous Driving | Reinforcement Learning-Based Motion Planning for Automatic Parking System</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-29 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-29T00:00:00+08:00">2022-09-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-05 19:15:47" itemprop="dateModified" datetime="2023-04-05T19:15:47+08:00">2023-04-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本文质量一般，只看了intro部分，文献较老，没继续看了。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/09/29/2022-09-29-Autonomous%20Driving%20-%20Reinforcement%20Learning-Based%20Motion%20Planning%20for%20Automatic%20Parking%20System/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/28/2022-09-28-Autonomous%20Driving%20-%20DL-IAPS%20and%20PJSO%20A%20Path%20Speed%20Decoupled%20Trajectory%20Optimization%20and%20its%20Application%20in%20Autonomous%20Driving/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/28/2022-09-28-Autonomous%20Driving%20-%20DL-IAPS%20and%20PJSO%20A%20Path%20Speed%20Decoupled%20Trajectory%20Optimization%20and%20its%20Application%20in%20Autonomous%20Driving/" class="post-title-link" itemprop="url">Autonomous Driving | DL-IAPS and PJSO: A Path/Speed Decoupled Trajectory Optimization and its Application in Autonomous Driving (Baidu, 2020)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-28T00:00:00+08:00">2022-09-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-05 19:17:19" itemprop="dateModified" datetime="2023-04-05T19:17:19+08:00">2023-04-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>整体而言就是在混合A*的基础上，加上了轨迹平滑、碰撞检测、速度规划三个后续模块，在parking任务上取得了一定效果，并集成与Apollo平台中</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/09/28/2022-09-28-Autonomous%20Driving%20-%20DL-IAPS%20and%20PJSO%20A%20Path%20Speed%20Decoupled%20Trajectory%20Optimization%20and%20its%20Application%20in%20Autonomous%20Driving/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/23/2022-09-23-Reinforcement%20Learning%20-%20[u]%20Human-level%20Atari%20200x%20faster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/23/2022-09-23-Reinforcement%20Learning%20-%20%5Bu%5D%20Human-level%20Atari%20200x%20faster/" class="post-title-link" itemprop="url">Reinforcement Learning | Human-level Atari 200x faster</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-23 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-23T00:00:00+08:00">2022-09-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-05 19:20:20" itemprop="dateModified" datetime="2023-04-05T19:20:20+08:00">2023-04-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Deepmind的Agent57是第一个在57款Atari游戏上全面超过人类水平的强化学习智能体。但是Agent57的数据利用效率很低，要求80亿帧数据。本文通过设置不同的策略集合实现了200倍的训练效率提高。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/09/23/2022-09-23-Reinforcement%20Learning%20-%20%5Bu%5D%20Human-level%20Atari%20200x%20faster/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/20/2022-09-20-Autonomous%20Driving%20-%20BEVFormer%20Learning%20Bird%E2%80%99s-Eye-View%20Representation%20from%20Multi-Camera%20Images%20via%20Spatiotemporal%20Transformers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/20/2022-09-20-Autonomous%20Driving%20-%20BEVFormer%20Learning%20Bird%E2%80%99s-Eye-View%20Representation%20from%20Multi-Camera%20Images%20via%20Spatiotemporal%20Transformers/" class="post-title-link" itemprop="url">Autonomous Driving | BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-20 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-20T00:00:00+08:00">2022-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-05 19:26:40" itemprop="dateModified" datetime="2023-04-05T19:26:40+08:00">2023-04-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="bevformer-learning-birds-eye-view-representation-from-multi-camera-images-via-spatiotemporal-transformers">BEVFormer:
Learning Bird’s-Eye-View Representation from Multi-Camera Images via
Spatiotemporal Transformers</h2>
<p>多相机3D检测的工作。提出了spatial
cross-attention整合空间信息；提出了temporal self-
attention整合历史BEV信息。</p>
<p>代码开源：https://github.com/zhiqi-li/BEVFormer</p>
<h3 id="related-work">Related Work</h3>
<p><strong>deformable attention mechanism:</strong></p>
<figure>
<img src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220920-1.png" alt="deformable attention mechanism">
<figcaption aria-hidden="true">deformable attention
mechanism</figcaption>
</figure>
<p><strong>Camera-based 3D Perception:</strong></p>
<p>过去的3D object detection 任务和map segmentation
任务是相互独立的。</p>
<ol type="1">
<li><p>predict the 3D bounding boxes based on 2D bounding
boxes.</p></li>
<li><p>Another solution is to transform image features into BEV features
and predict 3D bounding boxes from the top-down view.</p></li>
</ol>
<p>从多相机特征生成BEV图。A straightforward method is converting
perspective view into the BEV through Inverse Perspective Mapping (IPM)
[35, 5]</p>
<h3 id="bevformer">BEVFormer</h3>
<ul>
<li><p>几个特殊的设计：</p>
<ul>
<li><p><strong>BEV queries;</strong></p>
<p>BEV queries are grid-shaped learnable parameters, which is designed
to query features in BEV space from multi-camera views via attention
mechanisms.</p></li>
<li><p><strong>spatial cross-attention</strong> and <strong>temporal
self-attention</strong>;</p>
<p>Spatial cross-attention and temporal self-attention are attention
layers working with BEV queries, which are used to lookup and aggregate
spatial features from multi-camera images as well as temporal features
from history BEV, according to the BEV query.</p></li>
</ul></li>
</ul>
<h4 id="bev-queries">BEV Queries</h4>
<p>We predefine a group of grid-shaped learnable parameters <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="13.263ex" height="2.501ex" role="img" focusable="false" viewBox="0 -911.5 5862.2 1105.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(1068.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2013.6,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(888,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1666,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(2714,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(3492,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></g></g></svg></mjx-container></span> as the
queries of BEVFormer, where H, W are the spatial shape of the BEV plane.
To be specific, the query <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="10.713ex" height="2.712ex" role="img" focusable="false" viewBox="0 -911.5 4735.3 1198.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mi" transform="translate(824,-150) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g><g data-mml-node="mo" transform="translate(1507.5,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2452.2,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(500,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1278,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></g></g></svg></mjx-container></span> located at <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.324ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4121.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(780.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(1836.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2225.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2797.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3242.2,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3732.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> of <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.79ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 791 898"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g></g></g></svg></mjx-container></span> is responsible
for the corresponding grid cell region in the BEV plane.</p>
<h4 id="spatial-cross-attention">Spatial Cross-Attention</h4>
<p>未来降低高昂的computational cost of vanilla multi-head attention
[42]，本文采用了deformable attention
[56]模型，这是一个资源高效的注意力层，其中每个BEV查询<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="2.782ex" height="2.08ex" role="img" focusable="false" viewBox="0 -725.5 1229.7 919.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mi" transform="translate(824,413) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g></g></g></svg></mjx-container></span>只与它在摄像机视图上感兴趣的区域交互。</p>
<p>spatial cross-attention (SCA)</p>
<figure>
<img src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220922-1.png" alt="spatial cross-attention">
<figcaption aria-hidden="true">spatial cross-attention</figcaption>
</figure>
<h4 id="temporal-self-attention">Temporal Self-Attention</h4>
<p>temporal self-attention (TSA) layer</p>
<figure>
<img src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220922-2.png" alt="temporal self-attention">
<figcaption aria-hidden="true">temporal self-attention</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>后续没仔细看了，整体框架感觉各个模块衔接比较自然流畅，这也是目前最好的自动驾驶视觉感知方案。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/05/2022-09-05%20-%20Python%E6%8A%A5%E9%94%99libGL%20error%20MESA-LOADER%20failed%20to%20open%20iris/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/05/2022-09-05%20-%20Python%E6%8A%A5%E9%94%99libGL%20error%20MESA-LOADER%20failed%20to%20open%20iris/" class="post-title-link" itemprop="url">Python报错libGL error: MESA-LOADER: failed to open iris</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-05 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-05T00:00:00+08:00">2022-09-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-03-25 09:54:15" itemprop="dateModified" datetime="2023-03-25T09:54:15+08:00">2023-03-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="python报错libgl-error-mesa-loader-failed-to-open-iris">Python报错libGL
error: MESA-LOADER: failed to open iris</h2>
<h3 id="问题背景">1 问题背景</h3>
<p>执行gym-like环境代码时，渲染动画输出时报错无法找到系统显卡驱动（笔记本，核显，Ubuntu），报错信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">libGL error: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: iris</span><br><span class="line">libGL error: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: iris</span><br><span class="line">libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: swrast</span><br></pre></td></tr></table></figure>
<p>然而，根据提示，在<code>/usr/lib/x86_64-linux-gnu/dri</code>路径下是可以找<code>iris_dri.so</code>驱动的。</p>
<h3 id="处理办法">2 处理办法</h3>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/538877347">这篇博客</a>提供了一个有效解决方法：</p>
<p>Step 1: 建立一个 /usr/lib/dri/iris_dri.so 的软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:/usr/lib$ sudo mkdir dri</span><br><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:/usr/lib$ cd dri </span><br><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:/usr/lib/dri$ sudo ln -s /usr/lib/x86_64-linux-gnu/dri/iris_dri.so iris_dri.so</span><br></pre></td></tr></table></figure>
<p>Step 2: 再次python执行代码，报错变为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">libGL error: MESA-LOADER: failed to open iris: /home/pcon/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29&#x27; not found (required by /usr/lib/dri/iris_dri.so) (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: iris</span><br><span class="line">libGL error: MESA-LOADER: failed to open iris: /home/pcon/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29&#x27; not found (required by /usr/lib/dri/iris_dri.so) (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: iris</span><br><span class="line">libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: swrast</span><br></pre></td></tr></table></figure>
<p>这是由于 conda 里的 libstdcxx-ng 版本不够高造成的。</p>
<p>Step 3：执行以下命令查看GLIBCXX版本信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:~/anaconda3$ strings /home/pcon/anaconda3//lib/libstdc++.so.6 | grep GLIBCXX</span><br></pre></td></tr></table></figure>
<p>Step 4：升级 conda 里的 libstdcxx-ng (根据<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/538877347">博客</a>)
。（截至2022.09该版本号有效）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:~/anaconda3$ conda install libstdcxx-ng=12.1.0</span><br></pre></td></tr></table></figure>
<p>再次查看版本号</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:~/anaconda3$ strings /home/pcon/anaconda3//lib/libstdc++.so.6 | grep GLIBCXX</span><br></pre></td></tr></table></figure>
<p>执行Python代码，成功运行！</p>
<p>ref</p>
<p>https://zhuanlan.zhihu.com/p/538877347</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/25/2022-08-25-Autonomous%20Driving%20-%20MP3%20A%20Unified%20Model%20to%20Map%20Perceive%20Predict%20and%20Plan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/25/2022-08-25-Autonomous%20Driving%20-%20MP3%20A%20Unified%20Model%20to%20Map%20Perceive%20Predict%20and%20Plan/" class="post-title-link" itemprop="url">Autonomous Driving | MP3 A Unified Model to Map Perceive Predict and Plan (Uber ATG 2021)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-25 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-25T00:00:00+08:00">2022-08-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-03-25 09:54:15" itemprop="dateModified" datetime="2023-03-25T09:54:15+08:00">2023-03-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="mp3-a-unified-model-to-map-perceive-predict-and-plan-uber-atg-2021">MP3:
A Unified Model to Map, Perceive, Predict and Plan (Uber ATG, 2021)</h2>
<ul>
<li>HD map具有的语义和几何信息使其成为自动驾驶系统的关键部件。但HD
map的成本很高，难扩展，尤其是厘米级精度（centimeter-level
accuracy）的情况下。因此能摆脱HD
Map（地图加载失败、地图老旧等）的算法值得研究。本文提出了一种<strong>end2end</strong>的<strong>不依赖地图</strong>的自动驾驶算法——MP3。</li>
<li>输入为：
<ul>
<li><strong>raw sensor data</strong></li>
<li><strong>high-level command</strong> (e.g., turn left at the
intersection)</li>
</ul></li>
<li>本文的定位为：mapless technology 的自动驾驶</li>
</ul>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>没有HD map的劣势：</p>
<ul>
<li><p>感知不能再依赖“人行道上的行人”、“道路上的车辆”这样的先验信息；</p></li>
<li><p>进行规划的空间变大了</p></li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220826-1.png"
alt="有地图和无地图对比" />
<figcaption aria-hidden="true">有地图和无地图对比</figcaption>
</figure>
<ul>
<li>车辆需要把到达抽象成为路口直行（going straight at an
intersection）、左转（turning left）和右转（turning
right）等高阶的行为指令。</li>
<li>大多数的无地图方法模仿专家的驾驶行为（朝向角、加速度），但是没有提供可解释的中间表征（intermediate
interpretable representations），而这可以帮助解释车辆的决策行为
<ul>
<li>End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316, 2016.</li>
<li>End-to-end driving via conditional imitation learning. In ICRA,
2018.</li>
<li>Urban driving with conditional imitation learning. arXiv preprint
arXiv:1912.00177, 2019.</li>
<li>Lift, splat, shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3d. In Proceedings of the European Conference
on Computer Vision, 2020.</li>
</ul></li>
<li>这些方法没有结构信息和先验知识，容易受到分布漂移（distributional
shift）的影响
<ul>
<li>A reduction of imitation learning and structured prediction to
no-regret online learning. In Proceedings of the fourteenth
international conference on artificial intelligence and statistics,
pages 627–635, 2011.</li>
</ul></li>
<li>一些使用在线地图的方法（获得道路边界、中心线），要么过度简单（假设了车道是平行的，但这只在高速场景适用），要么难以将静态环境的不确定性纳入运动规划，而运动规划对于降低风险至关重要。[2,
16, 18, 21, 37],
<ul>
<li>Deep multi-sensor lane detection. In IROS, pages 3102–3109. IEEE,
2018.</li>
<li>3d-lanenet: End-to-end 3d multiple lane detection. In Proceedings of
the IEEE International Conference on Computer Vision, pages 2921–2930,
2019.</li>
<li>Gen-lanenet: A generalized and scalable approach for 3d lane
detection. arXiv, pages arXiv–2003, 2020.</li>
<li>Hierarchical recurrent attention networks for structured online
maps. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3417–3426, 2018.</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<h4 id="online-mapping">2.1 Online Mapping:</h4>
<ul>
<li>特点：
<ul>
<li>satellite imagery (卫星图像)</li>
<li>gather dense information (采集车多次经过同一地方)</li>
<li>human-in-the-loop</li>
</ul></li>
<li>predicting map elements online:
<ul>
<li>3d-lanenet: End-to-end 3d multiple lane detection. In Proceedings of
the IEEE International Conference on Computer Vision, pages 2921–2930,
2019.</li>
<li>Gen-lanenet: A generalized and scalable approach for 3d lane
detection. arXiv, pages arXiv–2003, 2020.</li>
</ul></li>
</ul>
<h4 id="perception-and-prediction">2.2 Perception and Prediction</h4>
<ul>
<li><strong>生成轨迹集合</strong> generate a fixed set of trajectories
[6, 8–10, 26, 28, 30, 36, 56]</li>
<li><strong>画出样本特征分布</strong> draw samples to characterize the
distribution
<ul>
<li>Implicit latent variable model for scene-consistent motion
forecasting. arXiv preprint arXiv:2007.12036, 2020.</li>
<li>R2p2: A reparameterized pushforward policy for diverse, precise
generative path forecasting. In ECCV, 2018.</li>
<li>Multiple futures prediction. In Advances in Neural Information
Processing Systems, pages 15398–15408, 2019.</li>
</ul></li>
<li><strong>预测时间占用图</strong> predict temporal occupancy maps
<ul>
<li>Discrete residual flow for probabilistic pedestrian behavior
prediction. arXiv preprint arXiv:1910.08041, 2019.</li>
<li>The garden of forking paths: Towards multi-future trajectory
prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 10508–10518, 2020.</li>
<li>Scene compliant trajectory forecast with agent-centric
spatio-temporal grids. IEEE RA-L, 5(2):2816–2823, 2020.</li>
</ul></li>
<li>这些方法由于涉及了非最大抑制（non-maximum
suppression）和可信度阈值（confidence
thresholding），可能出现不安全的情况</li>
<li>occupancy grids:
<ul>
<li>Motionnet: Joint perception and motion prediction for autonomous
driving based on bird’s eye view maps. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages
11385–11395, 2020.</li>
<li>Learning occupancy grid maps with forward sensor models. Autonomous
robots, 15(2):111–127, 2003.</li>
<li><strong>Perceive, predict, and plan: Safe motion planning through
interpretable semantic representations.</strong> In Proceedings of the
European Conference on Computer Vision (ECCV), 2020.</li>
</ul></li>
</ul>
<h4 id="motion-planning">2.3 Motion Planning</h4>
<ul>
<li>从感知直接输出控制信号 （Driving policy transfer via modularity and
abstraction. arXiv preprint arXiv:1804.09364,
2018.）会面临稳定性和鲁棒性的问题（stability and robustness
issues）（Exploring the limitations of behavior cloning for autonomous
driving. In Proceedings of the IEEE International Conference on Computer
Vision, pages 9329–9338, 2019.）</li>
</ul>
<h3 id="interpretable-mapless-driving">3 Interpretable Mapless
Driving</h3>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220826-2.png"
alt="MP3 predicts probabilistic scene representations that are leveraged in motion planning as interpretable cost functions" />
<figcaption aria-hidden="true">MP3 predicts probabilistic scene
representations that are leveraged in motion planning as interpretable
cost functions</figcaption>
</figure>
<h4 id="extracting-geometric-and-semantic-features">3.1 Extracting
Geometric and Semantic Features</h4>
<ul>
<li>The result is a 3D tensor of size <span
class="math display">\[(\frac{H}{a},\frac{W}{a},\frac{Z}{a}\cdot T_p
)\]</span>,which is the input to our backbone network.</li>
<li>This network combines ideas from [9, 53] to extract geometric,
semantic and motion information about the scene.</li>
</ul>
<h4 id="interpretable-scene-representations">3.2 Interpretable Scene
Representations</h4>
<ul>
<li>道路先验信息和一些可解释的知识，使用 <code>online map</code>
表示</li>
<li>动态目标的位置、速度信息，使用 <code>dynamic occupancy field</code>
表示（the dynamic objects position and velocity into the future,
captured in our dynamic occupancy field）</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220826-3.png"
alt="Interpretable Scene representations" />
<figcaption aria-hidden="true">Interpretable Scene
representations</figcaption>
</figure>
<ul>
<li>具体而言，两种表征信息包括：</li>
</ul>
<p><strong>Online map representation:</strong></p>
<ul>
<li>Drivable area：以道路边缘为界的可行驶区域；</li>
<li>Reachable
lanes：可用车道是SDV在不违反任何交通规则的情况下可以到达的运动路径的子集。规划轨迹时，我们希望SDV靠近这些可到达的车道，并按照它们的方向行驶。因此，对于地平面中的每个像素，我们预测到最近的可到达车道中心线的无符号距离，在10米处截断，以及最近的可到达车道中心线分段的角度。</li>
<li>Intersection：被交通信号等或者交通标志控制的路段，需要根据信号灯或者标志按交通规定行驶；</li>
</ul>
<p><strong>Dynamic occupancy field:</strong></p>
<p>现有的行为预测算法包括不安全的离散决策unsafe discrete decisions such
as confidence thresholding and non-maximum suppression (NMS)</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220826-4.png"
alt="The motion field warps the occupancy over time" />
<figcaption aria-hidden="true">The motion field warps the occupancy over
time</figcaption>
</figure>
<ul>
<li>Initial Occupancy：一个BEV网格单元</li>
<li>Temporal Motion Field：a 2D BEV velocity vector (in m/s).</li>
<li><code>Note</code>：车辆、行人和自行车被视为单独的类别，每个类别都有自己的占用流。</li>
</ul>
<p><strong>Probabilistic Model:</strong></p>
<p>online Map 分为以下几个通道：</p>
<ul>
<li><p>可到达区域<span class="math display">\[M^A_i\]</span></p></li>
<li><p>路口<span class="math display">\[M^I_i\]</span></p></li>
<li><p>到最近车道线的距离。the direction of the closest lane centerline
in the reachable lanes <span
class="math display">\[M^{\theta}_i\]</span> as a Von Mises distribution
since it has support between <span
class="math display">\[[\pi,\pi]\]</span>.</p></li>
<li><p>可到达车道中线的截断距离变换为拉普拉斯算子。We model the
truncated distance transform to the reachable lanes centerline <span
class="math display">\[M^D_i\]</span>​ as a Laplacian, which we
empirically found to yield more accurate results than a
Gaussian</p></li>
</ul>
<p>建模动态物体的occupancy <span
class="math display">\[O^c\]</span>,为伯努利随机分布<span
class="math display">\[O^c_{t,i}\]</span>，<span
class="math display">\[c\in
\{行人，车辆，自行车\}\]</span>（考虑这些物体未来行为的多模态（直走或左转）和不确定性），用<span
class="math display">\[K_{t,i}\]</span>建模基于K个BEV运动向量<span
class="math display">\[V^c_{t,i,k}\]</span>的行为分类分布</p>
<p>the probability of future occupancy under our probabilistic model, we
first define the probability of occupancy flowing from location <span
class="math display">\[i_1\]</span> to location <span
class="math display">\[i_2\]</span> between two consecutive time steps t
and t + 1 as follows:</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220829-1.png"
alt="the probability of occupancy flowing" />
<figcaption aria-hidden="true">the probability of occupancy
flowing</figcaption>
</figure>
<h4 id="motion-planning-1">3.3 Motion Planning</h4>
<p>设计了一个基于采样的轨迹规划器，其根据运动学灵活的生成多种轨迹，然后使用一个learned
scoring function选择轨迹。</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220830-1.png"
alt="规划器的轨迹选择方式" />
<figcaption aria-hidden="true">规划器的轨迹选择方式</figcaption>
</figure>
<h5 id="trajectory-sampling">3.3.1 Trajectory Sampling</h5>
<p>Search-based optimal motion planning for automated driving. In IROS,
2018</p>
<ul>
<li>根据<span
class="math display">\[(v_x,a_x,k_x)\]</span>在专家轨迹数据集中检索专家轨迹，<span
class="math display">\[x\]</span>表示当前自车状态，检索出的轨迹有不同的初始速度和朝向。因此使用加速度和转向角来描述轨迹<span
class="math display">\[(a,\dot k)_t,t=0,...,T\]</span>，输入到a bicycle
model [38]生成具有连续速度和转向角的轨迹。
<ul>
<li>[38]. The kinematic bicycle model: A consistent model for planning
feasible trajectories for autonomous vehicles? In 2017 IEEE Intelligent
Vehicles Symposium (IV), pages 812–818. IEEE, 2017.</li>
</ul></li>
<li>文献[37]提供了一个忽略自车初始状态的简化的轨迹生成模型。
<ul>
<li>[37]. Lift, splat, shoot: Encoding images from arbitrary camera rigs
by implicitly unprojecting to 3d. In Proceedings of the European
Conference on Computer Vision, 2020.</li>
</ul></li>
</ul>
<h5 id="route-prediction">3.3.2 Route Prediction</h5>
<ul>
<li>由于无地图驾驶没有车道线follow，本文假设遵循command来行驶，指令<span
class="math display">\[c = (a, d)\]</span>, where <span
class="math display">\[a \in \{keep lane, turn left, turn
right\}\]</span> is a discrete high-level action, and <span
class="math display">\[d\]</span> an approximate longitudinal distance
to the
action（行为的纵向距离）（d经过”rasterize”处理），输入给CoordConv[29]
<ul>
<li>An intriguing failing of convolutional neural networks and the
coordconv solution. In Advances in Neural Information Processing
Systems, pages 9605–9616, 2018.</li>
</ul></li>
</ul>
<h5 id="trajectory-scoring">3.3.2 Trajectory Scoring</h5>
<ul>
<li>Routing and Driving on Roads:
该评分函数鼓励车辆行驶在概率图R中概率高的区域</li>
</ul>
<p><span class="math display">\[
f_r(\tau,R)=-m(\tau)\min_{i \in m(\tau)} R_i
\]</span></p>
<p>其中<span
class="math display">\[m(\tau)\]</span>是BEV图中和自车轨迹<span
class="math display">\[\tau\]</span>重合的格子单元（grid-cells that
overlap with SDV polygon in trajectory <span
class="math display">\[\tau\]</span>)</p>
<p>离开车道损失： <span class="math display">\[
f_a(x,M)=\max_{i \in m(x)}[1-P(M_i^A)]
\]</span></p>
<ul>
<li><p>Safety</p></li>
<li><p>Comfort</p>
<p>惩罚jerk和加速度</p></li>
</ul>
<h4 id="learning">3.4. Learning</h4>
<p>两阶段的训练。我们分两个阶段优化我们的驾驶模型。我们首先训练<strong>online
map</strong>、<strong>dynamic occupancy
field</strong>和<strong>routing</strong>。一旦这些被收敛，在第二阶段，我们保持这些部分冻结，并为得分函数的线性组合训练规划器权重。我们发现这种两阶段的培训比端到端的培训更稳定。（We
optimize our driving model in two stages. We first train the online map,
dynamic occupancy field, and routing. Once these are converged, in a
second stage, we keep these parts frozen and train the planner weights
for the linear combination of scoring functions. We found this 2-stage
training empirically more stable than training end-to-end.）</p>
<h3 id="experimental-evaluation">4. Experimental Evaluation</h3>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-1.png"
alt="性能指标" />
<figcaption aria-hidden="true">性能指标</figcaption>
</figure>
<ul>
<li>Imitation Learning (IL), where the future positions of the SDV are
predicted directly from the scene context features, and is trained using
L2 loss.</li>
<li>Conditional Imitation Learning (CIL) [11], which is similar to IL
but the trajectory is conditioned on the driving command.
<ul>
<li>End-to-end driving via conditional imitation learning. In ICRA,
2018.</li>
</ul></li>
<li>Neural Motion Planner (NMP) [55], where a planning cost-volume as
well as detection and prediction are predicted in a multi-task fashion
from the scene context features, and Trajectory Classification (TC)
[37], where a cost-volume is predicted similar to NMP, but the
trajectory cost is used to create a probability distribution over the
trajectories and is trained by optimizing for the likelihood of the
expert trajectory.
<ul>
<li>Lift, splat, shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3d. In Proceedings of the European Conference
on Computer Vision, 2020.</li>
<li>End-to-end interpretable neural motion planner. In CVPR, 2019.</li>
</ul></li>
<li>Finally, we extend NMP to consider the high-level command by
learning a separate costing network for each discrete action
(CNMP).</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-2.png"
alt="Backbone Network" />
<figcaption aria-hidden="true">Backbone Network</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-3.png"
alt="Mapping Network" />
<figcaption aria-hidden="true">Mapping Network</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-4.png"
alt="Perception and Prediction Network" />
<figcaption aria-hidden="true">Perception and Prediction
Network</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-5.png"
alt="Routing Network" />
<figcaption aria-hidden="true">Routing Network</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-6.png"
alt="Sets of trajectories retrieved from the expert demonstrations." />
<figcaption aria-hidden="true">Sets of trajectories retrieved from the
expert demonstrations.</figcaption>
</figure>
<p>后面还有大量实验情景的展示图。</p>
<h3 id="总结">总结</h3>
<p>比较有想法的一个工作，做得比较细致，但是介绍相对粗略，可以仔细研究。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Txing"
      src="/images/my_photo.jpg">
  <p class="site-author-name" itemprop="name">Txing</p>
  <div class="site-description" itemprop="description">泛用类人决战型机器人博士</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">229</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/yourname" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Txing</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
