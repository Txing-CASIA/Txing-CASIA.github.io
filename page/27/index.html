<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"txing-casia.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="泛用类人决战型机器人博士">
<meta property="og:type" content="website">
<meta property="og:title" content="Txing">
<meta property="og:url" content="https://txing-casia.github.io/page/27/index.html">
<meta property="og:site_name" content="Txing">
<meta property="og:description" content="泛用类人决战型机器人博士">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Txing">
<meta property="article:tag" content="Txing">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://txing-casia.github.io/page/27/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Txing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Txing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到 | 伽蓝之堂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2019/05/17/2019-05-17-Q884-Uncommon-Words-from-Two-Sentences/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/17/2019-05-17-Q884-Uncommon-Words-from-Two-Sentences/" class="post-title-link" itemprop="url">Q884 Uncommon Words from Two Sentences</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-17 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-17T00:00:00+08:00">2019-05-17</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="uncommon-words-from-two-sentences"><a
target="_blank" rel="noopener" href="https://leetcode.com/problems/uncommon-words-from-two-sentences/">Uncommon
Words from Two Sentences</a></h1>
<h2 id="question">Question</h2>
<blockquote>
<p>We are given two sentences <code>A</code> and <code>B</code>. (A
<em>sentence</em> is a string of space separated words. Each word
consists only of lowercase letters.)</p>
<p>A word is <em>uncommon</em> if it appears exactly once in one of the
sentences, and does not appear in the other sentence.</p>
<p>Return a list of all uncommon words.</p>
<p>You may return the list in any order.</p>
</blockquote>
<blockquote>
<p><strong>Example 1:</strong> Input: A = "this apple is sweet", B =
"this apple is sour" Output: ["sweet","sour"]</p>
</blockquote>
<blockquote>
<p><strong>Example 2:</strong> Input: A = "apple apple", B = "banana"
Output: ["banana"]</p>
</blockquote>
<blockquote>
<p><strong>Note:</strong></p>
<ol type="1">
<li><code>0 &lt;= A.length &lt;= 200</code></li>
<li><code>0 &lt;= B.length &lt;= 200</code></li>
<li><code>A</code> and <code>B</code> both contain only spaces and
lowercase letters.</li>
</ol>
</blockquote>
<h3 id="approach-1-counting">Approach 1: Counting</h3>
<p><strong>Intuition and Algorithm</strong></p>
<p>Every uncommon word occurs exactly once in total. We can count the
number of occurrences of every word, then return ones that occur exactly
once.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">uncommonFromSentences</span>(<span class="params">self, A, B</span>):</span><br><span class="line">        count = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> A.split():</span><br><span class="line">            count[i] = count.get(i, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">            <span class="comment"># .get(i,0)意思是如果i在count里不存在，返回默认为0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> B.split():</span><br><span class="line">            count[i] = count.get(i, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#Alternatively:</span></span><br><span class="line">        <span class="comment">#count = collections.Counter(A.split())</span></span><br><span class="line">        <span class="comment">#count += collections.Counter(B.split())</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [word <span class="keyword">for</span> word <span class="keyword">in</span> count <span class="keyword">if</span> count[word] == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    A = <span class="string">&quot;this apple is sweet&quot;</span></span><br><span class="line">    B = <span class="string">&quot;this apple is sour&quot;</span></span><br><span class="line">    answer=Solution()</span><br><span class="line">    results=answer.uncommonFromSentences(A,B)</span><br><span class="line">    <span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li>Runtime: 32 ms, faster than 99.14% of Python3 online submissions for
Uncommon Words from Two Sentences.</li>
<li>Memory Usage: 13.3 MB, less than 23.76% of Python3 online
submissions for Uncommon Words from Two Sentences.</li>
</ul>
<hr />
<h3 id="approach-2-counting-original">Approach 2: Counting
(Original)</h3>
<p><strong>Intuition and Algorithm</strong></p>
<p>Every uncommon word occurs exactly once in total. We can count the
number of occurrences of every word, then return ones that occur exactly
once.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">uncommonFromSentences</span>(<span class="params">self, A: <span class="built_in">str</span>, B: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        count = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> A.split()+B.split():</span><br><span class="line">            count[i] = count.get(i, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> [i <span class="keyword">for</span> i <span class="keyword">in</span> count <span class="keyword">if</span> count[i] == <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Runtime: 36 ms, faster than 92.30% of Python3 online submissions
for Uncommon Words from Two Sentences.</p></li>
<li><p>Memory Usage: 13.1 MB, less than 75.47% of Python3 online
submissions for Uncommon Words from Two Sentences.</p></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2019/05/16/2019-05-16-Q136-Single-Number/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/16/2019-05-16-Q136-Single-Number/" class="post-title-link" itemprop="url">Q136 Single Number</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-16 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-16T00:00:00+08:00">2019-05-16</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>917</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="single-number"><a
target="_blank" rel="noopener" href="https://leetcode.com/problems/single-number/">Single
Number</a></h1>
<h2 id="question">Question</h2>
<blockquote>
<p>Given a <strong>non-empty</strong> array of integers, every element
appears <em>twice</em> except for one. Find that single one.</p>
</blockquote>
<blockquote>
<p><strong>Example 1:</strong> Input: [2,2,1] Output: 1</p>
</blockquote>
<blockquote>
<p><strong>Example 2:</strong> Input: [4,1,2,1,2] Output: 4</p>
</blockquote>
<blockquote>
<p><strong>Note:</strong></p>
<p>Your algorithm should have a linear runtime complexity. Could you
implement it without using extra memory?</p>
</blockquote>
<h3 id="approach-1-math">Approach 1: Math</h3>
<p><strong>Concept</strong></p>
<p><span class="math display">\[2 * (a + b + c) - (a + a + b + b + c) =
c\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">singleNumber</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">2</span> * <span class="built_in">sum</span>(<span class="built_in">set</span>(nums)) - <span class="built_in">sum</span>(nums)</span><br></pre></td></tr></table></figure>
<ul>
<li>Runtime: 32 ms, faster than 99.87% of Python3 online submissions for
Single Number.</li>
<li>Memory Usage: 15.1 MB, less than 14.91% of Python3 online
submissions for Single Number.</li>
</ul>
<h3 id="approach-2-hash-table">Approach 2: Hash Table</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">singleNumber</span>(<span class="params">self, nums</span>):</span><br><span class="line">        hash_table = &#123;&#125; <span class="comment"># 等同 hash_table=dict()</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                hash_table.pop(i) <span class="comment"># 如果字典为空，转入异常状态</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                hash_table[i] = <span class="number">1</span> <span class="comment"># 填充hash表</span></span><br><span class="line">        <span class="keyword">return</span> hash_table.popitem()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>Runtime: 44 ms, faster than 68.21% of Python3 online submissions for
Single Number.</li>
<li>Memory Usage: 15.1 MB, less than 12.23% of Python3 online
submissions for Single Number.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2019/05/15/2019-05-13-Pytorch-Tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/15/2019-05-13-Pytorch-Tutorial/" class="post-title-link" itemprop="url">Pytorch tutorial</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-15 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-15T00:00:00+08:00">2019-05-15</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>20 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="pytorch-tutorial"><a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/">Pytorch Tutorial</a></h1>
<h2 id="tensors">Tensors</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct a 5x3 matrix, uninitialized</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct a randomly initialized matrix</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct a matrix filled zeros and of dtype long</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct a tensor directly from data</span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># or create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user</span></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)      <span class="comment"># new_* methods take in sizes</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>)    <span class="comment"># override dtype!</span></span><br><span class="line"><span class="built_in">print</span>(x)                                      <span class="comment"># result has the same size</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get its size:</span></span><br><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"><span class="comment"># &#x27;torch.Size&#x27; is in fact a tuple, so it supports all tuple operations.</span></span><br></pre></td></tr></table></figure>
<h2 id="operations">Operations</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Addition: syntax 1</span></span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Addition: syntax 2</span></span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Addition: providing an output tensor as argument</span></span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Addition: in-place</span></span><br><span class="line">y.add_(x)<span class="comment"># adds x to y,y=x+y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Any operation that mutates a tensor in-place is post-fixed with an _. For example: x.copy_(y), x.t_(), will change x.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can use standard NumPy-like indexing with all bells and whistles!</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>])<span class="comment"># [row , column]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Resizing: If you want to resize/reshape tensor, you can use torch.view:</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)<span class="comment"># remember x is a tapel </span></span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line"><span class="built_in">print</span>(x.size(), y.size(), z.size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># If you have a one element tensor, use .item() to get the value as a Python number</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.item()) <span class="comment"># !!!!</span></span><br></pre></td></tr></table></figure>
<p><strong>Read later:</strong></p>
<p>100+ Tensor operations, including transposing, indexing, slicing,
mathematical operations, linear algebra, random numbers, etc., are
described <a target="_blank" rel="noopener" href="https://pytorch.org/docs/torch">here</a>.</p>
<h2 id="numpy-bridge">NumPy Bridge</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Converting a Torch Tensor to a NumPy array and vice versa is a breeze.</span></span><br><span class="line"><span class="comment"># The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), and changing one will change the other.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Converting a Torch Tensor to a NumPy Array</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line">a.add_(<span class="number">1</span>)<span class="comment"># b will change at the same time </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Converting NumPy Array to Torch Tensor</span></span><br><span class="line"><span class="comment"># See how changing the np array changed the Torch Tensor automatically</span></span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># All the Tensors on the CPU except a CharTensor support converting to NumPy and back.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="cuda-tensors">CUDA Tensors</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tensors can be moved onto any device using the &#x27;.to&#x27; method.</span></span><br><span class="line"><span class="comment"># let us run this cell only if CUDA is available</span></span><br><span class="line"><span class="comment"># We will use ``torch.device`` objects to move tensors in and out of GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)          <span class="comment"># a CUDA device object</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># or just use strings ``.to(&quot;cuda&quot;)``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))       <span class="comment"># ``.to`` can also change dtype together!</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="autograd-automatic-differentiation">AUTOGRAD: AUTOMATIC
DIFFERENTIATION</h2>
<p>Central to all neural networks in PyTorch is the
<code>autograd</code> package. Let’s first briefly visit this, and we
will then go to training our first neural network.</p>
<p>The <code>autograd</code> package provides automatic differentiation
for all operations on Tensors. It is a define-by-run framework, which
means that your backprop is defined by how your code is run, and that
every single iteration can be different.</p>
<p>Let us see this in more simple terms with some examples.</p>
<h3 id="tensor">Tensor</h3>
<p><code>torch.Tensor</code> is the central class of the package. If you
set its attribute <code>.requires_grad</code> as <code>True</code>, it
starts to track all operations on it. When you finish your computation
you can call <code>.backward()</code> and have all the gradients
computed automatically. The gradient for this tensor will be accumulated
into <code>.grad</code> attribute.</p>
<p>To stop a tensor from tracking history, you can call
<code>.detach()</code> to detach it from the computation history, and to
prevent future computation from being tracked.</p>
<p>To prevent tracking history (and using memory), you can also wrap the
code block in <code>with torch.no_grad():</code>. This can be
particularly helpful when evaluating a model because the model may have
trainable parameters with <code>requires_grad=True</code>, but for which
we don’t need the gradients.</p>
<p>There’s one more class which is very important for autograd
implementation - a <code>Function</code>.</p>
<p><code>Tensor</code> and <code>Function</code> are interconnected and
build up an acyclic graph, that encodes a complete history of
computation. Each tensor has a <code>.grad_fn</code> attribute that
references a <code>Function</code> that has created the
<code>Tensor</code> (except for Tensors created by the user - their
<code>grad_fn isNone</code>).</p>
<p>If you want to compute the derivatives, you can call
<code>.backward()</code> on a <code>Tensor</code>. If
<code>Tensor</code> is a scalar (i.e. it holds a one element data), you
don’t need to specify any arguments to <code>backward()</code>, however
if it has more elements, you need to specify a <code>gradient</code>
argument that is a tensor of matching shape.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z, out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x27;.requires_grad_( ... )&#x27; changes an existing Tensor’s requires_grad flag in-place. The input flag defaults to False if not given.</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">b = (a * a).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(b.grad_fn)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="gradients">Gradients</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Let’s backprop now. Because out contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.)).</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># Print gradients d(out)/dx</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># let’s take a look at an example of vector-Jacobian product:</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>: <span class="comment"># ||x||_P</span></span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y) </span><br><span class="line"><span class="comment"># tensor([ 50.3352, -563.2849, 1482.2238], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now in this case y is no longer a scalar. torch.autograd could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to backward as argument:</span></span><br><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also stop autograd from tracking history on Tensors with .requires_grad=True by wrapping the code block in with torch.no_grad():</span></span><br><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"><span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():<span class="comment"># introduce a condition</span></span><br><span class="line">    <span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    y=x**<span class="number">2</span>*t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradf</span>(<span class="params">x</span>):</span><br><span class="line">    dx=<span class="number">2</span>*x*t.exp(x)+x**<span class="number">2</span>*t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> dx</span><br><span class="line"></span><br><span class="line">x=V(t.rand(<span class="number">3</span>,<span class="number">4</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=f(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gradf(x)) <span class="comment"># artificial grad</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line">y.backward(t.ones(y.size()))</span><br><span class="line"><span class="built_in">print</span>(x.grad) <span class="comment"># autograd </span></span><br></pre></td></tr></table></figure>
<p><strong>Read Later:</strong> Documentation of autograd and Function
is at https://pytorch.org/docs/autograd</p>
<h2 id="neural-networks">NEURAL NETWORKS</h2>
<p>Neural networks can be constructed using the <code>torch.nn</code>
package.</p>
<p>Now that you had a glimpse of <code>autograd</code>, <code>nn</code>
depends on <code>autograd</code> to define models and differentiate
them. An <code>nn.Module</code> contains layers, and a method
<code>forward(input)</code>that returns the <code>output</code>.</p>
<p>For example, look at this network that classifies digit images:</p>
<h3 id="define-the-network">Define the network</h3>
<p>Let’s define this network:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>You just have to define the <code>forward</code> function, and the
<code>backward</code> function (where gradients are computed) is
automatically defined for you using <code>autograd</code>. You can use
any of the Tensor operations in the <code>forward</code> function.</p>
<p>The learnable parameters of a model are returned by
<code>net.parameters()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight</span></span><br></pre></td></tr></table></figure>
<p>Let try a random 32x32 input. Note: expected input size of this net
(LeNet) is 32x32. To use this net on MNIST dataset, please resize the
images from the dataset to 32x32.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>
<p>Zero the gradient buffers of all parameters and backprops with random
gradients:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>Notice:</strong></p>
<p><code>torch.nn</code> only supports mini-batches. The entire
<code>torch.nn</code> package only supports inputs that are a mini-batch
of samples, and not a single sample.</p>
<p>For example, <code>nn.Conv2d</code> will take in a 4D Tensor of
<code>nSamples x nChannels x Height x Width</code>.</p>
<p>If you have a single sample, just use
<strong><code>input.unsqueeze(0)</code> </strong>to add a fake batch
dimension.</p>
</blockquote>
<p>Before proceeding further, let’s recap all the classes you’ve seen so
far.</p>
<p><strong>Recap:</strong></p>
<ul>
<li><code>torch.Tensor</code> - A <em>multi-dimensional array</em> with
support for autograd operations like <code>backward()</code>. Also
<em>holds the gradient</em> w.r.t. the tensor.</li>
<li><code>nn.Module</code> - Neural network module. <em>Convenient way
of encapsulating parameters</em>, with helpers for moving them to GPU,
exporting, loading, etc.</li>
<li><code>nn.Parameter</code> - A kind of Tensor, that is
<em>automatically registered as a parameter when assigned as an
attribute to a</em> <code>Module</code>.</li>
<li><code>autograd.Function</code> - Implements <em>forward and backward
definitions of an autograd operation</em>. Every <code>Tensor</code>
operation creates at least a single <code>Function</code> node that
connects to functions that created a <code>Tensor</code> and <em>encodes
its history</em>.</li>
</ul>
<p><strong>At this point, we covered:</strong></p>
<ul>
<li>Defining a neural network</li>
<li>Processing inputs and calling backward</li>
</ul>
<p><strong>Still Left:</strong></p>
<ul>
<li>Computing the loss</li>
<li>Updating the weights of the network</li>
</ul>
<h3 id="loss-function">Loss Function</h3>
<p>A loss function takes the (output, target) pair of inputs, and
computes a value that estimates how far away the output is from the
target.</p>
<p>There are several different <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/nn.html#loss-functions">loss
functions</a> under the nn package. A simple loss is:
<code>nn.MSELoss</code> which computes the mean-squared error between
the input and the target.</p>
<p>For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line"><span class="comment"># -1指在不告诉函数有多少列的情况下，根据原tensor数据和batchsize自动分配列数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>
<p>Now, if you follow <code>loss</code> in the backward direction, using
its <code>.grad_fn</code> attribute, you will see a graph of
computations that looks like this:</p>
<blockquote>
<p>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu
-&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt;
relu -&gt; linear -&gt; MSELoss -&gt; loss</p>
</blockquote>
<p>So, when we call <code>loss.backward()</code>, the whole graph is
differentiated w.r.t. the loss, and all Tensors in the graph that has
<code>requires_grad=True</code> will have their <code>.grad</code>
Tensor accumulated with the gradient.</p>
<p>For illustration, let us follow a few steps backward:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure>
<h3 id="backprop">Backprop</h3>
<p>To backpropagate the error all we have to do is to
<code>loss.backward()</code>. You need to clear the existing gradients
though, else gradients will be accumulated to existing gradients.</p>
<p>Now we shall call <code>loss.backward()</code>, and have a look at
conv1’s bias gradients before and after the backward.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>
<p>Now, we have seen how to use loss functions.</p>
<p><strong>Read Later:</strong></p>
<p>The neural network package contains various modules and loss
functions that form the building blocks of deep neural networks. A full
list with documentation is <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/nn">here</a>.</p>
<p><strong>The only thing left to learn is:</strong></p>
<p>Updating the weights of the network</p>
<h3 id="update-the-weights">Update the weights</h3>
<p>The simplest update rule used in practice is the Stochastic Gradient
Descent (SGD): <span class="math display">\[
weight = weight - learning \ rate * gradient
\]</span> We can implement this using simple python code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>
<p>However, as you use neural networks, you want to use various
different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To
enable this, we built a small package: <code>torch.optim</code> that
implements all these methods. Using it is very simple:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Observe how gradient buffers had to be manually set to zero using
<code>optimizer.zero_grad()</code>. This is because gradients are
accumulated as explained in <a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#backprop">Backprop</a>
section.</p>
</blockquote>
<h2 id="training-a-classifier">TRAINING A CLASSIFIER</h2>
<p>This is it. You have seen how to define neural networks, compute loss
and make updates to the weights of the network.</p>
<p>Now you might be thinking,</p>
<h3 id="what-about-data">What about data?</h3>
<p>Generally, when you have to deal with image, text, audio or video
data, you can use standard python packages that load data into a numpy
array. Then you can convert this array into a
<code>torch.*Tensor</code>.</p>
<ul>
<li>For images, packages such as Pillow, OpenCV are useful</li>
<li>For audio, packages such as scipy and librosa</li>
<li>For text, either raw Python or Cython based loading, or NLTK and
SpaCy are useful</li>
</ul>
<p>Specifically for vision, we have created a package called
<code>torchvision</code>, that has data loaders for common datasets such
as Imagenet, CIFAR10, MNIST, etc. and data transformers for images,
viz., <code>torchvision.datasets</code> and
<code>torch.utils.data.DataLoader</code>.</p>
<p>This provides a huge convenience and avoids writing boilerplate
code.</p>
<p>For this tutorial, we will use the CIFAR10 dataset. It has the
classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’,
‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32,
i.e. 3-channel color images of 32x32 pixels in size.</p>
<h3 id="training-an-image-classifier">Training an image classifier</h3>
<p>We will do the following steps in order:</p>
<ol type="1">
<li>Load and normalizing the CIFAR10 training and test datasets using
<code>torchvision</code></li>
<li>Define a Convolutional Neural Network</li>
<li>Define a loss function</li>
<li>Train the network on the training data</li>
<li>Test the network on the test data</li>
</ol>
<h4 id="loading-and-normalizing-cifar10">1. Loading and normalizing
CIFAR10</h4>
<p>Using <code>torchvision</code>, it’s extremely easy to load
CIFAR10.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>
<p>The output of torchvision datasets are PILImage images of range [0,
1]. We transform them to Tensors of normalized range [-1, 1].</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>Let us show some of the training images, for fun.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(trainloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<h4 id="define-a-convolutional-neural-network">2. Define a Convolutional
Neural Network</h4>
<p>Copy the neural network from the Neural Networks section before and
modify it to take 3-channel images (instead of 1-channel images as it
was defined).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>
<h4 id="define-a-loss-function-and-optimizer">3. Define a Loss function
and optimizer</h4>
<p>Let’s use a Classification Cross-Entropy loss and SGD with
momentum.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h4 id="train-the-network">4. Train the network</h4>
<p>This is when things start to get interesting. We simply have to loop
over our data iterator, and feed the inputs to the network and
optimize.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="test-the-network-on-the-test-data">5. Test the network on the
test data</h4>
<p>We have trained the network for 2 passes over the training dataset.
But we need to check if the network has learnt anything at all.</p>
<p>We will check this by predicting the class label that the neural
network outputs, and checking it against the ground-truth. If the
prediction is correct, we add the sample to the list of correct
predictions.</p>
<p>Okay, first step. Let us display an image from the test set to get
familiar.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = <span class="built_in">iter</span>(testloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GroundTruth: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<p>Okay, now let us see what the neural network thinks these examples
above are:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure>
<p>The outputs are energies for the 10 classes. The higher the energy
for a class, the more the network thinks that the image is of the
particular class. So, let’s get the index of the highest energy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<p>The results seem pretty good.</p>
<p>Let us look at how the network performs on the whole dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>
<p>That looks waaay better than chance, which is 10% accuracy (randomly
picking a class out of 10 classes). Seems like the network learnt
something.</p>
<p>Hmmm, what are the classes that performed well, and the classes that
did not perform well:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class_correct = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line">class_total = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy of %5s : %2d %%&#x27;</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>
<p>Okay, so what next?</p>
<p>How do we run these neural networks on the GPU?</p>
<h3 id="training-on-gpu">Training on GPU</h3>
<p>Just like how you transfer a Tensor onto the GPU, you transfer the
neural net onto the GPU.</p>
<p>Let’s first define our device as the first visible cuda device if we
have CUDA available:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(device)</span><br></pre></td></tr></table></figure>
<p>The rest of this section assumes that <code>device</code> is a CUDA
device.</p>
<p>Then these methods will recursively go over all modules and convert
their parameters and buffers to CUDA tensors:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.to(device)</span><br></pre></td></tr></table></figure>
<p>Remember that you will have to send the inputs and targets at every
step to the GPU too:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = inputs.to(device), labels.to(device)</span><br></pre></td></tr></table></figure>
<p>Why don't I notice MASSIVE speedup compared to CPU? Because your
network is really small.</p>
<p><strong>Exercise:</strong> Try increasing the width of your network
(argument 2 of the first <code>nn.Conv2d</code>, and argument 1 of the
second <code>nn.Conv2d</code> – they need to be the same number), see
what kind of speedup you get.</p>
<p><strong>Goals achieved</strong>:</p>
<ul>
<li>Understanding PyTorch’s Tensor library and neural networks at a high
level.</li>
<li>Train a small neural network to classify images</li>
</ul>
<h2 id="training-on-multiple-gpus">Training on multiple GPUs</h2>
<p>If you want to see even more MASSIVE speedup using all of your GPUs,
please check out <a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html">Optional:
Data Parallelism</a>.</p>
<h2 id="where-do-i-go-next">Where do I go next?</h2>
<ul>
<li><a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">Train
neural nets to play video games</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/pytorch/examples/tree/master/imagenet">Train a
state-of-the-art ResNet network on imagenet</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/pytorch/examples/tree/master/dcgan">Train a
face generator using Generative Adversarial Networks</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/pytorch/examples/tree/master/word_language_model">Train
a word-level language model using Recurrent LSTM networks</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/examples">More examples</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/tutorials">More
tutorials</a></li>
<li><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/">Discuss PyTorch on the
Forums</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.slack.com/messages/beginner/">Chat with
other users on Slack</a></li>
</ul>
<hr />
<h2 id="save-model"><a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">Save
Model</a></h2>
<p>In PyTorch, the learnable parameters (i.e. weights and biases) of an
<code>torch.nn.Module</code> model are contained in the model’s
<em>parameters</em>(accessed with <code>model.parameters()</code>). A
<em>state_dict</em> is simply a Python dictionary object that maps each
layer to its parameter tensor. Note that only layers with learnable
parameters (convolutional layers, linear layers, etc.) and registered
buffers (batchnorm’s running_mean) have entries in the model’s
<em>state_dict</em>. Optimizer objects (<code>torch.optim</code>) also
have a <em>state_dict</em>, which contains information about the
optimizer’s state, as well as the hyperparameters used.</p>
<p>Because <em>state_dict</em> objects are Python dictionaries, they can
be easily saved, updated, altered, and restored, adding a great deal of
modularity to PyTorch models and optimizers.</p>
<p><strong>Example:</strong></p>
<p>Let’s take a look at the <em>state_dict</em> from the simple model
used in the <a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py">Training
a classifier</a> tutorial.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TheModelClass</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TheModelClass, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line">model = TheModelClass()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize optimizer</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print model&#x27;s state_dict</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model&#x27;s state_dict:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> param_tensor <span class="keyword">in</span> model.state_dict():</span><br><span class="line">    <span class="built_in">print</span>(param_tensor, <span class="string">&quot;\t&quot;</span>, model.state_dict()[param_tensor].size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print optimizer&#x27;s state_dict</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optimizer&#x27;s state_dict:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> var_name <span class="keyword">in</span> optimizer.state_dict():</span><br><span class="line">    <span class="built_in">print</span>(var_name, <span class="string">&quot;\t&quot;</span>, optimizer.state_dict()[var_name])</span><br></pre></td></tr></table></figure>
<p><strong>Save:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure>
<p><strong>Load:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>When saving a model for inference, it is only necessary to save the
trained model’s learned parameters. Saving the model’s
<em>state_dict</em> with the <code>torch.save()</code> function will
give you the most flexibility for restoring the model later, which is
why it is the recommended method for saving models.</p>
<p>A common PyTorch convention is to save models using either a
<code>.pt</code> or <code>.pth</code> file extension.</p>
<p>Remember that you must call <code>model.eval()</code> to set dropout
and batch normalization layers to evaluation mode before running
inference. Failing to do this will yield inconsistent inference
results.</p>
<p>NOTE</p>
<p>Notice that the <code>load_state_dict()</code> function takes a
dictionary object, NOT a path to a saved object. This means that you
must deserialize the saved <em>state_dict</em> before you pass it to the
<code>load_state_dict()</code> function. For example, you CANNOT load
using<code>model.load_state_dict(PATH)</code>.</p>
<h3 id="saveload-entire-model">Save/Load Entire Model</h3>
<p><strong>Save:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, PATH)</span><br></pre></td></tr></table></figure>
<p><strong>Load:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model class must be defined somewhere</span></span><br><span class="line">model = torch.load(PATH)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<p>This save/load process uses the most intuitive syntax and involves
the least amount of code. Saving a model in this way will save the
entire module using Python’s <a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/pickle.html">pickle</a> module.
The disadvantage of this approach is that the serialized data is bound
to the specific classes and the exact directory structure used when the
model is saved. The reason for this is because pickle does not save the
model class itself. Rather, it saves a path to the file containing the
class, which is used during load time. Because of this, your code can
break in various ways when used in other projects or after
refactors.</p>
<p>A common PyTorch convention is to save models using either a
<code>.pt</code> or <code>.pth</code> file extension.</p>
<p>Remember that you must call <code>model.eval()</code> to set dropout
and batch normalization layers to evaluation mode before running
inference. Failing to do this will yield inconsistent inference
results.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2019/05/14/2019-05-15-Q811-Subdomain-Visit-Count/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/14/2019-05-15-Q811-Subdomain-Visit-Count/" class="post-title-link" itemprop="url">Q811 Subdomain Visit Count</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-14 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-14T00:00:00+08:00">2019-05-14</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="subdomain-visit-count"><a
target="_blank" rel="noopener" href="https://leetcode.com/problems/subdomain-visit-count/">Subdomain
Visit Count</a></h1>
<h2 id="question">Question</h2>
<blockquote>
<p>A website domain like "discuss.leetcode.com" consists of various
subdomains. At the top level, we have "com", at the next level, we have
"leetcode.com", and at the lowest level, "discuss.leetcode.com". When we
visit a domain like "discuss.leetcode.com", we will also visit the
parent domains "leetcode.com" and "com" implicitly.</p>
<p>Now, call a "count-paired domain" to be a count (representing the
number of visits this domain received), followed by a space, followed by
the address. An example of a count-paired domain might be "9001
discuss.leetcode.com".</p>
<p>We are given a list <code>cpdomains</code> of count-paired domains.
We would like a list of count-paired domains, (in the same format as the
input, and in any order), that explicitly counts the number of visits to
each subdomain.</p>
</blockquote>
<blockquote>
<p><strong>Example 1:</strong> Input: ["9001 discuss.leetcode.com"]
Output: ["9001 discuss.leetcode.com", "9001 leetcode.com", "9001 com"]
Explanation: We only have one website domain: "discuss.leetcode.com". As
discussed above, the subdomain "leetcode.com" and "com" will also be
visited. So they will all be visited 9001 times.</p>
</blockquote>
<blockquote>
<p><strong>Example 2:</strong> Input: ["900 google.mail.com", "50
yahoo.com", "1 intel.mail.com", "5 wiki.org"] Output: ["901
mail.com","50 yahoo.com","900 google.mail.com","5 wiki.org","5 org","1
intel.mail.com","951 com"] Explanation: We will visit "google.mail.com"
900 times, "yahoo.com" 50 times, "intel.mail.com" once and "wiki.org" 5
times. For the subdomains, we will visit "mail.com" 900 + 1 = 901 times,
"com" 900 + 50 + 1 = 951 times, and "org" 5 times.</p>
</blockquote>
<blockquote>
<p><strong>Note:</strong></p>
<ol type="1">
<li>You may assume k is always valid, 1 ≤ k ≤ number of unique
elements.</li>
<li>Your algorithm's time complexity must be better than O(n log n),
where n is the array's size.</li>
</ol>
</blockquote>
<h3 id="approach-1-hash-map">Approach 1: Hash map</h3>
<p><strong>Intuition and Algorithm</strong></p>
<p>The algorithm is straightforward: we just do what the problem
statement tells us to do.</p>
<p>For an address like <code>a.b.c</code>, we will count
<code>a.b.c</code>, <code>b.c</code>, and <code>c</code>. For an address
like <code>x.y</code>, we will count <code>x.y</code> and
<code>y</code>.</p>
<p>To count these strings, we will use a hash map. To split the strings
into the required pieces, we will use library <code>split</code>
functions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">subdomainVisits</span>(<span class="params">self, cpdomains</span>):</span><br><span class="line">        ans = collections.Counter()</span><br><span class="line">        <span class="keyword">for</span> domain <span class="keyword">in</span> cpdomains:</span><br><span class="line">            count, domain = domain.split() <span class="comment"># 用空格分割字符串</span></span><br><span class="line">            count = <span class="built_in">int</span>(count)</span><br><span class="line">            frags = domain.split(<span class="string">&#x27;.&#x27;</span>) <span class="comment"># 用点分割域名</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(frags)):</span><br><span class="line">                ans[<span class="string">&quot;.&quot;</span>.join(frags[i:])] += count <span class="comment"># join()连接字符数组 </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;&#123;&#125; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(ct, dom) <span class="keyword">for</span> dom, ct <span class="keyword">in</span> ans.items()]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    cpdomains =[<span class="string">&quot;9001 discuss.leetcode.com&quot;</span>]</span><br><span class="line">    answer=Solution()</span><br><span class="line">    results=answer.subdomainVisits(cpdomains)</span><br><span class="line">    <span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li>Runtime: 52 ms, faster than 99.76% of Python3 online submissions for
Subdomain Visit Count.</li>
<li>Memory Usage: 13.3 MB, less than 7.07% of Python3 online submissions
for Subdomain Visit Count.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2019/05/13/2019-05-13-Q347-Top-K-Frequent-Elements/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/13/2019-05-13-Q347-Top-K-Frequent-Elements/" class="post-title-link" itemprop="url">Q347 Top K Frequent Elements</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-13 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-13T00:00:00+08:00">2019-05-13</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="top-k-frequent-elements"><a
target="_blank" rel="noopener" href="https://leetcode.com/problems/top-k-frequent-elements/">Top K
Frequent Elements</a></h1>
<h2 id="question">Question</h2>
<blockquote>
<p>Given a non-empty array of integers, return the <strong>k</strong>
most frequent elements.</p>
</blockquote>
<blockquote>
<p><strong>Example 1:</strong></p>
<p>Input: nums = [1,1,1,2,2,3], k = 2 Output: [1,2]</p>
</blockquote>
<blockquote>
<p><strong>Example 2:</strong></p>
<p>Input: nums = [1], k = 1 Output: [1]</p>
</blockquote>
<blockquote>
<p><strong>Note:</strong></p>
<ol type="1">
<li>You may assume k is always valid, 1 ≤ k ≤ number of unique
elements.</li>
<li>Your algorithm's time complexity must be better than O(n log n),
where n is the array's size.</li>
</ol>
</blockquote>
<h3 id="approach-1-heap">Approach 1: Heap</h3>
<p><strong>Intuition and Algorithm</strong></p>
<p>If <code>k = 1</code> the linear-time solution is quite simple. One
could keep the frequency of elements appearance in a hash map and update
the maximum element at each step.</p>
<p>When <code>k &gt; 1</code> we need a data structure that has a fast
access to the elements ordered by their frequencies. The idea here is to
use the heap which is also known as priority queue.</p>
<p>The first step is to build a hash map
<code>element -&gt; its frequency</code>. In Java we could use data
structure <code>HashMap</code> but have to fill it manually. Python
provides us both a dictionary structure for the hash map and a method
<code>Counter</code> in the <code>collections</code> library to build
the hash map we need. This step takes <span
class="math display">\[\mathcal{O}(N)\]</span> time where <code>N</code>
is number of elements in the list.</p>
<p>The second step is to build a heap. The time complexity of adding an
element in a heap is <span
class="math display">\[\mathcal{O}(\log(k))\]</span> and we do it
<code>N</code> times that means <span
class="math display">\[\mathcal{O}(N \log(k))\]</span> time complexity
for this step.</p>
<p>The last step to build an output list has <span
class="math display">\[\mathcal{O}(k \log(k))\]</span> time
complexity.</p>
<p>In Python there is a method <code>nlargest</code> in
<code>heapq</code> library (<a
target="_blank" rel="noopener" href="https://hg.python.org/cpython/file/2.7/Lib/heapq.py#l203">check
here the source code</a>) which has the same <span
class="math display">\[\mathcal{O}(k \log(k))​\]</span> time complexity
and combines two last steps in one line.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">topKFrequent</span>(<span class="params">self, nums, k</span>):</span><br><span class="line">        count = collections.Counter(nums)</span><br><span class="line">        <span class="keyword">return</span> heapq.nlargest(k, count.keys(), key=count.get)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    nums =[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">    k=<span class="number">2</span></span><br><span class="line">    answer=Solution()</span><br><span class="line">    results=answer.topKFrequent(nums,k)</span><br><span class="line">    <span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li>Runtime: 40 ms, faster than 99.60% of Python3 online submissions for
Top K Frequent Elements.</li>
<li>Memory Usage: 15.8 MB, less than 8.41% of Python3 online submissions
for Top K Frequent Elements.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2019/05/10/2019-05-10-Q575-Distribute-Candies/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/10/2019-05-10-Q575-Distribute-Candies/" class="post-title-link" itemprop="url">Q575 Distribute Candies</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-10 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-10T00:00:00+08:00">2019-05-10</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="distribute-candies"><a
target="_blank" rel="noopener" href="https://leetcode.com/problems/distribute-candies/">Distribute
Candies</a></h1>
<h2 id="question">Question</h2>
<blockquote>
<p>Given an integer array with <strong>even</strong> length, where
different numbers in this array represent different
<strong>kinds</strong> of candies. Each number means one candy of the
corresponding kind. You need to distribute these candies
<strong>equally</strong> in number to brother and sister. Return the
maximum number of <strong>kinds</strong> of candies the sister could
gain.</p>
</blockquote>
<blockquote>
<p><strong>Example 1:</strong></p>
<p>Input: candies = [1,1,2,2,3,3] Output: 3 Explanation: There are three
different kinds of candies (1, 2 and 3), and two candies for each kind.
Optimal distribution: The sister has candies [1,2,3] and the brother has
candies [1,2,3], too. The sister has three different kinds of
candies.</p>
</blockquote>
<blockquote>
<p><strong>Example 2:</strong></p>
<p>Input: candies = [1,1,2,3] Output: 2 Explanation: For example, the
sister has candies [2,3] and the brother has candies [1,1]. The sister
has two different kinds of candies, the brother has only one kind of
candies.</p>
</blockquote>
<blockquote>
<p><strong>Note:</strong></p>
<ol type="1">
<li>The length of the given array is in range [2, 10,000], and will be
even.</li>
<li>The number in given array is in range [-100,000, 100,000].</li>
</ol>
</blockquote>
<h3 id="approach-1-hash-table">Approach 1: Hash Table</h3>
<p><strong>Intuition and Algorithm</strong></p>
<p>First, counting the number of kinds. Then, returning the number of
given kinds.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">distributeCandies</span>(<span class="params">self, candies</span>):</span><br><span class="line">        Hash=<span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(candies)):</span><br><span class="line">            Hash[candies[i]]=i</span><br><span class="line">        kinds=<span class="built_in">len</span>(Hash)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">min</span>(kinds,<span class="built_in">len</span>(candies)//<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    candies =[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    answer=Solution()</span><br><span class="line">    results=answer.distributeCandies(candies)</span><br><span class="line">    <span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li>Runtime: 136 ms, faster than 35.19% of Python3 online submissions
for Distribute Candies.</li>
<li>Memory Usage: 14.5 MB, less than 5.88% of Python3 online submissions
for Distribute Candies.</li>
</ul>
<hr />
<h3 id="approach-2-using-set">Approach 2: Using Set</h3>
<p><strong>Intuition and Algorithm</strong></p>
<p>Another way to find the number of unique elements is to traverse over
all the elements of the given candies array and keep on putting the
elements in a set. By the property of a set, it will contain only unique
elements. At the end, we can count the number of elements in the set,
given by, say count. The value to be returned will again be given by
<span class="math display">\[\text{min}(count, n/2)\]</span>. Here,
<span class="math display">\[n\]</span> refers to the size of the
candies array.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">distributeCandies</span>(<span class="params">self, candies</span>):</span><br><span class="line">        candyType = <span class="built_in">set</span>(candies)<span class="comment"># 统计糖果种类数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">min</span>(<span class="built_in">len</span>(candyType), <span class="built_in">len</span>(candies) //<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 如果种类数&gt;n/2，则最多给n/2种糖果</span></span><br><span class="line">    <span class="comment"># 如果种类数&lt;n/2，则给所有种类的糖果</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>Runtime: 104 ms, faster than <strong><em>99.13%</em></strong> of
Python3 online submissions for Distribute Candies.</p></li>
<li><p>Memory Usage: 14.8 MB, less than 5.88% of Python3 online
submissions for Distribute Candies.</p></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2019/05/09/2019-05-09-Q938-Range%20Sum%20of%20BST/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/09/2019-05-09-Q938-Range%20Sum%20of%20BST/" class="post-title-link" itemprop="url">Q938 Range Sum of BST</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-09 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-09T00:00:00+08:00">2019-05-09</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.3k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="range-sum-of-bst"><a
target="_blank" rel="noopener" href="https://leetcode.com/problems/range-sum-of-bst/">Range Sum of
BST</a></h1>
<h2 id="question">Question</h2>
<blockquote>
<p>Given the <code>root</code> node of a binary search tree, return the
sum of values of all nodes with value between <code>L</code> and
<code>R</code> (inclusive).</p>
<p>The binary search tree is guaranteed to have unique values.</p>
</blockquote>
<blockquote>
<p><strong>Example 1:</strong></p>
<p>Input: root = [10,5,15,3,7,null,18], L = 7, R = 15 Output: 32</p>
</blockquote>
<blockquote>
<p><strong>Example 2:</strong></p>
<p>Input: root = [10,5,15,3,7,13,18,1,null,6], L = 6, R = 10 Output:
23</p>
</blockquote>
<blockquote>
<p><strong>Note:</strong></p>
<ol type="1">
<li>The number of nodes in the tree is at most <code>10000</code>.</li>
<li>The final answer is guaranteed to be less than
<code>2^31</code>.</li>
</ol>
</blockquote>
<h3 id="approach-1-depth-first-search">Approach 1: Depth First
Search</h3>
<p><strong>Intuition and Algorithm</strong></p>
<p>We traverse the tree using a depth first search. If
<code>node.val</code> falls outside the range <code>[L, R]</code>, (for
example <code>node.val &lt; L</code>), then we know that only the right
branch could have nodes with value inside <code>[L, R]</code>.</p>
<p>We showcase two implementations - one using a recursive algorithm,
and one using an iterative one.</p>
<p><strong>Recursive Implementation</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rangeSumBST</span>(<span class="params">self, root, L, R</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node</span>):</span><br><span class="line">            <span class="keyword">if</span> node:</span><br><span class="line">                <span class="keyword">if</span> L &lt;= node.val &lt;= R:</span><br><span class="line">                    self.ans += node.val</span><br><span class="line">                <span class="keyword">if</span> L &lt; node.val:</span><br><span class="line">                    dfs(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.val &lt; R:</span><br><span class="line">                    dfs(node.right)</span><br><span class="line"></span><br><span class="line">        self.ans = <span class="number">0</span></span><br><span class="line">        dfs(root)</span><br><span class="line">        <span class="keyword">return</span> self.ans</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>Iterative Implementation</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rangeSumBST</span>(<span class="params">self, root, L, R</span>):</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        stack = [root]</span><br><span class="line">        <span class="keyword">while</span> stack:</span><br><span class="line">            node = stack.pop()</span><br><span class="line">            <span class="keyword">if</span> node:</span><br><span class="line">                <span class="keyword">if</span> L &lt;= node.val &lt;= R:</span><br><span class="line">                    ans += node.val</span><br><span class="line">                <span class="keyword">if</span> L &lt; node.val:</span><br><span class="line">                    stack.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.val &lt; R:</span><br><span class="line">                    stack.append(node.right)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2019/05/08/2019-05-08-Q739-Daily-Temperatures/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/08/2019-05-08-Q739-Daily-Temperatures/" class="post-title-link" itemprop="url">Q739 Daily Temperatures</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-08 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-08T00:00:00+08:00">2019-05-08</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="daily-temperatures"><a
target="_blank" rel="noopener" href="https://leetcode.com/problems/daily-temperatures/">Daily
Temperatures</a></h1>
<h2 id="question">Question</h2>
<blockquote>
<p>Given a list of daily temperatures <code>T</code>, return a list such
that, for each day in the input, tells you how many days you would have
to wait until a warmer temperature. If there is no future day for which
this is possible, put <code>0</code> instead.</p>
<p>For example, given the list of temperatures
<code>T = [73, 74, 75, 71, 69, 72, 76, 73]</code>, your output should be
<code>[1, 1, 4, 2, 1, 1, 0, 0]</code>.</p>
</blockquote>
<blockquote>
<p><strong>Note</strong>:</p>
<p><code>The length of</code>temperatures<code>will be in the range</code>[1,
30000]<code>. Each temperature will be an integer in the range</code>[30,
100]`.</p>
</blockquote>
<h3 id="approach-1-stack">Approach 1: Stack</h3>
<p><strong>Intuition</strong></p>
<p>Consider trying to find the next warmer occurrence at
<code>T[i]</code>. What information (about <code>T[j]</code> for
<code>j &gt; i</code>) must we remember?</p>
<p>Say we are trying to find <code>T[0]</code>. If we remembered
<code>T[10] = 50</code>, knowing <code>T[20] = 50</code> wouldn't help
us, as any <code>T[i]</code> that has its next warmer ocurrence at
<code>T[20]</code> would have it at <code>T[10]</code> instead. However,
<code>T[20] = 100</code> would help us, since if <code>T[0]</code> were
<code>80</code>, then <code>T[20]</code> might be its next warmest
occurrence, while <code>T[10]</code>couldn't.</p>
<p>Thus, we should remember a list of indices representing a strictly
increasing list of temperatures. For example, <code>[10, 20, 30]</code>
corresponding to temperatures <code>[50, 80, 100]</code>. When we get a
new temperature like <code>T[i] = 90</code>, we will have
<code>[5, 30]</code> as our list of indices (corresponding to
temperatures <code>[90, 100]</code>). The most basic structure that will
satisfy our requirements is a <em>stack</em>, where the top of the stack
is the first value in the list, and so on.</p>
<p><strong>Algorithm</strong></p>
<p>As in <em>Approach #1</em>, process indices <code>i</code> in
descending order. We'll keep a <code>stack</code> of indices such that
<code>T[stack[-1]] &lt; T[stack[-2]] &lt; ...</code>, where
<code>stack[-1]</code> is the top of the stack, <code>stack[-2]</code>
is second from the top, and so on; and where
<code>stack[-1] &gt; stack[-2] &gt; ...</code>; and we will maintain
this invariant as we process each temperature.</p>
<p>After, it is easy to know the next occurrence of a warmer
temperature: it's simply the top index in the stack.</p>
<p>Here is a worked example of the contents of the <code>stack</code> as
we work through <code>T = [73, 74, 75, 71, 69, 72, 76, 73]</code> in
reverse order, at the end of the loop (after we add <code>T[i]</code>).
For clarity, <code>stack</code> only contains indices <code>i</code>,
but we will write the value of <code>T[i]</code> beside it in brackets,
such as <code>0 (73)</code>.</p>
<ul>
<li>When <code>i = 7</code>, <code>stack = [7 (73)]</code>.
<code>ans[i] = 0</code>.</li>
<li>When <code>i = 6</code>, <code>stack = [6 (76)]</code>.
<code>ans[i] = 0</code>.</li>
<li>When <code>i = 5</code>, <code>stack = [5 (72), 6 (76)]</code>.
<code>ans[i] = 1</code>.</li>
<li>When <code>i = 4</code>,
<code>stack = [4 (69), 5 (72), 6 (76)]</code>.
<code>ans[i] = 1</code>.</li>
<li>When <code>i = 3</code>,
<code>stack = [3 (71), 5 (72), 6 (76)]</code>.
<code>ans[i] = 2</code>.</li>
<li>When <code>i = 2</code>, <code>stack = [2 (75), 6 (76)]</code>.
<code>ans[i] = 4</code>.</li>
<li>When <code>i = 1</code>,
<code>stack = [1 (74), 2 (75), 6 (76)]</code>.
<code>ans[i] = 1</code>.</li>
<li>When <code>i = 0</code>,
<code>stack = [0 (73), 1 (74), 2 (75), 6 (76)]</code>.
<code>ans[i] = 1</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dailyTemperatures</span>(<span class="params">self, T</span>):</span><br><span class="line">        ans = [<span class="number">0</span>] * <span class="built_in">len</span>(T)</span><br><span class="line">        stack = [] <span class="comment">#从大到小的序号</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(T)-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):<span class="comment"># 从最后一个元素开始</span></span><br><span class="line">            <span class="keyword">while</span> stack <span class="keyword">and</span> T[i] &gt;= T[stack[-<span class="number">1</span>]]:</span><br><span class="line">                stack.pop()<span class="comment"># 移除最后一个元素</span></span><br><span class="line">            <span class="keyword">if</span> stack:</span><br><span class="line">                ans[i] = stack[-<span class="number">1</span>] - i</span><br><span class="line">            stack.append(i)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    A =[<span class="number">73</span>, <span class="number">74</span>, <span class="number">75</span>, <span class="number">71</span>, <span class="number">69</span>, <span class="number">72</span>, <span class="number">76</span>, <span class="number">73</span>]<span class="comment"># 73, 74, 75, 71, 69, 72, 76, 73</span></span><br><span class="line">    answer=Solution()</span><br><span class="line">    results=answer.dailyTemperatures(A)</span><br><span class="line">    <span class="built_in">print</span>(results) <span class="comment"># [1, 1, 4, 2, 1, 1, 0, 0]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Runtime: 308 ms, faster than 70.55% of Python3 online submissions
for Daily Temperatures.</p></li>
<li><p>Memory Usage: 16.7 MB, less than 9.45% of Python3 online
submissions for Daily Temperatures.</p>
<p>Next challenges:</p></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/26/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><span class="page-number current">27</span><a class="page-number" href="/page/28/">28</a><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/28/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Txing"
      src="/images/my_photo.jpg">
  <p class="site-author-name" itemprop="name">Txing</p>
  <div class="site-description" itemprop="description">泛用类人决战型机器人博士</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">229</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/txing-casia" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;txing-casia" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.uomi.moe/" title="https:&#x2F;&#x2F;blog.uomi.moe" rel="noopener" target="_blank">驱逐舰患者</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://m.mepai.me/photographyer/u_5a68085ba15aa.html?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;m.mepai.me&#x2F;photographyer&#x2F;u_5a68085ba15aa.html?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">隐之-INF</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Txing</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">538k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">8:09</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
