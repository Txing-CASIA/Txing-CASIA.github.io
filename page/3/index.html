<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"txing-casia.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="泛用人形决战型机器人博士">
<meta property="og:type" content="website">
<meta property="og:title" content="Txing">
<meta property="og:url" content="https://txing-casia.github.io/page/3/index.html">
<meta property="og:site_name" content="Txing">
<meta property="og:description" content="泛用人形决战型机器人博士">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Txing">
<meta property="article:tag" content="Txing">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://txing-casia.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Txing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Txing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到 | 伽蓝之堂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/28/2022-09-28-Autonomous%20Driving%20-%20DL-IAPS%20and%20PJSO%20A%20Path%20Speed%20Decoupled%20Trajectory%20Optimization%20and%20its%20Application%20in%20Autonomous%20Driving/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/28/2022-09-28-Autonomous%20Driving%20-%20DL-IAPS%20and%20PJSO%20A%20Path%20Speed%20Decoupled%20Trajectory%20Optimization%20and%20its%20Application%20in%20Autonomous%20Driving/" class="post-title-link" itemprop="url">Autonomous Driving | DL-IAPS and PJSO: A Path/Speed Decoupled Trajectory Optimization and its Application in Autonomous Driving (Baidu, 2020)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-28T00:00:00+08:00">2022-09-28</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>整体而言就是在混合A*的基础上，加上了轨迹平滑、碰撞检测、速度规划三个后续模块，在parking任务上取得了一定效果，并集成与Apollo平台中</p>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/23/2022-09-23-Reinforcement%20Learning%20-%20[u]%20Human-level%20Atari%20200x%20faster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/23/2022-09-23-Reinforcement%20Learning%20-%20%5Bu%5D%20Human-level%20Atari%20200x%20faster/" class="post-title-link" itemprop="url">Reinforcement Learning | Human-level Atari 200x faster</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-23 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-23T00:00:00+08:00">2022-09-23</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>595</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Deepmind的Agent57是第一个在57款Atari游戏上全面超过人类水平的强化学习智能体。但是Agent57的数据利用效率很低，要求80亿帧数据。本文通过设置不同的策略集合实现了200倍的训练效率提高。</p>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/20/2022-09-20-Autonomous%20Driving%20-%20BEVFormer%20Learning%20Bird%E2%80%99s-Eye-View%20Representation%20from%20Multi-Camera%20Images%20via%20Spatiotemporal%20Transformers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/20/2022-09-20-Autonomous%20Driving%20-%20BEVFormer%20Learning%20Bird%E2%80%99s-Eye-View%20Representation%20from%20Multi-Camera%20Images%20via%20Spatiotemporal%20Transformers/" class="post-title-link" itemprop="url">Autonomous Driving | BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-20 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-20T00:00:00+08:00">2022-09-20</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>多相机3D检测的工作。提出了spatial cross-attention整合空间信息；提出了temporal self-attention整合历史BEV信息。</p>
          <!--noindex-->
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/09/05/2022-09-05%20-%20Python%E6%8A%A5%E9%94%99libGL%20error%20MESA-LOADER%20failed%20to%20open%20iris/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/05/2022-09-05%20-%20Python%E6%8A%A5%E9%94%99libGL%20error%20MESA-LOADER%20failed%20to%20open%20iris/" class="post-title-link" itemprop="url">Python报错libGL error: MESA-LOADER: failed to open iris</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-05 00:00:00" itemprop="dateCreated datePublished" datetime="2022-09-05T00:00:00+08:00">2022-09-05</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="python报错libgl-error-mesa-loader-failed-to-open-iris">Python报错libGL
error: MESA-LOADER: failed to open iris</h2>
<h3 id="问题背景">1 问题背景</h3>
<p>执行gym-like环境代码时，渲染动画输出时报错无法找到系统显卡驱动（笔记本，核显，Ubuntu），报错信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">libGL error: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: iris</span><br><span class="line">libGL error: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: iris</span><br><span class="line">libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: swrast</span><br></pre></td></tr></table></figure>
<p>然而，根据提示，在<code>/usr/lib/x86_64-linux-gnu/dri</code>路径下是可以找<code>iris_dri.so</code>驱动的。</p>
<h3 id="处理办法">2 处理办法</h3>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/538877347">这篇博客</a>提供了一个有效解决方法：</p>
<p>Step 1: 建立一个 /usr/lib/dri/iris_dri.so 的软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:/usr/lib$ sudo mkdir dri</span><br><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:/usr/lib$ cd dri </span><br><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:/usr/lib/dri$ sudo ln -s /usr/lib/x86_64-linux-gnu/dri/iris_dri.so iris_dri.so</span><br></pre></td></tr></table></figure>
<p>Step 2: 再次python执行代码，报错变为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">libGL error: MESA-LOADER: failed to open iris: /home/pcon/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29&#x27; not found (required by /usr/lib/dri/iris_dri.so) (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: iris</span><br><span class="line">libGL error: MESA-LOADER: failed to open iris: /home/pcon/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29&#x27; not found (required by /usr/lib/dri/iris_dri.so) (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: iris</span><br><span class="line">libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$$&#123;ORIGIN&#125;/dri:/usr/lib/dri, suffix _dri)</span><br><span class="line">libGL error: failed to load driver: swrast</span><br></pre></td></tr></table></figure>
<p>这是由于 conda 里的 libstdcxx-ng 版本不够高造成的。</p>
<p>Step 3：执行以下命令查看GLIBCXX版本信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:~/anaconda3$ strings /home/pcon/anaconda3//lib/libstdc++.so.6 | grep GLIBCXX</span><br></pre></td></tr></table></figure>
<p>Step 4：升级 conda 里的 libstdcxx-ng (根据<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/538877347">博客</a>)
。（截至2022.09该版本号有效）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:~/anaconda3$ conda install libstdcxx-ng=12.1.0</span><br></pre></td></tr></table></figure>
<p>再次查看版本号</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) pcon@pcon-ThinkPad-L14-Gen-2:~/anaconda3$ strings /home/pcon/anaconda3//lib/libstdc++.so.6 | grep GLIBCXX</span><br></pre></td></tr></table></figure>
<p>执行Python代码，成功运行！</p>
<p>ref</p>
<p>https://zhuanlan.zhihu.com/p/538877347</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/25/2022-08-25-Autonomous%20Driving%20-%20MP3%20A%20Unified%20Model%20to%20Map%20Perceive%20Predict%20and%20Plan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/25/2022-08-25-Autonomous%20Driving%20-%20MP3%20A%20Unified%20Model%20to%20Map%20Perceive%20Predict%20and%20Plan/" class="post-title-link" itemprop="url">Autonomous Driving | MP3 A Unified Model to Map Perceive Predict and Plan (Uber ATG 2021)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-25 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-25T00:00:00+08:00">2022-08-25</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>8.5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>8 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="mp3-a-unified-model-to-map-perceive-predict-and-plan-uber-atg-2021">MP3:
A Unified Model to Map, Perceive, Predict and Plan (Uber ATG, 2021)</h2>
<ul>
<li>HD map具有的语义和几何信息使其成为自动驾驶系统的关键部件。但HD
map的成本很高，难扩展，尤其是厘米级精度（centimeter-level
accuracy）的情况下。因此能摆脱HD
Map（地图加载失败、地图老旧等）的算法值得研究。本文提出了一种<strong>end2end</strong>的<strong>不依赖地图</strong>的自动驾驶算法——MP3。</li>
<li>输入为：
<ul>
<li><strong>raw sensor data</strong></li>
<li><strong>high-level command</strong> (e.g., turn left at the
intersection)</li>
</ul></li>
<li>本文的定位为：mapless technology 的自动驾驶</li>
</ul>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>没有HD map的劣势：</p>
<ul>
<li><p>感知不能再依赖“人行道上的行人”、“道路上的车辆”这样的先验信息；</p></li>
<li><p>进行规划的空间变大了</p></li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220826-1.png"
alt="有地图和无地图对比" />
<figcaption aria-hidden="true">有地图和无地图对比</figcaption>
</figure>
<ul>
<li>车辆需要把到达抽象成为路口直行（going straight at an
intersection）、左转（turning left）和右转（turning
right）等高阶的行为指令。</li>
<li>大多数的无地图方法模仿专家的驾驶行为（朝向角、加速度），但是没有提供可解释的中间表征（intermediate
interpretable representations），而这可以帮助解释车辆的决策行为
<ul>
<li>End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316, 2016.</li>
<li>End-to-end driving via conditional imitation learning. In ICRA,
2018.</li>
<li>Urban driving with conditional imitation learning. arXiv preprint
arXiv:1912.00177, 2019.</li>
<li>Lift, splat, shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3d. In Proceedings of the European Conference
on Computer Vision, 2020.</li>
</ul></li>
<li>这些方法没有结构信息和先验知识，容易受到分布漂移（distributional
shift）的影响
<ul>
<li>A reduction of imitation learning and structured prediction to
no-regret online learning. In Proceedings of the fourteenth
international conference on artificial intelligence and statistics,
pages 627–635, 2011.</li>
</ul></li>
<li>一些使用在线地图的方法（获得道路边界、中心线），要么过度简单（假设了车道是平行的，但这只在高速场景适用），要么难以将静态环境的不确定性纳入运动规划，而运动规划对于降低风险至关重要。[2,
16, 18, 21, 37],
<ul>
<li>Deep multi-sensor lane detection. In IROS, pages 3102–3109. IEEE,
2018.</li>
<li>3d-lanenet: End-to-end 3d multiple lane detection. In Proceedings of
the IEEE International Conference on Computer Vision, pages 2921–2930,
2019.</li>
<li>Gen-lanenet: A generalized and scalable approach for 3d lane
detection. arXiv, pages arXiv–2003, 2020.</li>
<li>Hierarchical recurrent attention networks for structured online
maps. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3417–3426, 2018.</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<h4 id="online-mapping">2.1 Online Mapping:</h4>
<ul>
<li>特点：
<ul>
<li>satellite imagery (卫星图像)</li>
<li>gather dense information (采集车多次经过同一地方)</li>
<li>human-in-the-loop</li>
</ul></li>
<li>predicting map elements online:
<ul>
<li>3d-lanenet: End-to-end 3d multiple lane detection. In Proceedings of
the IEEE International Conference on Computer Vision, pages 2921–2930,
2019.</li>
<li>Gen-lanenet: A generalized and scalable approach for 3d lane
detection. arXiv, pages arXiv–2003, 2020.</li>
</ul></li>
</ul>
<h4 id="perception-and-prediction">2.2 Perception and Prediction</h4>
<ul>
<li><strong>生成轨迹集合</strong> generate a fixed set of trajectories
[6, 8–10, 26, 28, 30, 36, 56]</li>
<li><strong>画出样本特征分布</strong> draw samples to characterize the
distribution
<ul>
<li>Implicit latent variable model for scene-consistent motion
forecasting. arXiv preprint arXiv:2007.12036, 2020.</li>
<li>R2p2: A reparameterized pushforward policy for diverse, precise
generative path forecasting. In ECCV, 2018.</li>
<li>Multiple futures prediction. In Advances in Neural Information
Processing Systems, pages 15398–15408, 2019.</li>
</ul></li>
<li><strong>预测时间占用图</strong> predict temporal occupancy maps
<ul>
<li>Discrete residual flow for probabilistic pedestrian behavior
prediction. arXiv preprint arXiv:1910.08041, 2019.</li>
<li>The garden of forking paths: Towards multi-future trajectory
prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 10508–10518, 2020.</li>
<li>Scene compliant trajectory forecast with agent-centric
spatio-temporal grids. IEEE RA-L, 5(2):2816–2823, 2020.</li>
</ul></li>
<li>这些方法由于涉及了非最大抑制（non-maximum
suppression）和可信度阈值（confidence
thresholding），可能出现不安全的情况</li>
<li>occupancy grids:
<ul>
<li>Motionnet: Joint perception and motion prediction for autonomous
driving based on bird’s eye view maps. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages
11385–11395, 2020.</li>
<li>Learning occupancy grid maps with forward sensor models. Autonomous
robots, 15(2):111–127, 2003.</li>
<li><strong>Perceive, predict, and plan: Safe motion planning through
interpretable semantic representations.</strong> In Proceedings of the
European Conference on Computer Vision (ECCV), 2020.</li>
</ul></li>
</ul>
<h4 id="motion-planning">2.3 Motion Planning</h4>
<ul>
<li>从感知直接输出控制信号 （Driving policy transfer via modularity and
abstraction. arXiv preprint arXiv:1804.09364,
2018.）会面临稳定性和鲁棒性的问题（stability and robustness
issues）（Exploring the limitations of behavior cloning for autonomous
driving. In Proceedings of the IEEE International Conference on Computer
Vision, pages 9329–9338, 2019.）</li>
</ul>
<h3 id="interpretable-mapless-driving">3 Interpretable Mapless
Driving</h3>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220826-2.png"
alt="MP3 predicts probabilistic scene representations that are leveraged in motion planning as interpretable cost functions" />
<figcaption aria-hidden="true">MP3 predicts probabilistic scene
representations that are leveraged in motion planning as interpretable
cost functions</figcaption>
</figure>
<h4 id="extracting-geometric-and-semantic-features">3.1 Extracting
Geometric and Semantic Features</h4>
<ul>
<li>The result is a 3D tensor of size <span
class="math display">\[(\frac{H}{a},\frac{W}{a},\frac{Z}{a}\cdot T_p
)\]</span>,which is the input to our backbone network.</li>
<li>This network combines ideas from [9, 53] to extract geometric,
semantic and motion information about the scene.</li>
</ul>
<h4 id="interpretable-scene-representations">3.2 Interpretable Scene
Representations</h4>
<ul>
<li>道路先验信息和一些可解释的知识，使用 <code>online map</code>
表示</li>
<li>动态目标的位置、速度信息，使用 <code>dynamic occupancy field</code>
表示（the dynamic objects position and velocity into the future,
captured in our dynamic occupancy field）</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220826-3.png"
alt="Interpretable Scene representations" />
<figcaption aria-hidden="true">Interpretable Scene
representations</figcaption>
</figure>
<ul>
<li>具体而言，两种表征信息包括：</li>
</ul>
<p><strong>Online map representation:</strong></p>
<ul>
<li>Drivable area：以道路边缘为界的可行驶区域；</li>
<li>Reachable
lanes：可用车道是SDV在不违反任何交通规则的情况下可以到达的运动路径的子集。规划轨迹时，我们希望SDV靠近这些可到达的车道，并按照它们的方向行驶。因此，对于地平面中的每个像素，我们预测到最近的可到达车道中心线的无符号距离，在10米处截断，以及最近的可到达车道中心线分段的角度。</li>
<li>Intersection：被交通信号等或者交通标志控制的路段，需要根据信号灯或者标志按交通规定行驶；</li>
</ul>
<p><strong>Dynamic occupancy field:</strong></p>
<p>现有的行为预测算法包括不安全的离散决策unsafe discrete decisions such
as confidence thresholding and non-maximum suppression (NMS)</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220826-4.png"
alt="The motion field warps the occupancy over time" />
<figcaption aria-hidden="true">The motion field warps the occupancy over
time</figcaption>
</figure>
<ul>
<li>Initial Occupancy：一个BEV网格单元</li>
<li>Temporal Motion Field：a 2D BEV velocity vector (in m/s).</li>
<li><code>Note</code>：车辆、行人和自行车被视为单独的类别，每个类别都有自己的占用流。</li>
</ul>
<p><strong>Probabilistic Model:</strong></p>
<p>online Map 分为以下几个通道：</p>
<ul>
<li><p>可到达区域<span class="math display">\[M^A_i\]</span></p></li>
<li><p>路口<span class="math display">\[M^I_i\]</span></p></li>
<li><p>到最近车道线的距离。the direction of the closest lane centerline
in the reachable lanes <span
class="math display">\[M^{\theta}_i\]</span> as a Von Mises distribution
since it has support between <span
class="math display">\[[\pi,\pi]\]</span>.</p></li>
<li><p>可到达车道中线的截断距离变换为拉普拉斯算子。We model the
truncated distance transform to the reachable lanes centerline <span
class="math display">\[M^D_i\]</span>​ as a Laplacian, which we
empirically found to yield more accurate results than a
Gaussian</p></li>
</ul>
<p>建模动态物体的occupancy <span
class="math display">\[O^c\]</span>,为伯努利随机分布<span
class="math display">\[O^c_{t,i}\]</span>，<span
class="math display">\[c\in
\{行人，车辆，自行车\}\]</span>（考虑这些物体未来行为的多模态（直走或左转）和不确定性），用<span
class="math display">\[K_{t,i}\]</span>建模基于K个BEV运动向量<span
class="math display">\[V^c_{t,i,k}\]</span>的行为分类分布</p>
<p>the probability of future occupancy under our probabilistic model, we
first define the probability of occupancy flowing from location <span
class="math display">\[i_1\]</span> to location <span
class="math display">\[i_2\]</span> between two consecutive time steps t
and t + 1 as follows:</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220829-1.png"
alt="the probability of occupancy flowing" />
<figcaption aria-hidden="true">the probability of occupancy
flowing</figcaption>
</figure>
<h4 id="motion-planning-1">3.3 Motion Planning</h4>
<p>设计了一个基于采样的轨迹规划器，其根据运动学灵活的生成多种轨迹，然后使用一个learned
scoring function选择轨迹。</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220830-1.png"
alt="规划器的轨迹选择方式" />
<figcaption aria-hidden="true">规划器的轨迹选择方式</figcaption>
</figure>
<h5 id="trajectory-sampling">3.3.1 Trajectory Sampling</h5>
<p>Search-based optimal motion planning for automated driving. In IROS,
2018</p>
<ul>
<li>根据<span
class="math display">\[(v_x,a_x,k_x)\]</span>在专家轨迹数据集中检索专家轨迹，<span
class="math display">\[x\]</span>表示当前自车状态，检索出的轨迹有不同的初始速度和朝向。因此使用加速度和转向角来描述轨迹<span
class="math display">\[(a,\dot k)_t,t=0,...,T\]</span>，输入到a bicycle
model [38]生成具有连续速度和转向角的轨迹。
<ul>
<li>[38]. The kinematic bicycle model: A consistent model for planning
feasible trajectories for autonomous vehicles? In 2017 IEEE Intelligent
Vehicles Symposium (IV), pages 812–818. IEEE, 2017.</li>
</ul></li>
<li>文献[37]提供了一个忽略自车初始状态的简化的轨迹生成模型。
<ul>
<li>[37]. Lift, splat, shoot: Encoding images from arbitrary camera rigs
by implicitly unprojecting to 3d. In Proceedings of the European
Conference on Computer Vision, 2020.</li>
</ul></li>
</ul>
<h5 id="route-prediction">3.3.2 Route Prediction</h5>
<ul>
<li>由于无地图驾驶没有车道线follow，本文假设遵循command来行驶，指令<span
class="math display">\[c = (a, d)\]</span>, where <span
class="math display">\[a \in \{keep lane, turn left, turn
right\}\]</span> is a discrete high-level action, and <span
class="math display">\[d\]</span> an approximate longitudinal distance
to the
action（行为的纵向距离）（d经过”rasterize”处理），输入给CoordConv[29]
<ul>
<li>An intriguing failing of convolutional neural networks and the
coordconv solution. In Advances in Neural Information Processing
Systems, pages 9605–9616, 2018.</li>
</ul></li>
</ul>
<h5 id="trajectory-scoring">3.3.2 Trajectory Scoring</h5>
<ul>
<li>Routing and Driving on Roads:
该评分函数鼓励车辆行驶在概率图R中概率高的区域</li>
</ul>
<p><span class="math display">\[
f_r(\tau,R)=-m(\tau)\min_{i \in m(\tau)} R_i
\]</span></p>
<p>其中<span
class="math display">\[m(\tau)\]</span>是BEV图中和自车轨迹<span
class="math display">\[\tau\]</span>重合的格子单元（grid-cells that
overlap with SDV polygon in trajectory <span
class="math display">\[\tau\]</span>)</p>
<p>离开车道损失： <span class="math display">\[
f_a(x,M)=\max_{i \in m(x)}[1-P(M_i^A)]
\]</span></p>
<ul>
<li><p>Safety</p></li>
<li><p>Comfort</p>
<p>惩罚jerk和加速度</p></li>
</ul>
<h4 id="learning">3.4. Learning</h4>
<p>两阶段的训练。我们分两个阶段优化我们的驾驶模型。我们首先训练<strong>online
map</strong>、<strong>dynamic occupancy
field</strong>和<strong>routing</strong>。一旦这些被收敛，在第二阶段，我们保持这些部分冻结，并为得分函数的线性组合训练规划器权重。我们发现这种两阶段的培训比端到端的培训更稳定。（We
optimize our driving model in two stages. We first train the online map,
dynamic occupancy field, and routing. Once these are converged, in a
second stage, we keep these parts frozen and train the planner weights
for the linear combination of scoring functions. We found this 2-stage
training empirically more stable than training end-to-end.）</p>
<h3 id="experimental-evaluation">4. Experimental Evaluation</h3>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-1.png"
alt="性能指标" />
<figcaption aria-hidden="true">性能指标</figcaption>
</figure>
<ul>
<li>Imitation Learning (IL), where the future positions of the SDV are
predicted directly from the scene context features, and is trained using
L2 loss.</li>
<li>Conditional Imitation Learning (CIL) [11], which is similar to IL
but the trajectory is conditioned on the driving command.
<ul>
<li>End-to-end driving via conditional imitation learning. In ICRA,
2018.</li>
</ul></li>
<li>Neural Motion Planner (NMP) [55], where a planning cost-volume as
well as detection and prediction are predicted in a multi-task fashion
from the scene context features, and Trajectory Classification (TC)
[37], where a cost-volume is predicted similar to NMP, but the
trajectory cost is used to create a probability distribution over the
trajectories and is trained by optimizing for the likelihood of the
expert trajectory.
<ul>
<li>Lift, splat, shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3d. In Proceedings of the European Conference
on Computer Vision, 2020.</li>
<li>End-to-end interpretable neural motion planner. In CVPR, 2019.</li>
</ul></li>
<li>Finally, we extend NMP to consider the high-level command by
learning a separate costing network for each discrete action
(CNMP).</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-2.png"
alt="Backbone Network" />
<figcaption aria-hidden="true">Backbone Network</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-3.png"
alt="Mapping Network" />
<figcaption aria-hidden="true">Mapping Network</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-4.png"
alt="Perception and Prediction Network" />
<figcaption aria-hidden="true">Perception and Prediction
Network</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-5.png"
alt="Routing Network" />
<figcaption aria-hidden="true">Routing Network</figcaption>
</figure>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220901-6.png"
alt="Sets of trajectories retrieved from the expert demonstrations." />
<figcaption aria-hidden="true">Sets of trajectories retrieved from the
expert demonstrations.</figcaption>
</figure>
<p>后面还有大量实验情景的展示图。</p>
<h3 id="总结">总结</h3>
<p>比较有想法的一个工作，做得比较细致，但是介绍相对粗略，可以仔细研究。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/24/2022-08-24-Autonomous%20Driving%20-%20On%20the%20Choice%20of%20Data%20for%20Efficient%20Training%20and%20Validation%20of%20End-to-End%20Driving%20Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/24/2022-08-24-Autonomous%20Driving%20-%20On%20the%20Choice%20of%20Data%20for%20Efficient%20Training%20and%20Validation%20of%20End-to-End%20Driving%20Models/" class="post-title-link" itemprop="url">Autonomous Driving | On the Choice of Data for Efficient Training and Validation of End-to-End Driving Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-24 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-24T00:00:00+08:00">2022-08-24</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>7.4k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>7 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="on-the-choice-of-data-for-efficient-training-and-validation-of-end-to-end-driving-models">On
the Choice of Data for Efficient Training and Validation of End-to-End
Driving Models</h2>
<p>本文关注数据集的设计，包括针对自动驾驶端到端模型训练集和验证集。</p>
<p>主要工作：</p>
<ul>
<li>调研训练数据量如何影响最终驾驶表现，以及通过当前使用的生成训练数据的机制导致了哪些表现限制；</li>
<li>相关性分析表明，验证设计使得在验证期间测量的驾驶性能能够很好地推广到未知的测试环境；</li>
<li>调查了随机种子和不确定性的影响，给出了哪些报告的改进可以被认为是显著的；</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-1.png"
alt="本文关注数据而非模型" />
<figcaption aria-hidden="true">本文关注数据而非模型</figcaption>
</figure>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>Towards End-to-End Deep Driving:
高维场景信息输入，训练自动驾驶算法。</p></li>
<li><p>Training of Deep Driving
Models：端到端的自动驾驶模型的初衷是移除人工的中间表征。</p>
<ul>
<li>Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun,
and Alexey Dosovitskiy. <strong>End-to-end Driving via Conditional
Imitation Learning</strong>. In Proc. of ICRA, pages 4693–4700,
Brisbane, Australia, May 2018.</li>
<li>Kashyap Chitta, Aditya Prakash, and Andreas Geiger. NEAT:
<strong>Neural Attention Fields for End-to-End Autonomous
Driving</strong>. In Proc. of ICCV, pages 15793–15803, Virtual, Oct.
2021.</li>
<li>Keishi Ishihara, Anssi Kanervisto, Jun Miura, and Ville Hautamaki.
<strong>Multi-task Learning with Attention for End-to-end Autonomous
Driving</strong>. In Proc. of CVPR - Workshops, pages 2902–2911,
Virtual, June 2021</li>
</ul></li>
<li><p>Evaluation of Deep Driving
Models：可以开环和offline专家对比，也可以闭环仿真评估，但是评估是非确定性的，不会执行两次相同的评估。</p>
<ul>
<li>Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele
Reda, Nikolay Nikolov, Przemysław Mazur, Sean Micklethwaite, Nicolas
Griffiths, Amar Shah, and Alex Kendall. <strong>Urban Driving with
Conditional Imitation Learning</strong>. In Proc. of ICRA, pages
251–257, Virtual, May 2020.</li>
</ul></li>
<li><p>Contributions:</p>
<ul>
<li>调研了end2end模型的训练集大小对性能的影响</li>
<li>分析了end2end模型的局限性</li>
<li>为驾驶模型的验证集设计提供了建议</li>
<li>调研了random seed和非确定性end2end模型</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<h4 id="training-of-end-to-end-deep-driving-models">Training of
End-to-End Deep Driving Models</h4>
<ul>
<li><p>Reinforcement Learning</p>
<ul>
<li>Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele
Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah.
<strong>Learning to Drive in a Day</strong>. In Proc. of ICRA, pages
8248–8254, Montréal, Canada, May 2019.</li>
<li>Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.
<strong>End-to-End Model-Free Reinforcement Learningfor Urban Driving
using Implicit Affordances</strong>. In Proc. of CVPR, pages 7153–7162,
Virtual, June 2020.</li>
<li>Dian Chen, Vladlen Koltun, and Philipp Krähenbühl. <strong>Learning
to Drive from a World on Rails</strong>. In Proc. of ICCV, pages
15590–15599, Virtual, Oct. 2021</li>
</ul></li>
<li><p>two-stage training:
先训练一个专家，然后将知识传递给自动驾驶智能体</p>
<ul>
<li>Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krähenbühl.
<strong>Learning by Cheating</strong>. In Proc. of CoRL, pages 66–75,
Virtual, Nov. 2020.</li>
<li>Jiaming Zhang, Kailun Yang, Angela Constantinescu, Kunyu Peng, Karin
Müller, and Rainer Stiefelhagen. <strong>Trans4Trans: Efficient
Transformer for Transparent Object Segmentation To Help Visually
Impaired People Navigate in the Real World</strong>. In Proc. of ICCV -
Workshops, pages 1760–1770, Virtual, Oct. 2021</li>
</ul></li>
<li><p>inverse reinforcement learning</p>
<ul>
<li>Sascha Rosbach, Vinit James, Simon Großjohann, Silviu Homoceanu, and
Stefan Roth. <strong>Driving with Style: Inverse Reinforcement Learning
in General-PurposePlanning for Automated Driving</strong>. In Proc. of
IROS, pages 2658–2665, Macau, China, Nov. 2019.</li>
<li>Sahand Sharifzadeh, Ioannis Chiotellis, Rudolph Triebel, and Daniel
Cremers. <strong>Learning to Drive using Inverse Reinforcement Learning
and Deep Q-Networks</strong>. In Proc. of NIPS Workshops, pages 1–7,
Barcelona, Spain, Dec. 2016.</li>
</ul></li>
<li><p>LSTM</p>
<ul>
<li>Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell.
<strong>End-to-end Learning of Driving Models from Large-scale Video
Datasets</strong>. In Proc. of CVPR, pages 2174–2182, Honolulu, HI, USA,
July 2017.</li>
</ul></li>
<li><p>self-attention</p>
<ul>
<li>Keishi Ishihara, Anssi Kanervisto, Jun Miura, and Ville Hautamaki.
<strong>Multi-task Learning with Attention for End-to-end Autonomous
Driving</strong>. In Proc. of CVPR - Workshops, pages 2902–2911,
Virtual, June 2021.</li>
</ul></li>
<li><p>multi-task networks</p>
<ul>
<li>Dan Wang, Junjie Wen, Yuyong Wang, Xiangdong Huang, and Feng Pei.
<strong>End-to-End Self-Driving Using Deep Neural Networks with
Multi-auxiliary Tasks</strong>. Automotive Innovation, 2(2):127–136, May
2019.</li>
<li>Zhengyuan Yang, Yixuan Zhang, Jerry Yu, Junjie Cai, and Jiebo Luo.
<strong>End-to-end Multi-Modal Multi-Task Vehicle Controlfor
Self-Driving Cars with Visual Perceptions</strong>. In Proc. of ICPR,
pages 2289–2294, Beijing, China, Aug. 2018.</li>
</ul></li>
<li><p>The fusion of different input modalities</p>
<ul>
<li>Aditya Prakash, Kashyap Chitta, and Andreas Geiger.
<strong>Multi-Modal Fusion Transformer for End-to-End Autonomous
Driving</strong>. In Proc. of CVPR, pages 7077–7087, Virtual, June
2021.</li>
</ul></li>
<li><p>affordances</p>
<ul>
<li>Axel Sauer, Nikolay Savinov, and Andreas Geiger. <strong>Conditional
Affordance Learning for Driving in Urban Environments</strong>. In Proc.
of CoRL, pages 237–252, Zürich, Switzerland, Oct. 2018.</li>
</ul></li>
<li><p>waypoints 替代速度</p></li>
<li><p>Kashyap Chitta, Aditya Prakash, and Andreas Geiger. NEAT:
<strong>Neural Attention Fields for End-to-End Autonomous
Driving</strong>. In Proc. of ICCV, pages 15793–15803, Virtual, Oct.
2021.</p></li>
<li><p>probabilistic output</p>
<ul>
<li>Alexander Amini, Guy Rosman, Sertac Karaman, and Daniela Rus.
<strong>Variational End-to-End Navigation and Localization</strong>. In
Proc. of ICRA, pages 8958–8964, Montréal, QC, Canada, May 2019.</li>
</ul></li>
<li><p>sim-2-real</p>
<ul>
<li>Blażej Osiński, Adam Jakubowski, Pawel Ziecina, Piotr Miloś,
Christopher Galias, Silviu Homoceanu, and Henryk Michalewski.
<strong>Simulation-Based Reinforcement Learningfor Real-World Autonomous
Driving</strong>. In Proc. of ICRA, pages 6411–6418, Virtual, May
2020</li>
<li>GAN-based
<ul>
<li>Matthias Müller, Alexey Dosovitskiy, Bernard Ghanem, and Vladlen
Koltun. <strong>Driving Policy Transfer via Modularity and
Abstraction</strong>. In Proc. of CoRL, pages 1–15, Zürich, Switzerland,
Oct. 2018.</li>
<li>Luona Yang, Xiaodan Liang, Tairui Wang, and Eric Xing.
<strong>Real-to-Virtual Domain Unification for End-to-</strong>
<strong>EndAutonomous Driving</strong>. In Proc. of ECCV, pages 530–545,
Munich, Germany, Sept. 2018.</li>
</ul></li>
</ul></li>
<li><p>attention</p>
<ul>
<li>Bob Wei, Mengye Ren, Wenyuan Zeng, Ming Liang, Bin Yang, and Raquel
Urtasun. <strong>Perceive, Attend, and Drive: Learning Spatial Attention
for Safe Self-Driving</strong>. In Proc. of ICRA, pages 4875–4881,
Virtual, May 2021.</li>
<li>Luca Cultrera, Lorenzo Seidenari, Federico Becattini, Pietro Pala,
and Alberto Del Bimbo. <strong>Explaining Autonomous Driving by Learning
End-to-End Visual Attention</strong>. In Proc. of CVPR - Workshops,
pages 340–341, Virtual, June 2020.</li>
</ul></li>
<li><p>intermediate semantic representation</p>
<ul>
<li>Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and
Raquel Urtasun. <strong>Perceive, Predict, and Plan: Safe Motion
Planning Through Interpretable Semantic Representations</strong>. In
Proc. of ECCV, pages 414–430, Virtual, Aug. 2020.</li>
</ul></li>
<li><p>on-line data selection techniques (视觉输入情况)</p>
<ul>
<li>Aditya Prakash, Aseem Behl, Eshed Ohn-Bar, Kashyap Chitta, and
Andreas Geiger. <strong>Exploring Data Aggregation in Policy Learning
for Vision-based Urban Autonomous Driving</strong>. In Proc. of CVPR,
pages 11763–11773, Virtual, June 2020.</li>
<li>Soumi Das, Harikrishna Patibandla, Suparna Bhattacharya, Kshounis
Bera, Niloy Ganguly, and Sourangshu Bhat tacharya. <strong>TMCOSS:
Thresholded Multi-Criteria Online Subset Selection forData-Efficient
Autonomous Driving</strong>. In Proc. of ICCV, pages 6341–6350, Virtual,
Oct. 2021. 2</li>
</ul></li>
</ul>
<h4 id="evaluation-of-end-to-end-deep-driving-models">Evaluation of
End-to-End Deep Driving Models</h4>
<ul>
<li>CARLA benchmarks CoRL2017 [20]
<ul>
<li>Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and
Vladlen Koltun. CARLA: An Open Urban Driving Simulator. In Proc. of
CoRL, pages 1–16, Mountain View, CA, USA, Nov. 2017.</li>
</ul></li>
<li>NoCrash [17]
<ul>
<li>Felipe Codevilla, Eder Santana, Antonio M. López, and Adrien Gaidon.
Exploring the Limitations of Behavior Cloning for Autonomous Driving. In
Proc. of ICCV, pages 9329–9338, Seoul, Korea, Oct. 2019.</li>
</ul></li>
<li>Leaderboard [1]
<ul>
<li>CARLA Autonomous Driving Leaderboard. https://leaderboard.carla.org,
2020. 3, 5, 6</li>
</ul></li>
</ul>
<h3 id="end-to-end-deep-driving">3 End-to-End Deep Driving</h3>
<ul>
<li><p>文章设置了一个城市驾驶环境的点到点导航任务，预先定义一个稀疏点的路线为粗略的行驶路线，车辆需要避免违法和碰撞，从初始点驾驶到终止点（CARLA
Leaderboard标准配置）。</p></li>
<li><p>输入：一个RGB前置相机图像、一个LiDAR点云</p></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-2.png"
alt="End-to-end driving method" />
<figcaption aria-hidden="true">End-to-end driving method</figcaption>
</figure>
<ul>
<li><p>使用Conditional Imitation Learning</p></li>
<li><p>不同训练集设置情况：</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-3.png"
alt="Training set design" />
<figcaption aria-hidden="true">Training set design</figcaption>
</figure></li>
<li><p>验证集和测试集路线类型</p></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-4.png"
alt="Validation and test route types" />
<figcaption aria-hidden="true">Validation and test route
types</figcaption>
</figure>
<ul>
<li>不同验证集和测试集设置情况</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-5.png"
alt="Validation and test design" />
<figcaption aria-hidden="true">Validation and test design</figcaption>
</figure>
<ul>
<li><p>不同数据集上的驾驶得分<span
class="math display">\[DS\in[0,1]\]</span>，包括两部分分数：</p>
<ul>
<li><p>完成路线 route completion percentage <span
class="math display">\[RC\in [0, 1]\]</span>;</p></li>
<li><p>避免事故 infraction score <span class="math display">\[IS \in [0,
1]\]</span>;</p></li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-6.png"
alt="Driving scores" />
<figcaption aria-hidden="true">Driving scores</figcaption>
</figure>
<ul>
<li>不同训练集和验证集的性能对比</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-7.png"
alt="Pearson correlation between the validation set performance and the test set performance (given by the driving score)" />
<figcaption aria-hidden="true">Pearson correlation between the
validation set performance and the test set performance (given by the
driving score)</figcaption>
</figure>
<ul>
<li>不同验证集下的测试集得分对比</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-8.png"
alt="Test driving scores given in (%) obtained on R^test, having used different validation sets" />
<figcaption aria-hidden="true">Test driving scores given in (%) obtained
on R^test, having used different validation sets</figcaption>
</figure>
<ul>
<li>不同训练集下的性能对比</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-9.png"
alt="Performance on different training sets" />
<figcaption aria-hidden="true">Performance on different training
sets</figcaption>
</figure>
<ul>
<li><p>结论（不完整）：</p>
<ul>
<li><p>随机性对仿真实验的影响大。随机种子、carla等。</p></li>
<li><p>训练集和验证集的选择对结果影响大。</p></li>
<li><p>数据量从100k-220k变化时，对大约160，000幅图像的训练似乎已经在性能和复杂性之间提供了良好的平衡，同时使用更大但计算上更昂贵的数据量可以获得最佳结果。（Training
on approximately 160, 000 images already seems to provide a good
trade-off between performance and complexity, while best results are
obtained using larger but computationally more expensive amounts of
data.）。</p></li>
<li><p>数据越少路线完成率越高，但事故率也高；数据越多，事故率显著降低，但路线完成率也降低；</p></li>
<li><p>专家数据是必要的。不完美的专家数据对结果影响大</p></li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<p>总的来说比较玄学，对于视觉输入的模型而言，数据量偏小（100k-220k），给出的经验参数不具有普遍意义。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/23/2022-08-23-Autonomous%20Driving%20-%20%E5%85%B3%E4%BA%8E%E5%8A%A0%E5%87%8F%E9%80%9F%E5%8F%98%E9%81%93%E7%9A%84%E6%80%9D%E8%80%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/23/2022-08-23-Autonomous%20Driving%20-%20%E5%85%B3%E4%BA%8E%E5%8A%A0%E5%87%8F%E9%80%9F%E5%8F%98%E9%81%93%E7%9A%84%E6%80%9D%E8%80%83/" class="post-title-link" itemprop="url">Autonomous Driving | 关于加减速变道的思考</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-23 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-23T00:00:00+08:00">2022-08-23</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>515</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="关于加减速变道的思考">关于加减速变道的思考</h2>
<h3 id="为什么要加减速变道"><strong>1. 为什么要加减速变道</strong></h3>
<p>车端的基于规则的变道算法已经实现了常规情况下的变道过程，但目前基于规则的算法都是匀速变道，对于需要加减速的较复杂的变道场景并不能自适应。因此需要一种既能应对常规简单变道情况，又能灵活适应车流密集、需要的较大幅度变速的变道算法模型。</p>
<h3 id="加减速变道并不是变道场景中的特例">2.
<strong>加减速变道并不是变道场景中的特例</strong></h3>
<p>在行车过程中，根据实际路况灵活地加减速度是一项融合于几乎所有驾驶场景中的基本驾驶技能，不应当被视为一种只在特定场景使用的技能。因此，不适合单独识别出一个类别用于概括加减速变道情况。</p>
<h3 id="车辆加减速的目的">3. 车辆加减速的目的</h3>
<p>在复杂路况中，理想便道路径上存在障碍物，或者路径上存在潜在不安全因素，具有一定风险。为避免碰撞、规避风险，使用加减速策略灵活调整路线，实现安全、合规的自动驾驶。</p>
<h3 id="如何使得agent获得变速变道能力">4.
如何使得Agent获得变速变道能力</h3>
<p>既然加减速并非独立的技能，因此只需在正常的驾驶规划训练中学习即可。由于加减速变道的目的在于规避碰撞等风险，因此需要考虑设置碰撞相关的损失函数。另一方面，模仿学习中的专家数据中如果他车、障碍物比较少，可以尝试生成一些障碍物，增加规划的避障难度。</p>
<p>因此，总的来说，有两个方案可以提高agent变速变道能力：<code>碰撞loss</code>
和 <code>障碍生成</code>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/20/2022-08-20-Autonomous%20Driving%20-%20Off-Policy%20Deep%20Reinforcement%20Learning%20without%20Exploration/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用人形决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/20/2022-08-20-Autonomous%20Driving%20-%20Off-Policy%20Deep%20Reinforcement%20Learning%20without%20Exploration/" class="post-title-link" itemprop="url">Autonomous Driving | Off-Policy Deep Reinforcement Learning without Exploration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-20 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-20T00:00:00+08:00">2022-08-20</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="off-policy-deep-reinforcement-learning-without-exploration-2019">Off-Policy
Deep Reinforcement Learning without Exploration (2019)</h2>
<p>类似DQN和DDPG的off-policy
RL算法在被禁止探索，并在没有数据策略分布修正的的情况下，难以取得好的效果。本文通过限制off-policy
agent的行为空间，使其行为类似与on-policy算法，最后提出了一个较为通用的，针对连续控制的deep
reinforcement learning algorithm。</p>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p><strong>batch reinforcement
learning</strong>：在一些数据收集面临costly, risky, or
time-consuming的情境时，智能体只能从固定的数据集中学习策略，并且其对数据质量的要求较低。</p></li>
<li><p>当数据集中的分布与当前算法策略的分布不同时，标准的off-policy
RL算法将会失败。</p></li>
<li><p>extrapolation
error：未见过的状态-动作对被错误地估计为具有不现实的价值的现象。</p></li>
<li><p>引入了batch-constrained reinforcement learning来克服extrapolation
error，最大化奖励的同时，最小化批量数据中状态-行为对和策略访问的状态行为对的误匹配。</p></li>
<li><p>本文提出算法：<strong>Batch-Constrained deep Q-learning</strong>
(BCQ)，利用一个与Q-network结合的条件状态生成模型，只生成过去见过的行为。在较弱的假设下，证明了这种批约束范式对于有限确定MDP的不完全数据集的无偏估计是必要的。</p></li>
<li><p>代码开源：https://github.com/sfujim/BCQ</p></li>
</ul>
<h3 id="background">2 Background</h3>
<ul>
<li>贝尔曼算子（Bellman operator）<span
class="math display">\[\Tau^{\pi}\]</span>： <span
class="math display">\[
\Tau^{\pi}Q(s,a)=\mathbb{E}_{s&#39;}[r+\gamma Q(s&#39;,\pi(s&#39;))]
\]</span></li>
</ul>
<h3 id="extrapolation-error-外推误差">3 Extrapolation Error
(外推误差)</h3>
<ul>
<li><p>Extrapolation error is an error in off-policy value learning
which is introduced by the mismatch between the dataset and true
state-action visitation of the current policy，或者<span
class="math display">\[(s,a)\]</span>在数据集中不存在</p></li>
<li><p>外推误差的成因：</p>
<ul>
<li><p><strong>数据缺省</strong>（Absent Data）：state-action pair (s,
a) is unavailable，因此在估计状态-行为价值Q的时候会引入误差。</p></li>
<li><p><strong>模型偏置</strong>（Model Bias）：when performing
off-policy Q-learning with a batch B，状态转移动力学的有偏估计为：</p>
<p><span class="math display">\[\Tau^{\pi}Q(s,a) \approx
\mathbb{E}_{s&#39;\sim B}[r+\gamma Q(s&#39;,\pi(s&#39;))]\]</span>
其中，状态转移依据buffer B，而不是真实的MDP。</p></li>
<li><p><strong>训练误匹配</strong>（Training
Mismatch）：当数据的分布和当前策略的分布不匹配，对action的估计会有误差</p></li>
</ul></li>
<li><p>As a result, learning a value estimate with off-policy data can
result in large amounts of extrapolation error if the policy selects
actions which are not similar to the data found in the batch.</p></li>
</ul>
<h4 id="extrapolation-error-in-deep-reinforcement-learning">3.1
Extrapolation Error in Deep Reinforcement Learning</h4>
<ul>
<li>本节使用SOTA Actor-Critic off-policy RL
算法DDPG，在与策略无关的数据集上学习，观察到性能迅速恶化。</li>
<li>These results suggest that off-policy deep reinforcement learning
algorithms are ineffective when learning truly off-policy.</li>
<li>训练环境：OpenAI gym’s Hopper-v1 environment</li>
<li>train an off-policy DDPG agent with no interaction with the
environment.</li>
<li>三个batch：
<ul>
<li><strong>Batch 1 (Final buffer)</strong>：1 million time
steps，行为加上（0,0.5）的高斯噪声，store all experienced
transitions。</li>
<li><strong>Batch 2 (Concurrent)</strong>：训练behavioral DDPG agents，1
million time
steps，行为加上（0,0.1）的高斯噪声，每一次转移经验都放入buffer，即该情况下每个behavioral和off-policy智能体使用同一的数据集训练。</li>
<li><strong>Batch 3 (Imitation)</strong>：使用一个完全训练的DDPG，收集1
million time steps数据作为专家数据。</li>
</ul></li>
<li>实验中的off-policy DDPG完全使用离线数据训练，behavioral
DDPG与环境交互正常训练。</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220820-1.png"
alt="Figure 1" />
<figcaption aria-hidden="true">Figure 1</figcaption>
</figure>
<p>Even in the concurrent experiment, where both agents are trained with
the same dataset, there is a large gap in performance in every single
trial.</p>
<ul>
<li>结论：
<ul>
<li>Batch 1中，即使有了充足的探索，agent依然难以具有稳定的value
estimation</li>
<li>Batch
2中，即使使用相同的数据训练，agent之间的差异巨大。说明policy初始化的不同足以引入外推误差。</li>
<li>Batch
3中，尽管有了专家数据，agent仍快速学习非专家策略，最终导致效果很差。</li>
</ul></li>
<li>外推误差提供了一个噪声源，可导致持续高估偏差。extrapolation error
provides a source of noise that can induce a persistent overestimation
bias (Thrun &amp; Schwartz, 1993; Van Hasselt et al., 2016; Fujimoto et
al.,2018).</li>
<li>在完全off-policy情况下，外推误差无法通过与环境交互获得新数据来消除</li>
</ul>
<h3 id="batch-constrained-reinforcement-learning">4 Batch-Constrained
Reinforcement Learning</h3>
<h3 id="总结">总结</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/30/">30</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Txing"
      src="/images/my_photo.jpg">
  <p class="site-author-name" itemprop="name">Txing</p>
  <div class="site-description" itemprop="description">泛用人形决战型机器人博士</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">234</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/txing-casia" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;txing-casia" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.uomi.moe/" title="https:&#x2F;&#x2F;blog.uomi.moe" rel="noopener" target="_blank">驱逐舰患者</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://m.mepai.me/photographyer/u_5a68085ba15aa.html?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;m.mepai.me&#x2F;photographyer&#x2F;u_5a68085ba15aa.html?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">隐之-INF</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Txing</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">572k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">8:40</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
