<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"txing-casia.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="泛用类人决战型机器人博士">
<meta property="og:type" content="website">
<meta property="og:title" content="Txing">
<meta property="og:url" content="https://txing-casia.github.io/page/3/index.html">
<meta property="og:site_name" content="Txing">
<meta property="og:description" content="泛用类人决战型机器人博士">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Txing">
<meta property="article:tag" content="Txing">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://txing-casia.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Txing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Txing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到 | 伽蓝之堂</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/24/2022-08-24-Autonomous%20Driving%20-%20On%20the%20Choice%20of%20Data%20for%20Efficient%20Training%20and%20Validation%20of%20End-to-End%20Driving%20Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/24/2022-08-24-Autonomous%20Driving%20-%20On%20the%20Choice%20of%20Data%20for%20Efficient%20Training%20and%20Validation%20of%20End-to-End%20Driving%20Models/" class="post-title-link" itemprop="url">Autonomous Driving | On the Choice of Data for Efficient Training and Validation of End-to-End Driving Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-24 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-24T00:00:00+08:00">2022-08-24</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>7.4k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>7 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="on-the-choice-of-data-for-efficient-training-and-validation-of-end-to-end-driving-models">On
the Choice of Data for Efficient Training and Validation of End-to-End
Driving Models</h2>
<p>本文关注数据集的设计，包括针对自动驾驶端到端模型训练集和验证集。</p>
<p>主要工作：</p>
<ul>
<li>调研训练数据量如何影响最终驾驶表现，以及通过当前使用的生成训练数据的机制导致了哪些表现限制；</li>
<li>相关性分析表明，验证设计使得在验证期间测量的驾驶性能能够很好地推广到未知的测试环境；</li>
<li>调查了随机种子和不确定性的影响，给出了哪些报告的改进可以被认为是显著的；</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-1.png"
alt="本文关注数据而非模型" />
<figcaption aria-hidden="true">本文关注数据而非模型</figcaption>
</figure>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>Towards End-to-End Deep Driving:
高维场景信息输入，训练自动驾驶算法。</p></li>
<li><p>Training of Deep Driving
Models：端到端的自动驾驶模型的初衷是移除人工的中间表征。</p>
<ul>
<li>Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun,
and Alexey Dosovitskiy. <strong>End-to-end Driving via Conditional
Imitation Learning</strong>. In Proc. of ICRA, pages 4693–4700,
Brisbane, Australia, May 2018.</li>
<li>Kashyap Chitta, Aditya Prakash, and Andreas Geiger. NEAT:
<strong>Neural Attention Fields for End-to-End Autonomous
Driving</strong>. In Proc. of ICCV, pages 15793–15803, Virtual, Oct.
2021.</li>
<li>Keishi Ishihara, Anssi Kanervisto, Jun Miura, and Ville Hautamaki.
<strong>Multi-task Learning with Attention for End-to-end Autonomous
Driving</strong>. In Proc. of CVPR - Workshops, pages 2902–2911,
Virtual, June 2021</li>
</ul></li>
<li><p>Evaluation of Deep Driving
Models：可以开环和offline专家对比，也可以闭环仿真评估，但是评估是非确定性的，不会执行两次相同的评估。</p>
<ul>
<li>Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele
Reda, Nikolay Nikolov, Przemysław Mazur, Sean Micklethwaite, Nicolas
Griffiths, Amar Shah, and Alex Kendall. <strong>Urban Driving with
Conditional Imitation Learning</strong>. In Proc. of ICRA, pages
251–257, Virtual, May 2020.</li>
</ul></li>
<li><p>Contributions:</p>
<ul>
<li>调研了end2end模型的训练集大小对性能的影响</li>
<li>分析了end2end模型的局限性</li>
<li>为驾驶模型的验证集设计提供了建议</li>
<li>调研了random seed和非确定性end2end模型</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<h4 id="training-of-end-to-end-deep-driving-models">Training of
End-to-End Deep Driving Models</h4>
<ul>
<li><p>Reinforcement Learning</p>
<ul>
<li>Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele
Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah.
<strong>Learning to Drive in a Day</strong>. In Proc. of ICRA, pages
8248–8254, Montréal, Canada, May 2019.</li>
<li>Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.
<strong>End-to-End Model-Free Reinforcement Learningfor Urban Driving
using Implicit Affordances</strong>. In Proc. of CVPR, pages 7153–7162,
Virtual, June 2020.</li>
<li>Dian Chen, Vladlen Koltun, and Philipp Krähenbühl. <strong>Learning
to Drive from a World on Rails</strong>. In Proc. of ICCV, pages
15590–15599, Virtual, Oct. 2021</li>
</ul></li>
<li><p>two-stage training:
先训练一个专家，然后将知识传递给自动驾驶智能体</p>
<ul>
<li>Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krähenbühl.
<strong>Learning by Cheating</strong>. In Proc. of CoRL, pages 66–75,
Virtual, Nov. 2020.</li>
<li>Jiaming Zhang, Kailun Yang, Angela Constantinescu, Kunyu Peng, Karin
Müller, and Rainer Stiefelhagen. <strong>Trans4Trans: Efficient
Transformer for Transparent Object Segmentation To Help Visually
Impaired People Navigate in the Real World</strong>. In Proc. of ICCV -
Workshops, pages 1760–1770, Virtual, Oct. 2021</li>
</ul></li>
<li><p>inverse reinforcement learning</p>
<ul>
<li>Sascha Rosbach, Vinit James, Simon Großjohann, Silviu Homoceanu, and
Stefan Roth. <strong>Driving with Style: Inverse Reinforcement Learning
in General-PurposePlanning for Automated Driving</strong>. In Proc. of
IROS, pages 2658–2665, Macau, China, Nov. 2019.</li>
<li>Sahand Sharifzadeh, Ioannis Chiotellis, Rudolph Triebel, and Daniel
Cremers. <strong>Learning to Drive using Inverse Reinforcement Learning
and Deep Q-Networks</strong>. In Proc. of NIPS Workshops, pages 1–7,
Barcelona, Spain, Dec. 2016.</li>
</ul></li>
<li><p>LSTM</p>
<ul>
<li>Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell.
<strong>End-to-end Learning of Driving Models from Large-scale Video
Datasets</strong>. In Proc. of CVPR, pages 2174–2182, Honolulu, HI, USA,
July 2017.</li>
</ul></li>
<li><p>self-attention</p>
<ul>
<li>Keishi Ishihara, Anssi Kanervisto, Jun Miura, and Ville Hautamaki.
<strong>Multi-task Learning with Attention for End-to-end Autonomous
Driving</strong>. In Proc. of CVPR - Workshops, pages 2902–2911,
Virtual, June 2021.</li>
</ul></li>
<li><p>multi-task networks</p>
<ul>
<li>Dan Wang, Junjie Wen, Yuyong Wang, Xiangdong Huang, and Feng Pei.
<strong>End-to-End Self-Driving Using Deep Neural Networks with
Multi-auxiliary Tasks</strong>. Automotive Innovation, 2(2):127–136, May
2019.</li>
<li>Zhengyuan Yang, Yixuan Zhang, Jerry Yu, Junjie Cai, and Jiebo Luo.
<strong>End-to-end Multi-Modal Multi-Task Vehicle Controlfor
Self-Driving Cars with Visual Perceptions</strong>. In Proc. of ICPR,
pages 2289–2294, Beijing, China, Aug. 2018.</li>
</ul></li>
<li><p>The fusion of different input modalities</p>
<ul>
<li>Aditya Prakash, Kashyap Chitta, and Andreas Geiger.
<strong>Multi-Modal Fusion Transformer for End-to-End Autonomous
Driving</strong>. In Proc. of CVPR, pages 7077–7087, Virtual, June
2021.</li>
</ul></li>
<li><p>affordances</p>
<ul>
<li>Axel Sauer, Nikolay Savinov, and Andreas Geiger. <strong>Conditional
Affordance Learning for Driving in Urban Environments</strong>. In Proc.
of CoRL, pages 237–252, Zürich, Switzerland, Oct. 2018.</li>
</ul></li>
<li><p>waypoints 替代速度</p></li>
<li><p>Kashyap Chitta, Aditya Prakash, and Andreas Geiger. NEAT:
<strong>Neural Attention Fields for End-to-End Autonomous
Driving</strong>. In Proc. of ICCV, pages 15793–15803, Virtual, Oct.
2021.</p></li>
<li><p>probabilistic output</p>
<ul>
<li>Alexander Amini, Guy Rosman, Sertac Karaman, and Daniela Rus.
<strong>Variational End-to-End Navigation and Localization</strong>. In
Proc. of ICRA, pages 8958–8964, Montréal, QC, Canada, May 2019.</li>
</ul></li>
<li><p>sim-2-real</p>
<ul>
<li>Blażej Osiński, Adam Jakubowski, Pawel Ziecina, Piotr Miloś,
Christopher Galias, Silviu Homoceanu, and Henryk Michalewski.
<strong>Simulation-Based Reinforcement Learningfor Real-World Autonomous
Driving</strong>. In Proc. of ICRA, pages 6411–6418, Virtual, May
2020</li>
<li>GAN-based
<ul>
<li>Matthias Müller, Alexey Dosovitskiy, Bernard Ghanem, and Vladlen
Koltun. <strong>Driving Policy Transfer via Modularity and
Abstraction</strong>. In Proc. of CoRL, pages 1–15, Zürich, Switzerland,
Oct. 2018.</li>
<li>Luona Yang, Xiaodan Liang, Tairui Wang, and Eric Xing.
<strong>Real-to-Virtual Domain Unification for End-to-</strong>
<strong>EndAutonomous Driving</strong>. In Proc. of ECCV, pages 530–545,
Munich, Germany, Sept. 2018.</li>
</ul></li>
</ul></li>
<li><p>attention</p>
<ul>
<li>Bob Wei, Mengye Ren, Wenyuan Zeng, Ming Liang, Bin Yang, and Raquel
Urtasun. <strong>Perceive, Attend, and Drive: Learning Spatial Attention
for Safe Self-Driving</strong>. In Proc. of ICRA, pages 4875–4881,
Virtual, May 2021.</li>
<li>Luca Cultrera, Lorenzo Seidenari, Federico Becattini, Pietro Pala,
and Alberto Del Bimbo. <strong>Explaining Autonomous Driving by Learning
End-to-End Visual Attention</strong>. In Proc. of CVPR - Workshops,
pages 340–341, Virtual, June 2020.</li>
</ul></li>
<li><p>intermediate semantic representation</p>
<ul>
<li>Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and
Raquel Urtasun. <strong>Perceive, Predict, and Plan: Safe Motion
Planning Through Interpretable Semantic Representations</strong>. In
Proc. of ECCV, pages 414–430, Virtual, Aug. 2020.</li>
</ul></li>
<li><p>on-line data selection techniques (视觉输入情况)</p>
<ul>
<li>Aditya Prakash, Aseem Behl, Eshed Ohn-Bar, Kashyap Chitta, and
Andreas Geiger. <strong>Exploring Data Aggregation in Policy Learning
for Vision-based Urban Autonomous Driving</strong>. In Proc. of CVPR,
pages 11763–11773, Virtual, June 2020.</li>
<li>Soumi Das, Harikrishna Patibandla, Suparna Bhattacharya, Kshounis
Bera, Niloy Ganguly, and Sourangshu Bhat tacharya. <strong>TMCOSS:
Thresholded Multi-Criteria Online Subset Selection forData-Efficient
Autonomous Driving</strong>. In Proc. of ICCV, pages 6341–6350, Virtual,
Oct. 2021. 2</li>
</ul></li>
</ul>
<h4 id="evaluation-of-end-to-end-deep-driving-models">Evaluation of
End-to-End Deep Driving Models</h4>
<ul>
<li>CARLA benchmarks CoRL2017 [20]
<ul>
<li>Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and
Vladlen Koltun. CARLA: An Open Urban Driving Simulator. In Proc. of
CoRL, pages 1–16, Mountain View, CA, USA, Nov. 2017.</li>
</ul></li>
<li>NoCrash [17]
<ul>
<li>Felipe Codevilla, Eder Santana, Antonio M. López, and Adrien Gaidon.
Exploring the Limitations of Behavior Cloning for Autonomous Driving. In
Proc. of ICCV, pages 9329–9338, Seoul, Korea, Oct. 2019.</li>
</ul></li>
<li>Leaderboard [1]
<ul>
<li>CARLA Autonomous Driving Leaderboard. https://leaderboard.carla.org,
2020. 3, 5, 6</li>
</ul></li>
</ul>
<h3 id="end-to-end-deep-driving">3 End-to-End Deep Driving</h3>
<ul>
<li><p>文章设置了一个城市驾驶环境的点到点导航任务，预先定义一个稀疏点的路线为粗略的行驶路线，车辆需要避免违法和碰撞，从初始点驾驶到终止点（CARLA
Leaderboard标准配置）。</p></li>
<li><p>输入：一个RGB前置相机图像、一个LiDAR点云</p></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-2.png"
alt="End-to-end driving method" />
<figcaption aria-hidden="true">End-to-end driving method</figcaption>
</figure>
<ul>
<li><p>使用Conditional Imitation Learning</p></li>
<li><p>不同训练集设置情况：</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-3.png"
alt="Training set design" />
<figcaption aria-hidden="true">Training set design</figcaption>
</figure></li>
<li><p>验证集和测试集路线类型</p></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-4.png"
alt="Validation and test route types" />
<figcaption aria-hidden="true">Validation and test route
types</figcaption>
</figure>
<ul>
<li>不同验证集和测试集设置情况</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-5.png"
alt="Validation and test design" />
<figcaption aria-hidden="true">Validation and test design</figcaption>
</figure>
<ul>
<li><p>不同数据集上的驾驶得分<span
class="math display">\[DS\in[0,1]\]</span>，包括两部分分数：</p>
<ul>
<li><p>完成路线 route completion percentage <span
class="math display">\[RC\in [0, 1]\]</span>;</p></li>
<li><p>避免事故 infraction score <span class="math display">\[IS \in [0,
1]\]</span>;</p></li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-6.png"
alt="Driving scores" />
<figcaption aria-hidden="true">Driving scores</figcaption>
</figure>
<ul>
<li>不同训练集和验证集的性能对比</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-7.png"
alt="Pearson correlation between the validation set performance and the test set performance (given by the driving score)" />
<figcaption aria-hidden="true">Pearson correlation between the
validation set performance and the test set performance (given by the
driving score)</figcaption>
</figure>
<ul>
<li>不同验证集下的测试集得分对比</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-8.png"
alt="Test driving scores given in (%) obtained on R^test, having used different validation sets" />
<figcaption aria-hidden="true">Test driving scores given in (%) obtained
on R^test, having used different validation sets</figcaption>
</figure>
<ul>
<li>不同训练集下的性能对比</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220824-9.png"
alt="Performance on different training sets" />
<figcaption aria-hidden="true">Performance on different training
sets</figcaption>
</figure>
<ul>
<li><p>结论（不完整）：</p>
<ul>
<li><p>随机性对仿真实验的影响大。随机种子、carla等。</p></li>
<li><p>训练集和验证集的选择对结果影响大。</p></li>
<li><p>数据量从100k-220k变化时，对大约160，000幅图像的训练似乎已经在性能和复杂性之间提供了良好的平衡，同时使用更大但计算上更昂贵的数据量可以获得最佳结果。（Training
on approximately 160, 000 images already seems to provide a good
trade-off between performance and complexity, while best results are
obtained using larger but computationally more expensive amounts of
data.）。</p></li>
<li><p>数据越少路线完成率越高，但事故率也高；数据越多，事故率显著降低，但路线完成率也降低；</p></li>
<li><p>专家数据是必要的。不完美的专家数据对结果影响大</p></li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<p>总的来说比较玄学，对于视觉输入的模型而言，数据量偏小（100k-220k），给出的经验参数不具有普遍意义。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/23/2022-08-23-Autonomous%20Driving%20-%20%E5%85%B3%E4%BA%8E%E5%8A%A0%E5%87%8F%E9%80%9F%E5%8F%98%E9%81%93%E7%9A%84%E6%80%9D%E8%80%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/23/2022-08-23-Autonomous%20Driving%20-%20%E5%85%B3%E4%BA%8E%E5%8A%A0%E5%87%8F%E9%80%9F%E5%8F%98%E9%81%93%E7%9A%84%E6%80%9D%E8%80%83/" class="post-title-link" itemprop="url">Autonomous Driving | 关于加减速变道的思考</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-23 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-23T00:00:00+08:00">2022-08-23</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>515</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="关于加减速变道的思考">关于加减速变道的思考</h2>
<h3 id="为什么要加减速变道"><strong>1. 为什么要加减速变道</strong></h3>
<p>车端的基于规则的变道算法已经实现了常规情况下的变道过程，但目前基于规则的算法都是匀速变道，对于需要加减速的较复杂的变道场景并不能自适应。因此需要一种既能应对常规简单变道情况，又能灵活适应车流密集、需要的较大幅度变速的变道算法模型。</p>
<h3 id="加减速变道并不是变道场景中的特例">2.
<strong>加减速变道并不是变道场景中的特例</strong></h3>
<p>在行车过程中，根据实际路况灵活地加减速度是一项融合于几乎所有驾驶场景中的基本驾驶技能，不应当被视为一种只在特定场景使用的技能。因此，不适合单独识别出一个类别用于概括加减速变道情况。</p>
<h3 id="车辆加减速的目的">3. 车辆加减速的目的</h3>
<p>在复杂路况中，理想便道路径上存在障碍物，或者路径上存在潜在不安全因素，具有一定风险。为避免碰撞、规避风险，使用加减速策略灵活调整路线，实现安全、合规的自动驾驶。</p>
<h3 id="如何使得agent获得变速变道能力">4.
如何使得Agent获得变速变道能力</h3>
<p>既然加减速并非独立的技能，因此只需在正常的驾驶规划训练中学习即可。由于加减速变道的目的在于规避碰撞等风险，因此需要考虑设置碰撞相关的损失函数。另一方面，模仿学习中的专家数据中如果他车、障碍物比较少，可以尝试生成一些障碍物，增加规划的避障难度。</p>
<p>因此，总的来说，有两个方案可以提高agent变速变道能力：<code>碰撞loss</code>
和 <code>障碍生成</code>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/20/2022-08-20-Autonomous%20Driving%20-%20Off-Policy%20Deep%20Reinforcement%20Learning%20without%20Exploration/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/20/2022-08-20-Autonomous%20Driving%20-%20Off-Policy%20Deep%20Reinforcement%20Learning%20without%20Exploration/" class="post-title-link" itemprop="url">Autonomous Driving | Off-Policy Deep Reinforcement Learning without Exploration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-20 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-20T00:00:00+08:00">2022-08-20</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="off-policy-deep-reinforcement-learning-without-exploration-2019">Off-Policy
Deep Reinforcement Learning without Exploration (2019)</h2>
<p>类似DQN和DDPG的off-policy
RL算法在被禁止探索，并在没有数据策略分布修正的的情况下，难以取得好的效果。本文通过限制off-policy
agent的行为空间，使其行为类似与on-policy算法，最后提出了一个较为通用的，针对连续控制的deep
reinforcement learning algorithm。</p>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p><strong>batch reinforcement
learning</strong>：在一些数据收集面临costly, risky, or
time-consuming的情境时，智能体只能从固定的数据集中学习策略，并且其对数据质量的要求较低。</p></li>
<li><p>当数据集中的分布与当前算法策略的分布不同时，标准的off-policy
RL算法将会失败。</p></li>
<li><p>extrapolation
error：未见过的状态-动作对被错误地估计为具有不现实的价值的现象。</p></li>
<li><p>引入了batch-constrained reinforcement learning来克服extrapolation
error，最大化奖励的同时，最小化批量数据中状态-行为对和策略访问的状态行为对的误匹配。</p></li>
<li><p>本文提出算法：<strong>Batch-Constrained deep Q-learning</strong>
(BCQ)，利用一个与Q-network结合的条件状态生成模型，只生成过去见过的行为。在较弱的假设下，证明了这种批约束范式对于有限确定MDP的不完全数据集的无偏估计是必要的。</p></li>
<li><p>代码开源：https://github.com/sfujim/BCQ</p></li>
</ul>
<h3 id="background">2 Background</h3>
<ul>
<li>贝尔曼算子（Bellman operator）<span
class="math display">\[\Tau^{\pi}\]</span>： <span
class="math display">\[
\Tau^{\pi}Q(s,a)=\mathbb{E}_{s&#39;}[r+\gamma Q(s&#39;,\pi(s&#39;))]
\]</span></li>
</ul>
<h3 id="extrapolation-error-外推误差">3 Extrapolation Error
(外推误差)</h3>
<ul>
<li><p>Extrapolation error is an error in off-policy value learning
which is introduced by the mismatch between the dataset and true
state-action visitation of the current policy，或者<span
class="math display">\[(s,a)\]</span>在数据集中不存在</p></li>
<li><p>外推误差的成因：</p>
<ul>
<li><p><strong>数据缺省</strong>（Absent Data）：state-action pair (s,
a) is unavailable，因此在估计状态-行为价值Q的时候会引入误差。</p></li>
<li><p><strong>模型偏置</strong>（Model Bias）：when performing
off-policy Q-learning with a batch B，状态转移动力学的有偏估计为：</p>
<p><span class="math display">\[\Tau^{\pi}Q(s,a) \approx
\mathbb{E}_{s&#39;\sim B}[r+\gamma Q(s&#39;,\pi(s&#39;))]\]</span>
其中，状态转移依据buffer B，而不是真实的MDP。</p></li>
<li><p><strong>训练误匹配</strong>（Training
Mismatch）：当数据的分布和当前策略的分布不匹配，对action的估计会有误差</p></li>
</ul></li>
<li><p>As a result, learning a value estimate with off-policy data can
result in large amounts of extrapolation error if the policy selects
actions which are not similar to the data found in the batch.</p></li>
</ul>
<h4 id="extrapolation-error-in-deep-reinforcement-learning">3.1
Extrapolation Error in Deep Reinforcement Learning</h4>
<ul>
<li>本节使用SOTA Actor-Critic off-policy RL
算法DDPG，在与策略无关的数据集上学习，观察到性能迅速恶化。</li>
<li>These results suggest that off-policy deep reinforcement learning
algorithms are ineffective when learning truly off-policy.</li>
<li>训练环境：OpenAI gym’s Hopper-v1 environment</li>
<li>train an off-policy DDPG agent with no interaction with the
environment.</li>
<li>三个batch：
<ul>
<li><strong>Batch 1 (Final buffer)</strong>：1 million time
steps，行为加上（0,0.5）的高斯噪声，store all experienced
transitions。</li>
<li><strong>Batch 2 (Concurrent)</strong>：训练behavioral DDPG agents，1
million time
steps，行为加上（0,0.1）的高斯噪声，每一次转移经验都放入buffer，即该情况下每个behavioral和off-policy智能体使用同一的数据集训练。</li>
<li><strong>Batch 3 (Imitation)</strong>：使用一个完全训练的DDPG，收集1
million time steps数据作为专家数据。</li>
</ul></li>
<li>实验中的off-policy DDPG完全使用离线数据训练，behavioral
DDPG与环境交互正常训练。</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220820-1.png"
alt="Figure 1" />
<figcaption aria-hidden="true">Figure 1</figcaption>
</figure>
<p>Even in the concurrent experiment, where both agents are trained with
the same dataset, there is a large gap in performance in every single
trial.</p>
<ul>
<li>结论：
<ul>
<li>Batch 1中，即使有了充足的探索，agent依然难以具有稳定的value
estimation</li>
<li>Batch
2中，即使使用相同的数据训练，agent之间的差异巨大。说明policy初始化的不同足以引入外推误差。</li>
<li>Batch
3中，尽管有了专家数据，agent仍快速学习非专家策略，最终导致效果很差。</li>
</ul></li>
<li>外推误差提供了一个噪声源，可导致持续高估偏差。extrapolation error
provides a source of noise that can induce a persistent overestimation
bias (Thrun &amp; Schwartz, 1993; Van Hasselt et al., 2016; Fujimoto et
al.,2018).</li>
<li>在完全off-policy情况下，外推误差无法通过与环境交互获得新数据来消除</li>
</ul>
<h3 id="batch-constrained-reinforcement-learning">4 Batch-Constrained
Reinforcement Learning</h3>
<h3 id="总结">总结</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/18/2022-08-18-Autonomous%20Driving%20-%20[U]%20Goal-driven%20Self-Attentive%20Recurrent%20Networks%20for%20Trajectory%20Prediction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/18/2022-08-18-Autonomous%20Driving%20-%20%5BU%5D%20Goal-driven%20Self-Attentive%20Recurrent%20Networks%20for%20Trajectory%20Prediction/" class="post-title-link" itemprop="url">Autonomous Driving | Goal-driven Self-Attentive Recurrent Networks for Trajectory Prediction</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-18 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-18T00:00:00+08:00">2022-08-18</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>224</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="goal-driven-self-attentive-recurrent-networks-for-trajectory-prediction">Goal-driven
Self-Attentive Recurrent Networks for Trajectory Prediction</h2>
<p>提出了一种基于U-Net
architecture和注意力的循环网络框架，增加了语义信息来预测轨迹终点。提出的算法在公开数据集SDD,
inD, ETH/UCY上部分取得了SOTA的成绩，还简化了模型。</p>
<h3 id="introduction">1 Introduction</h3>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220817-2.png"
alt="Levels of multi-agent learning in autonomous driving" />
<figcaption aria-hidden="true">Levels of multi-agent learning in
autonomous driving</figcaption>
</figure>
<h3 id="总结">总结</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/17/2022-08-17-Autonomous%20Driving%20-%20SMARTS%20Scalable%20Multi-Agent%20Reinforcement%20Learning%20Training%20School%20for%20Autonomous%20Driving/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/17/2022-08-17-Autonomous%20Driving%20-%20SMARTS%20Scalable%20Multi-Agent%20Reinforcement%20Learning%20Training%20School%20for%20Autonomous%20Driving/" class="post-title-link" itemprop="url">Autonomous Driving | SMARTS Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving (Huawei)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-17 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-17T00:00:00+08:00">2022-08-17</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.6k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="smarts-scalable-multi-agent-reinforcement-learning-training-school-for-autonomous-driving">SMARTS:
Scalable Multi-Agent Reinforcement Learning Training School for
Autonomous Driving</h2>
<p>顾名思义，SMARTS是针对多智能体算法的自动驾驶强化学习仿真平台。</p>
<p>开源了基准任务和代码：https://github.com/huawei-noah/SMARTS</p>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>自动驾驶仿真的挑战之一是天气问题；绝大部分数据是在好天气（fair
weather）下采集的；当前的L4自动驾驶面对复杂的交互情况时。倾向于减速等待，而不是提前主动找办法通过；</p></li>
<li><p>Waymo的California2018的自动驾驶事故数据中, 57%是发生了追尾（rear
endings），29%是发生了侧面碰撞（sideswipes），并且事故都是由于他车造成的，因此说明过于保守的驾驶策略</p></li>
<li><p>waymo的汽车相比人类驾驶员经常过分刹车，导致乘客晕车</p></li>
<li><p>多智能体交互的分级标准：“multi-agent learning levels”, or
“M-levels“</p></li>
<li><p>double
merge道路场景（即&gt;--&lt;形道路）是多智能体交互的难点，车辆需考虑是继续走还是等待；在间隙不够大的时候是否需要变道；其他车开到了自车车道上，是否和它交换位置？等</p></li>
<li><p>平台设计目标：</p>
<ul>
<li><p>Bootstrapping Realistic Interaction</p>
<ul>
<li><ol type="1">
<li>physics,</li>
</ol></li>
<li><ol start="2" type="1">
<li>behavior of road users,</li>
</ol></li>
<li><ol start="3" type="1">
<li>road structure &amp; regula-tions,</li>
</ol></li>
<li><ol start="4" type="1">
<li>background traffic flow.</li>
</ol></li>
</ul></li>
<li><p>Heterogeneous Agent Computing（异构智能体计算）</p></li>
<li><p>Simulation Providers</p></li>
<li><p>Interaction Scenarios</p></li>
<li><p>Distributed Computing</p></li>
</ul></li>
<li><p>Key Features：</p></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220817-1.png"
alt="Levels of multi-agent learning in autonomous driving" />
<figcaption aria-hidden="true">Levels of multi-agent learning in
autonomous driving</figcaption>
</figure>
<ul>
<li>场景：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220817-2.png"
alt="Levels of multi-agent learning in autonomous driving" />
<figcaption aria-hidden="true">Levels of multi-agent learning in
autonomous driving</figcaption>
</figure>
<ul>
<li><p><strong>Observation.</strong> The observation is a stack of three
consecutive frames, which covers the dynamic objects and key events. For
each frame, it contains: 1) relative position of goal; 2) distance to
the center of lane; 3) speed; 4) steering; 5) a list of heading errors;
6) at most eight neighboring vehicles’ driving states (relative
distance, speed and position); 7) a bird’s-eye view gray-scale image
with the agent at the center.</p></li>
<li><p><strong>Action.</strong> The action used here is a
four-dimensional vector of discrete values, for longitudinal
control—keep lane and slow down—and lateral control—turn right and turn
left.</p></li>
<li><p><strong>Reward.</strong> The reward is a weighted sum of the
reward components shaped according to ego vehicle states, interactions
involving surrounding vehicles, and key traffic events. More details can
be found in our implementation code.</p></li>
</ul>
<h3 id="总结">总结</h3>
<p>华为推出的针对多智能体的自动驾驶仿真环境，设计相对灵活，支持场景和他车的编辑，以及Waymo的真实数据，后续可以继续看看</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/12/2022-08-12-Autonomous%20Driving%20-%20An%20Optimistic%20Perspective%20on%20Offline%20Reinforcement%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/2022-08-12-Autonomous%20Driving%20-%20An%20Optimistic%20Perspective%20on%20Offline%20Reinforcement%20Learning/" class="post-title-link" itemprop="url">Autonomous Driving | An Optimistic Perspective on Offline Reinforcement Learning (Google)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.3k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="an-optimistic-perspective-on-offline-reinforcement-learning-google">An
Optimistic Perspective on Offline Reinforcement Learning (Google)</h2>
<p>作者用online DQN在60款 Atari
2600游戏上获取数据样本，然后用这些样本(fixed
dataset)训练offline强化学习算法，一些offline的算法性能可以超过online的算法。本文提出的Random
Ensemble Mixture
(REM)算法在离线回放数据上的表现超过了强的基准算法。因此作者认为在离线样本足够多，多样化充分的情况下，使用鲁棒的RL算法可以获得高质量的策略。</p>
<p>代码： github.com/google-research/batch_rl</p>
<h3 id="introduction">1 Introduction</h3>
<p>离线强化学习的设定是不与真实环境的主动交互，而是通过对收集的离线回放数据中学习策略，在评估模型中生成新的交互数据。相应的情景在以下场景均会面临：</p>
<ul>
<li><strong>robotics</strong>
<ul>
<li>Cabi, S., Colmenarejo, S. G., Novikov, A., Konyushkova, K., Reed,
S., Jeong, R., Zołna, K., Aytar, Y., Budden, D., Vecerik, M., et al. A
framework for data-driven robotics. arXiv preprint arXiv:1909.12200,
2019.</li>
<li>Dasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper,
K., Singh, S., Levine, S., and Finn, C. Robonet: Large-scale multi-robot
learning. CoRL, 2019.</li>
</ul></li>
<li><strong>autonomous driving</strong>
<ul>
<li>Yu, F., Xian, W., Chen, Y., Liu, F., Liao, M., Madhavan, V., and
Darrell, T. Bdd100k: A diverse driving video database with scalable
annotation tooling. CVPR, 2018.</li>
</ul></li>
<li><strong>recommendation systems</strong>
<ul>
<li>Strehl, A. L., Langford, J., Li, L., and Kakade, S. Learning from
logged implicit exploration data. NeurIPS, 2010.</li>
<li>Bottou, L., Peters, J., Quiñonero-Candela, J., Charles, D. X.,
Chickering, D. M., Portugaly, E., Ray, D., Simard, P., and Snelson, E.
Counterfactual reasoning and learning systems: The example of
computational advertising. JMLR, 2013.</li>
</ul></li>
<li><strong>healthcare</strong>
<ul>
<li>Shortreed, S. M., Laber, E., Lizotte, D. J., Stroup, T. S., Pineau,
J., and Murphy, S. A. Informing sequential clinical decisionmaking
through reinforcement learning: an empirical study. Machine learning,
2011.</li>
</ul></li>
</ul>
<p>面对离线强化学习问题时，off-policy算法普遍表现不好，设计大的replay
buffer反而会损害off-policy算法的性能（由于算法的off-policyness）</p>
<ul>
<li>对比的方法包括：
<ul>
<li><strong>offline QR-DQN</strong>：Dabney, W., Rowland, M., Bellemare,
M. G., and Munos, R. Distributional reinforcement learning with quantile
regression. AAAI, 2018.</li>
<li><strong>DQN</strong>：（Nature）</li>
<li><strong>Random Ensemble
Mixture</strong>（REM，随机集成混合）：为本文提出方法</li>
<li><strong>online C51</strong>： 分布式DQN算法。Bellemare, M. G.,
Dabney, W., and Munos, R. A distributional perspective on reinforcement
learning. ICML, 2017.</li>
<li><strong>distributional QR-DQN (SOTA)</strong>：Dabney, W., Rowland,
M., Bellemare, M. G., and Munos, R. Distributional reinforcement
learning with quantile regression. AAAI, 2018</li>
</ul></li>
</ul>
<h3 id="off-policy-reinforcement-learning">2 Off-policy Reinforcement
Learning</h3>
<ul>
<li><p>DQN算法介绍</p>
<ul>
<li>Huber loss：介于MSE和MAE之间的，对数据异常值更不敏感的loss</li>
</ul>
<p><span class="math display">\[
l_{\lambda}(u)=
\begin{align}
\begin{cases}
\frac{1}{2}u^2,&amp;\mid u \mid \leq \lambda\\
\lambda(\mid u\mid-\frac{1}{2}\lambda),&amp; \text{otherwise}
\end{cases}
\end{align}
\]</span></p></li>
<li><p>baseline方法：分布式RL（Distributional RL）</p>
<ul>
<li>C51</li>
<li>QR-DQN</li>
</ul></li>
</ul>
<h3 id="offline-reinforcement-learning">3 Offline Reinforcement
Learning</h3>
<ul>
<li><p>offline的模式分离了模型对经验的利用、生成能力（exploit） vs
探索效率（explore）</p></li>
<li><p>offline RL面临的挑战是<strong>distribution
mismatch</strong>：错误匹配当前使用的策略和固定的离线数据集。例如，当采取了数据集中不存在的行为时，并不知道响应的奖励是多少。<br />
</p></li>
<li><p>本文尝试在不解决distribution
mismatch的基础上，训练高性能的agent</p></li>
</ul>
<h3 id="developing-robust-offline-rl-algorithms">4 Developing Robust
Offline RL Algorithms</h3>
<ul>
<li>采用集成（Ensemble）可以提高模型的泛化能力，本文使用了Ensemble
DQN和REM两个采取该思想的方法。</li>
</ul>
<h4 id="ensemble-dqn">4.1 Ensemble-DQN</h4>
<ul>
<li><p>该方法是对DQN算法的简单扩展，使用集成多个参数化的Q函数来近视Q值。</p>
<ul>
<li>Faußer, S. and Schwenker, F. Neural network ensembles in
reinforcement learning. Neural Processing Letters, 2015<br />
</li>
<li>Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep
exploration via bootstrapped DQN. NeurIPS, 2016.<br />
</li>
<li>Anschel, O., Baram, N., and Shimkin, N. Averaged-dqn: Variance
reduction and stabilization for deep reinforcement learning. ICML,
2017.</li>
</ul></li>
<li><p>每个参数化Q函数的优化目标是近似真实的Q值，参考下面这篇文章：</p>
<ul>
<li><strong>Bootstrapped-DQN</strong>：Osband, I., Blundell, C.,
Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped DQN.
NeurIPS, 2016.</li>
</ul></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220812-2.png"
alt="损失函数" />
<figcaption aria-hidden="true">损失函数</figcaption>
</figure>
<p>其中，<span
class="math display">\[l_{\lambda}\]</span>是Huber损失。算法使用所有Q函数的均值作为输出。</p></li>
</ul>
<h4 id="random-ensemble-mixture-rem">4.2 Random Ensemble Mixture
(REM)</h4>
<ul>
<li>引入了dropout：
<ul>
<li>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and
Salakhutdinov, R. Dropout: a simple way to prevent neural networks from
overfitting. JMLR, 2014</li>
</ul></li>
<li>不同于Ensemble-DQN，REM构造一个包含多个Q函数近似器的凸组合（convex
combination），将多个Q 函数近似器作为1个近似器使用。使用(K −
1)-simplex计算混合的概率。</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220812-3.png"
alt="模型结构" />
<figcaption aria-hidden="true">模型结构</figcaption>
</figure>
<ul>
<li><p>对于每个mini-batch，随机产生一个分类分布（categorical
distribution）<span
class="math display">\[\alpha\]</span>，它定义了一个逼近最优Q-函数的K个估计器的凸组合</p></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220817-3.png"
alt="REM的loss形式" />
<figcaption aria-hidden="true">REM的loss形式</figcaption>
</figure>
<p>其中，<span
class="math display">\[P_{\Delta}\]</span>表示标准的(K-1)-simplex的概率分布，<span
class="math display">\[\Delta^{K-1}=\{\alpha\in R^K:
\alpha_1+\alpha_2+...+\alpha_K=1,\alpha_k\geq0,k=1,...,K
\}\]</span></p></li>
<li><p>对于<span class="math display">\[P_{\Delta}\]</span>：先从Uniform
(0,1)分布中采样K个独立同分布的值，然后归一化它们获得有效的分类分布（<span
class="math display">\[a&#39;_k \sim U(0,1),a_k=a&#39;_k/\sum_k
a&#39;_i\]</span>）</p></li>
<li><p>对于Q值的求解，使用<span
class="math display">\[Q(s,a)=\frac{1}{K}\sum_k Q_{\theta}^k
(s,a)\]</span></p></li>
</ul>
<h3 id="offline-rl-on-atari-2600-games">5 Offline RL on Atari 2600
Games</h3>
<ul>
<li>将Nature DQN在60个Atari游戏上的行为数据用来构建DQN
replay数据集，每个游戏2亿帧（200 million frames）</li>
<li>每个游戏训练5个智能体，因此60个游戏一共有60个数据集</li>
<li>ofline RL算法性能对比：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220812-1.png"
alt="Offline RL on Atari 2600." />
<figcaption aria-hidden="true">Offline RL on Atari 2600.</figcaption>
</figure>
<ul>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220818-1.png"
alt="Offline QR-DQN vs. DQN (Nature)" />
<figcaption aria-hidden="true">Offline QR-DQN vs. DQN
(Nature)</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220818-2.png"
alt="Offline Agents on DQN Replay Dataset" />
<figcaption aria-hidden="true">Offline Agents on DQN Replay
Dataset</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220818-3.png"
alt="Asymptotic performance of offline agents" />
<figcaption aria-hidden="true">Asymptotic performance of offline
agents</figcaption>
</figure></li>
<li><p>文中提到offline连续强化学习方法，实验了offline DDPG，offline
TD3，offline BCQ等算法</p>
<ul>
<li><p>Fujimoto, S., Meger, D., and Precup, D. Off-policy deep
reinforcement learning without exploration. ICML, 2019b.</p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220818-4.png"
alt="Asymptotic performance of offline agents" />
<figcaption aria-hidden="true">Asymptotic performance of offline
agents</figcaption>
</figure></li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<p>总的来说，文章本身目的更倾向于提供一种乐观的横向对比，证明offline
RL在一些情况下可以获取SOTA的性能，甚至超过online RL。提供的几个offline
Q-learning变体和offline连续情况的RL算法可以再看看。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/04/2022-08-04-Autonomous%20Driving%20-%20UMBRELLA%20Uncertainty-Aware%20Model-Based%20Offline%20Reinforcement%20Learning%20Leveraging%20Planning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/04/2022-08-04-Autonomous%20Driving%20-%20UMBRELLA%20Uncertainty-Aware%20Model-Based%20Offline%20Reinforcement%20Learning%20Leveraging%20Planning/" class="post-title-link" itemprop="url">Autonomous Driving | UMBRELLA: Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging Planning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-04 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-04T00:00:00+08:00">2022-08-04</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>5.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>5 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="umbrella-uncertainty-aware-model-based-offline-reinforcement-learning-leveraging-planning">UMBRELLA:
Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging
Planning</h2>
<ul>
<li>在学习过程中考虑了随机不确定性的影响，提高了模型的可迁移性和可解释性</li>
</ul>
<h3 id="introduction">1 Introduction</h3>
<ul>
<li><p>目前的模型考虑到多智能体之间的交互和复杂行为，多采用工程设计的驾驶策略，但这样难以适用更复杂的任务。强化学习通过试错学习的方式避免了这些手工设计，但需要大量的试错机会。相比于模仿学习习得次优行为，强化学习可以提升不同类型数据的质量</p></li>
<li><p>主要贡献：</p>
<ul>
<li>提出了一个基于模型的离线规划算法UMBRELLA，在观测上考虑了认知和偶然的不确定性（epistemic
和 aleatoric）<br />
</li>
<li>引入了一个消融实验，优化最坏情况模型</li>
<li>实验中，在稠密车流（with dense
traffic）的城市和高速场景中，胜过了BC行为克隆方法和MBOP算法</li>
</ul></li>
</ul>
<h3 id="related-work">2 Related Work</h3>
<ul>
<li><p><strong>Model-based Offline Reinforcement Learning</strong>：</p>
<p>其主要的问题是the distributional
shift。MOReL和MBOP方法将动态模型的认知不确定性估计结合到奖励函数中，以惩罚未被行为分布覆盖的状态</p>
<ul>
<li>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline
reinforcement learning: Tutorial, review, and perspectives on open
problems. CoRR, 2020. arXiv:2005.01643.</li>
</ul>
<p>这些方法在多智能体环境中测试，没考虑到由行人和其它车辆行为造成的偶然不确定性（aleatoric
uncertainty）的影响。[Henaff et al.,
2019]通过由条件变分自动编码器（conditional variational
autoencoder，CVAE）表示的随机动力学模型解决了这个问题[Kingma and
Welling,
2014]。然而，他们的方法依赖于策略学习，这与基于模型的离线规划相比，可解释性和控制灵活性有所降低。</p>
<ul>
<li>Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive
policy learning with uncertainty regularization for driving in dense
traffic. In International Conference on Learning Representations,
\2019.<br />
</li>
<li>Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes.
In International Conference on Learning Representations, 2014.</li>
</ul></li>
<li><p><strong>Interaction-aware Motion Prediction and
Planning</strong>：</p>
<p>单纯通过规划安全的pass进行自动驾驶忽略了车辆之间包含规划和预测（planning
and prediction）的行为交互</p>
<p>引入博弈论的方法考虑了多智能体的动力学过程，但带来了计算上的开销。</p>
<p>一些基于学习的模型可以生成更多的场景，但是没有考虑认知不确定性</p>
<ul>
<li>Jerry Liu, Wenyuan Zeng, Raquel Urtasun, and Ersin Yumer. Deep
structured reactive planning. In IEEE International Conference on
Robotics and Automation, 2021.<br />
</li>
<li>Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A. Wright,
Rowan McAllister, Joseph E. Gonzalez, and Sergey Levine. Contingencies
from observations: Tractable contingency planning with learned behavior
models. In IEEE International Conference on Robotics and Automation,
2021.</li>
</ul></li>
</ul>
<h3 id="the-umbrella-framework">3 The UMBRELLA Framework</h3>
<ul>
<li><p>UMBRELLA 是 MBOP 算法的扩展</p>
<ul>
<li>Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline
planning. In International Conference on Learning Representations,
2021.</li>
</ul></li>
<li><p>问题形式化，用MDP的<span class="math display">\[(S; A; p; r;
\gamma)\]</span>
表示，在offline设定中，智能体不与环境直接交互，而是从数据集中学习策略<span
class="math display">\[\pi_d\]</span>。如果观测值<span
class="math display">\[O\]</span>并不是完全可得，那么问题就变化为（partially
observable MDP，POMDP），使用 <span
class="math display">\[M_{PO}=(S;A;O;p;r;\gamma)\]</span>，处理该问题的常用方法是使用nth-order
history
方法，可以近似得到状态估计，然后将其转变为MDP，用标准的RL方法处理。</p></li>
<li><p>UMBRELLA使用连续的潜在变量（continuous latent variable）<span
class="math display">\[z_t\in Z\]</span>
，并枚举自车所有的可能行为（行为采样通过学到的BC
policy）。预测N条长度界限为H的轨迹。还使用了一个return-weighted
trajectory optimizer，处理POMDP问题，用持续观测状态从<span
class="math display">\[o_{t-_c:t}\]</span>直到时间步t估计缺失的观测状态。（相关解释参考VAE算法）</p>
<ul>
<li><span
class="math display">\[z_t=\mu_{\phi}+\sigma_{\phi}*\epsilon\]</span></li>
<li><span
class="math display">\[(\mu_{\phi},\sigma_{\phi})=q_{\phi}(s_t,s_{t+1})\]</span></li>
<li><span class="math display">\[\epsilon \sim
\mathcal{N}(0,1)\]</span></li>
</ul></li>
<li><p>随机动力学模型：</p>
<ul>
<li><p>为了建模不同的未来可能情况，学习随机前向动力学模型<span
class="math display">\[f_{m,\theta}:S\times A\times Z \rightarrow S
\times \mathbb{R}\]</span></p></li>
<li><p>该模型采用CVAE架构，预测下一时刻的状态</p>
<ul>
<li>Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes.
In International Conference on Learning Representations, 2014.</li>
</ul></li>
<li><p><span
class="math display">\[\hat{s}_{t+1}=f_m(s_t,a_t,z_t)_s\]</span></p></li>
<li><p><span
class="math display">\[\hat{r}_{t}=f_m(s_t,a_t,z_t)_r\]</span></p></li>
<li><p>其中潜在变量<span
class="math display">\[z_t\]</span>建模了不同的未来预测，并确保了输出对于输入是非确定性的。在训练过程中，该潜在变量从后验分布<span
class="math display">\[q_{\phi}(z\mid s_t,s_{t+1})\]</span>中采样，<span
class="math display">\[\phi\]</span>是参数。由于实际采样只能从先验分布中采，因此使用Kullback-Leibler
(KL) divergence度量后验分布和先验分布<span
class="math display">\[p(z)\]</span>并最小化距离。</p>
<ul>
<li>潜变量用于区分细分情况，精细化建模</li>
</ul></li>
<li><p>定义Evidence Lower BOund (ELBO)目标训练VAEs。</p>
<ul>
<li>Evidence Lower BOund： https://zhuanlan.zhihu.com/p/400322786</li>
</ul></li>
<li><p>损失函数为： <span class="math display">\[
L(\theta,\phi;s_t,s_{t+1},a_t,r_t)=\mid\mid s_{t+1} -
f_{m,\theta}(s_t,a_t,z_t)_s\mid\mid_2^2 + \mid\mid
r_t-f_{m,\theta}(s_t,a_t,z_t)_r\mid\mid^2_2 \\
\zeta D_{KL}(q_{\phi}(z_t\mid s_t,s_{t+1})\mid\mid p(z_t))
\]</span></p></li>
<li><p>BC policies <span class="math display">\[f_{b,\psi}:S\times
A^{n_c} \rightarrow A \]</span> ，<span
class="math display">\[f_b(s_t,a_{(t-n_c):(t-1)})\]</span>，该函数使用当前的状态和<span
class="math display">\[n_c\]</span>个之前的行为作为输入，输出行为<span
class="math display">\[a_t\]</span>。通过将之前的行为串联考虑，可以使得输出的行为更加平滑。</p></li>
<li><p>训练BC Policy使用最小化损失函数：</p>
<p><span
class="math display">\[L(\psi;s_t,a_{(t-n_c):(t-1)},a_t)=\mid\mid a_t -
f_{b,\psi}(s_t,a_{(t-n_c):(t-1)})\mid\mid^2_2\]</span></p></li>
<li><p>截断价值函数<span class="math display">\[f_{R,\xi}:S\times
A^{n_c}\rightarrow \mathbb{R}\]</span> 估计H个时间步后的期望回报<span
class="math display">\[\hat{R}_H\]</span></p></li>
</ul></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-3.png"
alt="Simplified network architecture of the stochastic forward dynamics model during training" />
<figcaption aria-hidden="true">Simplified network architecture of the
stochastic forward dynamics model during training</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-4.png"
alt="Behavior-cloned policy network architecture" />
<figcaption aria-hidden="true">Behavior-cloned policy network
architecture</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-5.png"
alt="Truncated value function network architecture" />
<figcaption aria-hidden="true">Truncated value function network
architecture</figcaption>
</figure></li>
<li><figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-6.png"
alt="Signal flow of the stochastic forward dynamics model" />
<figcaption aria-hidden="true">Signal flow of the stochastic forward
dynamics model</figcaption>
</figure></li>
<li><p><strong>UMBRELLA-Planning</strong>：采用了MPC，在每一步规划未来H步的最优控制轨迹，然后只执行第一步行为，之后再次进行规划，并不断迭代。通过这样的迭代优化可以降低模型误差带来的影响。</p></li>
<li><p><strong>UMBRELLA Trajectory Optimizer</strong>：</p>
<p>最终的轨迹输出使用MPPI framework</p>
<ul>
<li>Grady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model
predictive path integral control: From theory to parallel computation.
Journal of Guidance, Control, and Dynamics, 40(2): 344–357, 2017.</li>
</ul></li>
<li><p>最后，模型通过re-weighting获得最优轨迹：</p></li>
</ul>
<p><span class="math display">\[
T^*_t = \frac{\sum_{n=1}^N e^{kR_n} A_{n,t+1}}{\sum_{n=1}^N e^{kR_n}}
\]</span></p>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-1.png"
alt="UMBRELLA Planning" />
<figcaption aria-hidden="true">UMBRELLA Planning</figcaption>
</figure>
<ul>
<li><strong>Pessimistic Trajectory Optimizer</strong>：
面对认知的不确定性，UMBRELLA-P对最坏情况的结果进行优化，并采取悲观的行动</li>
</ul>
<h3 id="experimental-evaluation">4 Experimental Evaluation</h3>
<ul>
<li>环境：
<ul>
<li><strong>NGSIM</strong>：多智能体自动驾驶环境<br />
</li>
<li><strong>CARLA</strong>：城市多智能体自动驾驶场景</li>
</ul></li>
<li>基线方法：
<ul>
<li><strong>1-step IL</strong>：模仿专家行为的BC policy</li>
<li><strong>MBOP</strong>：当前最好的基于模型的离线RL方法
<ul>
<li>Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline
planning. In International Conference on Learning Representations,
2021.<br />
</li>
</ul></li>
<li><strong>MPUR</strong>：最好的基于模型策略学习方法
<ul>
<li>Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive
policy learning with uncertainty regularization for driving in dense
traffic. In International Conference on Learning Representations,
2019.<br />
</li>
</ul></li>
</ul></li>
<li>指标：
<ul>
<li>Success rate (SR)：The rate of collision-free
episodes，并要求在时间内到达目标位置</li>
<li>Mean distance (MD)：NGSIM中的纵向行驶距离</li>
<li>mean successful time (MST)<br />
</li>
<li>mean proximity reward <span
class="math display">\[\overline{r}_{prox}\]</span></li>
<li>mean lane reward <span
class="math display">\[\overline{r}_{lane}\]</span><br />
</li>
<li>mean final reward <span
class="math display">\[\overline{r}\]</span><br />
</li>
</ul></li>
<li>实验结果：</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220804-2.png"
alt="性能对比" />
<figcaption aria-hidden="true">性能对比</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>奖励函数误设计仍然是一个较大的问题，也是自动驾驶策略不像人的原因之一。</p>
<ul>
<li>W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt,
and Peter Stone. Reward (mis)design for autonomous driving. 2021.
arxiv:2104.13906.</li>
</ul>
<p>模型考虑偶然不确定性：使用潜变量<span
class="math display">\[z\]</span>，枚举可能的轨迹</p>
<p>模型考虑认知不确定性：在reward中进行re-weighting和使用参数<span
class="math display">\[\beta\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://txing-casia.github.io/2022/08/03/2022-08-03-Autonomous%20Driving%20-%20Urban%20Driver%20Learning%20to%20Drive%20from%20Real-world%20Demonstrations%20Using%20Policy%20Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/my_photo.jpg">
      <meta itemprop="name" content="Txing">
      <meta itemprop="description" content="泛用类人决战型机器人博士">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Txing">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/03/2022-08-03-Autonomous%20Driving%20-%20Urban%20Driver%20Learning%20to%20Drive%20from%20Real-world%20Demonstrations%20Using%20Policy%20Gradients/" class="post-title-link" itemprop="url">Autonomous Driving | Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-03 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-03T00:00:00+08:00">2022-08-03</time>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="urban-driver-learning-to-drive-from-real-world-demonstrations-using-policy-gradients">Urban
Driver: Learning to Drive from Real-world Demonstrations Using Policy
Gradients</h2>
<ul>
<li>取得了城市驾驶场景中最好的效果（Urban driving scenarios）</li>
<li>数据：使用100小时的城市道路专家示教数据</li>
<li>不必添加复杂的状态扰动；</li>
<li>不必在训练中收集额外的同策略数据；</li>
</ul>
<h3 id="introduction">Introduction</h3>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220803-1.png"
alt="本文的闭环训练算法概览" />
<figcaption aria-hidden="true">本文的闭环训练算法概览</figcaption>
</figure>
<ul>
<li>工业界最好轨迹规划器文献：
<ul>
<li>H. Fan, F. Zhu, C. Liu, L. Zhang, L. Zhuang, D. Li, W. Zhu, J. Hu,
H. Li, and Q. Kong. Baidu apollo em motion planner. ArXiv, 2018.</li>
</ul></li>
<li>本文主要贡献：
<ul>
<li>复杂城市驾驶场景中，第一个证明了用策略梯度学习，可以从大量真实世界演示数据中学习模仿驾驶策略；</li>
<li>一个新的可微分仿真器，可基于过去的数据进行闭环仿真，并通过时间的反向传播计算策略梯度，实现快速学习；</li>
<li>单纯在仿真器中训练可在真实世界中控制自动驾驶车辆，优于其他方法；</li>
<li>源码可得：https://planning.l5kit.org.</li>
</ul></li>
</ul>
<h3 id="related-work">Related work</h3>
<ul>
<li><p><strong>Trajectory-based optimization</strong>：</p>
<ul>
<li><p>这是当前工业界的主流方法（a dominant approach）</p></li>
<li><p>依赖手工定义的损失和奖励</p></li>
<li><p>损失的优化可结合一系列经典的算法：</p>
<ul>
<li>A* [11]</li>
<li>RRTs [12]</li>
<li>POMDP with solver [13]</li>
<li>dynamic programming [14]<br />
</li>
</ul></li>
<li><p>整体上是依赖human engineering，而不是数据驱动</p></li>
</ul></li>
<li><p><strong>Reinforcement learning（RL）</strong>：</p>
<ul>
<li>依赖仿真器的构造、精确编码和优化的奖励信号
<ul>
<li>S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent,
reinforcement learning for autonomous driving. ArXiv, 2016.<br />
</li>
</ul></li>
<li>手工编程的仿真器，不能还原真实的长尾场景</li>
<li>本文直接通过 mid-level representations 从真实世界的 log
中构建仿真环境</li>
</ul></li>
<li><p><strong>Imitation learning (IL) and Inverse Reinforcement
Learning (IRL)</strong>：</p>
<ul>
<li>原始的行为克隆（Naive behavioral
cloning）面临协变量偏移问题（covariate shift）</li>
<li>Adversarial Imitation Learning [31, 32,
33]，还没有在自动驾驶场景使用</li>
</ul></li>
<li><p><strong>Neural Motion Planners</strong>：</p>
<ul>
<li>在[34]中，原始感觉输入和高清地图被用于估计未来可能的SDV位置的成本量。基于这些成本量，可以对轨迹进行采样，并且选择最低成本的轨迹来执行。这些方法目前没有在实车测试。
<ul>
<li>W. Zeng, W. Luo, S. Suo, A. Sadat, B. Yang, S. Casas, and R.
Urtasun. End-to-end interpretable neural motion planner. Int. Conference
on Computer Vision and Pattern Recognition (CVPR), \2019.<br />
</li>
<li>S. Casas, A. Sadat, and R. Urtasun. Mp3: A unified model to map,
perceive, predict and plan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 14403–14412, 2021.</li>
</ul></li>
</ul></li>
<li><p><strong>Mid-representations and the availability of large-scale
real-world AD datasets</strong>：</p>
<ul>
<li>J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V.
Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving
motion prediction dataset. Conference on Robot Learning (CoRL),
2020.</li>
<li>M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
P. C. De Wang, S. Lucey, D. Ramanan, and J. Hays. Argoverse: 3d tracking
and forecasting with rich maps supplementary material. Int. Conf. on
Computer Vision and Pattern Recognition (CVPR).<br />
</li>
<li>state-of-the-art solutions for motion forecasting [8, 9]
<ul>
<li>[8] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C.
Schmid. Vectornet: Encoding hd maps and agent dynamics from vectorized
representation. In Int. Conf. on Computer Vision and Pattern Recognition
(CVPR), 2020.</li>
<li>[9] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R.
Urtasun. Learning lane graph representations for motion forecasting.
2020.<br />
</li>
</ul></li>
</ul></li>
<li><p><strong>Data-driven simulation</strong>：</p>
<ul>
<li>[23] created a photo-realistic simulator for training an end-to-end
RL policy.</li>
<li>[5] simulated a bird’s-eye view of dense traffic on a highway.</li>
<li>Finally, two recent works [39, 40] developed data-driven simulators
and showed their usefulness for training and validating ML
planners.</li>
</ul></li>
</ul>
<h3
id="differentiable-traffic-simulator-from-real-world-driving-data">Differentiable
Traffic Simulator from Real-world Driving Data</h3>
<ul>
<li>真实世界的经验轨迹：<span
class="math display">\[\overline{\tau}=\{\overline{s}_1,\overline{s}_2,...,\overline{s}_T\}\]</span></li>
<li>仿真的目标是迭代地生成观测状态序列<span
class="math display">\[\tau=\{s_1,s_2,...,s_T\}\]</span>，然后计算车辆轨迹<span
class="math display">\[p_t\]</span>，包括<span
class="math display">\[(x;y;\theta)\]</span></li>
<li><span class="math display">\[s_{t+1}=S(s_t,a_t)\]</span>，<span
class="math display">\[p_{t+1}=f(p_t,a_t)\]</span></li>
</ul>
<h3 id="imitation-learning-using-a-differentiable-simulator">Imitation
Learning Using a Differentiable Simulator</h3>
<ul>
<li><span class="math display">\[L(s_t,a_t)=\mid\mid \overline{p}_t -
p_t\mid\mid_1\]</span></li>
<li><span
class="math display">\[J(\pi)=\mathbb{E}_{\overline{\tau}\sim\pi_E}\mathbb{E}_{\tau\sim\pi}
\sum_{t}\gamma^t L(s_t,a_t)\]</span>，<span
class="math display">\[\pi_E\]</span>是专家策略，<span
class="math display">\[\pi\]</span>是模型的策略，希望两个策略接近</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220803-2.png"
alt="Imitation learning from expert demonstrations" />
<figcaption aria-hidden="true">Imitation learning from expert
demonstrations</figcaption>
</figure>
<blockquote>
<p>P.S.：由于轨迹的开始阶段均来自专家策略，会引入bias，在策略更新的时候，在运动开始的第K步之后才计算梯度，以此避免bias</p>
</blockquote>
<ul>
<li><p>策略梯度的计算（用下标表示偏微分，<span
class="math display">\[\theta\]</span>是策略参数）： <span
class="math display">\[
J_s^t = L_s+L_a\pi_s+\gamma J^{t+1}_{\theta}(S_s+S_a\pi_{s})\\
J_{\theta}^t =
L_a\pi_{\theta}+\gamma(J_s^{t+1}S_a\pi_{\theta}+J_{\theta}^{t+1})
\]</span> &gt; Ref: N. Heess, G. Wayne, D. Silver, T. Lillicrap, T.
Erez, and Y. Tassa. Learning continuous control policies by stochastic
value gradients. In Advances in Neural Information Processing Systems,
\2015.<br />
### Experiments</p></li>
<li><p>Lyft Motion Prediction Dataset
[6]：数据采集自加利福尼亚州帕洛阿尔托的复杂城市路线。数据集捕捉各种真实世界的情况，例如在多车道交通中驾驶、转弯、在十字路口与车辆互动等。</p>
<ul>
<li>J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V.
Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving
motion prediction dataset. Conference on Robot Learning (CoRL),
2020.<br />
</li>
</ul></li>
<li><p>模型在100小时子集上训练，并在25小时子集上测试。</p></li>
<li><p>three state-of-the-art baselines：</p>
<ul>
<li>Naive Behavioral Cloning (BC)<br />
</li>
<li>Behavioral Cloning + Perturbations (BC-perturb)
<ul>
<li>M. Bansal, A. Krizhevsky, and A. Ogale. Chauffeurnet: Learning to
drive by imitating the best and synthesizing the worst. 12 2018.<br />
</li>
</ul></li>
<li>Multi-step Prediction (MS Prediction)
<ul>
<li>A. Venkatraman, M. Hebert, and J. Bagnell. Improving multi-step
prediction of learned time series models. In AAAI, 2015.</li>
</ul></li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220803-3.png"
alt="性能对比" />
<figcaption aria-hidden="true">性能对比</figcaption>
</figure>
<blockquote>
<p>指标值越小越好，本文模型取得最好的表现以及最低的l1K指标（综合其它指标，每1000英里干预次数）</p>
</blockquote>
<ul>
<li>评价指标：
<ul>
<li><strong>L2</strong>: L2 distance to the underlying expert position
in the driving log in meters.</li>
<li><strong>Off-road events</strong>: we report a failure if the planner
deviates more than 2m laterally from the reference trajectory – this
captures events such as running off-road and into opposing traffic.</li>
<li><strong>Collisions</strong>: collisions of the SDV with any other
agent, broken down into front, side and rear collisions w.r.t. the
SDV.</li>
<li><strong>Comfort</strong>: we monitor the absolute value of
acceleration, and raise a failure should this exceed 3 m/s2.</li>
<li><strong>I1K</strong>: we accumulate safety-critical failures
(collisions and off-road events) into one key metric for ease of
comparison, namely Interventions per 1000 Miles (I1K)</li>
</ul></li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20220803-4.png"
alt="仿真结果" />
<figcaption aria-hidden="true">仿真结果</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>策略梯度的推导部分可以继续看看，本文有仿真和实车实验，但方法对比上，对其它算法进行了修改，因此并不完整。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Txing"
      src="/images/my_photo.jpg">
  <p class="site-author-name" itemprop="name">Txing</p>
  <div class="site-description" itemprop="description">泛用类人决战型机器人博士</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">229</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.uomi.moe/" title="https:&#x2F;&#x2F;blog.uomi.moe" rel="noopener" target="_blank">驱逐舰患者</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://m.mepai.me/photographyer/u_5a68085ba15aa.html?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;m.mepai.me&#x2F;photographyer&#x2F;u_5a68085ba15aa.html?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">隐之-INF</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Txing</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">538k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">8:09</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
