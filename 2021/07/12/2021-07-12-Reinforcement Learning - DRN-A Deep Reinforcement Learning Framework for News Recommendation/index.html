<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
          Reinforcement Learning | DRN: A Deep Reinforcement Learning Framework for News Recommendation - undefined
        
    </title>

    <link rel="canonical" href="https://txing-casia.github.io/2021/07/12/2021-07-12-Reinforcement Learning - DRN-A Deep Reinforcement Learning Framework for News Recommendation/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/hux-blog.min.css">


    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 6.3.0"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Hexo-Txing</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    
<!-- Image to hack wechat -->
<!-- <img src="https://txing-casia.github.io/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="{{ site.baseurl }}/{% if page.header-img %}{{ page.header-img }}{% else %}{{ site.header-img }}{% endif %}" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('img/post-bg-py.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                          <a class="tag" href="/tags/#Reinforcement Learning" title="Reinforcement Learning">Reinforcement Learning</a>
                        
                          <a class="tag" href="/tags/#Recommendation" title="Recommendation">Recommendation</a>
                        
                    </div>
                    <h1>Reinforcement Learning | DRN: A Deep Reinforcement Learning Framework for News Recommendation</h1>
                    <h2 class="subheading">Microsoft 提出的新闻推荐算法</h2>
                    <span class="meta">
                        Posted by Txing on
                        2021-07-12
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="DRN-A-Deep-Reinforcement-Learning-Framework-for-News-Recommendation"><a href="#DRN-A-Deep-Reinforcement-Learning-Framework-for-News-Recommendation" class="headerlink" title="DRN: A Deep Reinforcement Learning Framework for News Recommendation"></a>DRN: A Deep Reinforcement Learning Framework for News Recommendation</h1><p>论文链接：<a target="_blank" rel="noopener" href="http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf">http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf</a></p>
<h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><ul>
<li><p>针对新闻和用户动态特征的问题，目前的方法主要存在三个问题：</p>
<ul>
<li>模型<strong>只考虑当前奖励</strong>（例如CTR，点击转化率）</li>
<li>很少有模型考虑<strong>使用用户的反馈</strong>而不是点击&#x2F;不点击的标签（例如用户反馈频率）</li>
<li>模型总是<strong>推荐相似的东西</strong>，会使得用户感到无聊</li>
</ul>
</li>
<li><p>一些典型方法：</p>
<ul>
<li>content based methods [19, 22, 33], </li>
<li>collaborative filtering based methods [11, 28, 34]</li>
<li>hybrid methods [12, 24, 25].</li>
<li>deep learning models [8, 45, 52]</li>
</ul>
</li>
<li><p>三个挑战：</p>
<ul>
<li>First, the dynamic changes in news recommendations are difficult to handle.<ul>
<li>First, news become outdated very fast.</li>
<li>Second, users’ interest on different news might evolve during time.</li>
</ul>
</li>
<li>Second, current recommendation methods [23, 35, 36, 43] usually only consider the click &#x2F; no click labels or ratings as users’ feedback.</li>
<li>The third major issue of current recommendation methods is its tendency to keep recommending similar items to users, which might decrease users’ interest in similar topics.</li>
</ul>
</li>
<li><p>一般的强化学习推荐方法使用 $$\epsilon-greedy$$ 或者 Upper Confidence Bound (UCB) 方法增加推荐的探索能力。</p>
<p>但是 $$\epsilon-greedy$$ 会损害当前推荐的性能，而UCB在一件物品尝试了几次之后才能得到相对准确的奖励评估。因此需要更高效的探索方式。</p>
</li>
<li><p>通过使用 Dueling Bandit Gradient Descent (DBGD) 算法作为探索策略，它会随机从候选项的邻域或者候选项中选择推荐对象。这个探索策略可以避免推荐完全无关的信息，因此可以保持当前策略的精度。</p>
</li>
<li><p>模型框图：</p>
<p><img src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-1.png"></p>
</li>
<li><p>算法每1小时更新一次。Every one hour, the agent will use the log in the memory to update its recommendation algorithm.</p>
</li>
<li><p>主要贡献：</p>
<ul>
<li>我们认为用户的活跃性有助于提高推荐的准确性，这可以提供额外的信息，而不是简单地使用用户点击标签。 </li>
<li>应用了更有效的探索方法，避免了由经典探索方法(例如，$$\epsilon$$-greedy和置信上限)引起的推荐准确度下降。 </li>
<li>我们的系统已经在线部署在商业新闻推荐应用程序中。大量的离线和在线实验显示了我们方法的优越性能。</li>
</ul>
</li>
<li><p>一些推荐方法的特点：</p>
<ul>
<li>Content-based methods：统计内容的频率，推荐相似的内容；</li>
<li>Collaborative filtering methods：使用过去的转化率或者其他类似用户的转化率，或者结合两者来预测；</li>
<li>Hybrid methods：提升用户画像模型；</li>
<li>deep learning models：建模复杂的用户-物品关系</li>
</ul>
</li>
</ul>
<h3 id="算法模型："><a href="#算法模型：" class="headerlink" title="算法模型："></a>算法模型：</h3><ul>
<li><p>算法步骤：</p>
<ol>
<li><strong>PUSH</strong>: In each timestamp (t1, t2, t3, t4, t5, …), when a user sends a news request to the system, the recommendation agent G will take the feature representation of the current user and news candidates as input, and generate a top-k list of news to recommend L. L is generated by combining the exploitation of current model (will be discussed in Section 4.3) and exploration of novel items (will be discussed in Section 4.5).</li>
<li><strong>FEEDBACK</strong>: User u who has received recommended news L will give their feedback B by his clicks on this set of news. </li>
<li><strong>MINOR UPDATE</strong>: After each timestamp (e.g., after timestamp t1), with the feature representation of the previous user u and news list L, and the feedback B, agent G will update the model by comparing the recommendation performance of exploitation network Q and exploration network Q˜ (will be discussed in Section 4.5). If Q˜ gives better recommendation result, the current network will be updated towards Q˜ . Otherwise, Q will be kept unchanged. Minor update can happen after every recommendation impression happens.</li>
<li><strong>MAJOR UPDATE</strong>: After certain period of timeTR(e.g., after timestamp t3), agent G will use the user feedback B and user activeness stored in the memory to update the network Q. Here, we use the experience replay technique [31] to update the network. Specifically, agent G maintains a memory with recent historical click and user activeness records. When each update happens, agent G will sample a batch of records to update the model. Major update usually happens after a certain time interval, like one hour, during which thousands of recommendation impressions are conducted and their feedbacks are collected.</li>
<li>Repeat step (1)-(4).</li>
</ol>
</li>
<li><p>特征设计：</p>
<ul>
<li><p><strong>News features</strong> includes 417 dimension one hot features that describe whether certain property appears in this piece of news, including headline, provider, ranking, entity name, category, topic category, and click counts in last 1 hour, 6 hours, 24 hours, 1 week, and 1 year respectively. </p>
</li>
<li><p><strong>User features</strong> mainly describes the features (i.e., headline, provider, ranking, entity name, category, and topic category) of the news that the user clicked in 1 hour, 6 hours, 24 hours, 1 week, and 1 year respectively. There is also a total click count for each time granularity. Therefore, there will be totally 413 × 5 &#x3D; 2065 dimensions. </p>
</li>
<li><p><strong>User news features</strong>. These 25-dimensional features describe the interaction between user and one certain piece of news, i.e., the frequency for the entity (also category, topic category and provider) to appear in the history of the user’s readings. </p>
</li>
<li><p><strong>Context features</strong>. These 32-dimensional features describe the context when a news request happens, including time, weekday, and the freshness of the news (the gap between request time and news publish time).</p>
</li>
</ul>
</li>
<li><p>模型框架，包含离线和在线部分</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-3.png" alt="Model framework"></p>
<p><img src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-4.png" alt="Q network"><br>$$<br>y_{s,a}&#x3D;Q(s,a)&#x3D;r_{immediate}+\gamma r_{future}\<br>y_{s,a,t}&#x3D;r_{a,t+1}+\gamma Q\big(s_{a,t+1},\arg \max_{a’} Q(s_{a,t+1},a’;W_t);W_t’\big)<br>$$</p>
<h3 id="User-Activeness"><a href="#User-Activeness" class="headerlink" title="User Activeness"></a>User Activeness</h3><p><img src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-3.png" alt="User activeness estimation"><br>$$<br>\lambda(t)&#x3D;\lim_{dt\rightarrow 0} \frac{\Pr{t \leq T&lt;t+dt\mid T\geq t }}{d t}\<br>S(t)&#x3D;e^{-\int_0^\infty \lambda(x)dx}\<br>T_0&#x3D;\int_0^{\infty}S(t)dt<br>$$<br>the maximum user activeness is truncated to 1.</p>
<p>The click &#x2F; no click label $$r_{click}$$ and the user activeness $$r_{active}$$ are combined as:<br>$$<br>r_{total}&#x3D;r_{click}+\beta r_{active}<br>$$</p>
<ul>
<li>网络框架</li>
</ul>
<p><img src="https://raw.githubusercontent.com/txing-casia/txing-casia.github.io/master/img/20210712-2.png" alt="Exploration by Dueling Bandit Gradient Descent"></p>
<p>$$\hat{L}$$ 首先使用概率交错方法从 $$L$$ 和 $$\tilde{L}$$ 。大致就是首先随机从 $$L$$ 和 $$\tilde{L}$$ 选择推荐项，之后按照概率值更新网络 $$\tilde{Q}$$</p>
<p>$$<br>\Delta W&#x3D;\alpha \cdot rand(-1,1)\cdot W\<br>W’&#x3D;W+\eta \tilde{W}<br>$$</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>感觉整体框架还是很好理解的，针对任务的改进比较多，对强化学习框架的改动比较小。</p>
<p>两个网络的构造也类似于DQN中的target network，输入的高维特征类比于输入的图像信息。</p>


                <hr>

                

                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2021/07/13/2021-07-13-Q130-被围绕的区域-中等-深度and广度优先搜索/" data-toggle="tooltip" data-placement="top" title="Q130-被围绕的区域-中等-DFS/BFS">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2021/07/08/2021-07-08-Reinforcement Learning - Option-critic architecture/" data-toggle="tooltip" data-placement="top" title="Reinforcement Learning | Option-critic Architecture">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                

                

            </div>
    <!-- Side Catalog Container -->
        

    <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                

                <!-- Friends Blog -->
                
            </div>

        </div>
    </div>
</article>









    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Hexo-Txing 2023 
                    <br>
                    Theme by <a target="_blank" rel="noopener" href="http://huangxuan.me">Hux</a> 
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span> 
                    Ported by <a target="_blank" rel="noopener" href="http://blog.kaijun.rocks">Kaijun</a> | 
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=kaijun&repo=hexo-theme-huxblog&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://txing-casia.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->




<!-- Baidu Tongji -->


<!-- Side Catalog -->





<!-- Image to hack wechat -->
<img src="https://txing-casia.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
